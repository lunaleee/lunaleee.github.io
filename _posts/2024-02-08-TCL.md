---
title: "[논문 리뷰] Learning to Generate Text grounded Mask for Open World Semantic Segmentation (TCL)"
author: lunalee
date: 2024-02-09 22:02:11 +0800
categories: [AI, Paper Review]
tags: [Image, Segmentation, Contrastive Learning]
pin: true
math: true
---

<br/><br/>
`Kakao Brain` `CVPR 2023`

- Paper: [https://arxiv.org/abs/2212.00785](https://arxiv.org/abs/2212.00785)
- Git: [https://github.com/kakaobrain/tcl](https://github.com/kakaobrain/tcl)
- Project Page: [https://huggingface.co/spaces/khanrc/tcl](https://huggingface.co/spaces/khanrc/tcl)
<br/><br/><br/><br/>


# Introduction

---

**Open-world semantic segmentation** 

 조밀한 annotation 없이 이미지-텍스트 쌍만을 사용하여 이미지에서 임의의 시각적 개념을 분할하는 것을 목표로 한다. 기존의 semantic segmentation은 미리 정의된 소수의 target category에 대한 segmentation feature를 학습하는 것을 목표로 하는 반면, open-world semantic segmentation은 제한되지 않은 임의 category 또는 자유 형식 텍스트를 다룬다.
<br/><br/>

Open-world segmentation의 첫 번째 과제는 미리 정의된 범주를 넘어서서 임의의 category를 학습하는 것이다. 이전의 접근 방식은 CLIP의 성공에 영향을 받아 웹에서 크롤링된 대규모 image-text pair를 이용했다. 대규모 image-text pair를 이용하는 방식은 임의의 semantic category에 대한 풍부한 지식을 제공하지만, dense한 annotation이 없으므로 임의의 개념을 정확하게 localization하기 어려운 문제가 있다. dense한 annotation은 segmentation 성능을 향상시키는데 도움이 되지만 labeling cost가 매우 높은 문제가 있다.
<br/><br/><br/>

![TCL_1](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/ee1d2047-2cf9-49ba-96d8-3c4fafffe06f){: width="500px"}

따라서 본 논문에서는 dense한 annotation 없이 image-text pair만으로 open-world semantic segmentation하는 것에 중점을 둔다. 기존 방법은 image-text alignment을 통한 학습을 통해서도 좋은 결과를 보여 주었지만 그림 2에서 볼 수 있는 것처럼 training 단계와 test 단계 사이의 alignment 수준 불일치로 인해 여전히 어려움을 겪고 있다.(이미지 내에서 정확한 영역을 정렬하기 어려움)<br/>

이러한 훈련-테스트 불일치를 해결하기 위해 논문에서는 모델이 dense한 annotation 없이 image-text pair에서 직접 region-test alignment을 학습할 수 있는 **Text-grounded Contrastive Learning(TCL)** 프레임워크를 제안한다. 논문의 핵심 아이디어는 그림 2와 같이 text-grounded 절차를 추가하여 text-grounded 영역을 나타내는 segmentation mask를 생성하고, text와 grounded region 사이에 contrastive learning을 적용하는 것이다.
<br/><br/><br/><br/><br/>

# Method

---

![TCL_2](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/9695be23-4e2b-474b-b22c-27ff92a45dd1){: width="700px"}

Open-world semantic segmentation에서 training 중 모델이 image-text alignment를 학습하도록 하지만, test시에는 image 전체가 아닌 region-text alignment를 계산해야하는 alignment-level 불일치 문제가 있다. 이를 해결하기 위해 본 논문에서는 CL내에 text grounding 프로세스를 통합하여, region-text alignment를 직접 할 수 있는 TCL을 제안했다. Grounding 모듈은 text-described region에 대한 segmentation mask를 생성하도록 한다. 이렇게 영역을 암시적으로 학습하는 기존 방법과 비교하여 TCL은 end-to-end 훈련이 가능한 grounder를 통해 grounding 능력을 명시적으로 학습한다는 이점이 있다.

![TCL_3](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/2a3cdab6-0be3-4ef7-a1f3-116a59273922){: width="1000px"}
<br/><br/><br/>

## 1. Grounder

입력 image-text pair 배치에 대해 text-grounded mask을 찾는 것을 목적으로 하며, 3단계로 구성되어있다.<br/><br/>

1. **Image Encoder  $E_v$** (CLIP, frozen): Global feature, Dense patch level feature 생성
2. **Text Encoder $E_t$** (CLIP, frozen): Text embedding feature 생성
3. **Grounding Decoder $D_g$**: 1번(Image Encoder)의 dense patch level feature를 텍스트 정렬을 위해 더 미세한 pixel-level embedding으로 변환

<br/>
Text-grounded mask는 text embedding과 pixel-level embedding간의 position-wise dot product로 계산된다.
<br/><br/><br/><br/>

## 2. Text-grounded Contrastive Learning

#### **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>1. Image-level TCL Loss</span></mark>**

1. Grounder에서 생성된 마스크 $M_{i,i}$ 에 Gumble-Max를 적용하여 이진화된 마스크 $M^b_{i,i}$를  생성한다.(backprop이 가능하게 하기 위함)

2. 주어진 이미지 $X^V_i$ 와 이진화된 마스크 $M^b_{i,i}$ 를 곱하여 미분가능한 masked 이미지를 생성한다.

3. Masked 이미지는 Image Encoder(CLIP, frozen)에 들어가고, text-grounded image embedding $\tilde{v^g_i}$ 를 얻는다.
> $$\quad \tilde{v^g_i}, \tilde{v^d_i} = E_v(M^b_{i,i} \cdot X^V_i)
> $$
> 

4. Text-grounded image embedding과 text embedding의 consine similarity를 구한다.<br/>
> $$ S^m_{i,j} = {\tilde{v}^g_i}^\top t_j
> $$
> 

5. Image-level TCL loss를 정의하기 위해 symmetric version of InfoNCE 사용: positive image-text pair는 서로 유사하게 만들고 negative image-text pair는 similarity matrix기반으로 유사하지 않게 만든다.<br/>
> $$ \mathcal{L}_{TCL_v} =  InfoNCE(S^m)
> $$
> 

<br/>

![TCL_4](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/9b8feaf3-54a3-4bf1-b2bb-9bd4e12e6f14){: width="400px"}
<br/><br/><br/><br/>

#### **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>2. Feature-level TCL Loss</span></mark>**

Image-level TCL loss는 pair을 이루는 텍스트(즉, positive pair의 텍스트)에 대한 segmentation 마스크를 생성하도록 모델을 학습한다. 여기에 text에 기술되지 않은 영역, 즉 negative 영역을 잘 생성하기 위한 loss를 추가했다. Negative pair에서 얻은 negative mask의 feature를 잘 생성할 수 있는 feature-level TCL loss를 제안했다.
<br/><br/>
1. Negative mask $M_{i,j (i≠j)}$  에 대해서, grounding decoder의 pixel-level dense embeddings $V^s_i$ 과 text embedding $t_j$에  아래와 같은 식을 적용하여 feature-level text-grounded image embedding $v^f_{i,j}$ 를 얻는다.<br/>
    ![TCL_5](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/8521c317-9158-4631-99bc-20826eb36bcf){: width="300px"}

2. 여기에 마찬가지로 배치안의 모든 text embedding - feature level text-grounded image embedding pair에 대해 cosine similarity를 구한다.<br/>
> $$ S^f_{i,j} = {v^f_{i,j}}^\top t_j
> $$
> 

3. Feature-level TCL loss<br/>
> $$\mathcal{L}_{TCL_f} = InfoNCE(S^f)
> $$
> 

<br/><br/><br/><br/>

#### **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>3. Area TCL Loss</span></mark>**

위의 두가지 loss만을 사용해서 End-to-End로 학습을 진행할 때, 모델은 전체 이미지에 대한 마스크를 생성하는, 즉 전체 이미지를 positive region을 보는 잘못된 방식으로 학습되기 쉽다. 이러한 한쪽으로 치우치는 문제를 방지하기 위해 positive, negative 각 영역에 대한 패널티를 줘서 영역의 비율을 맞추는 loss를 추가한다. 

![TCL_6](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/4e52c7eb-8539-4821-99cd-dabe84e07f8c){: width="600px"}
<br/><br/><br/><br/>

#### **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>4. Smooth regularization</span></mark>**

Image-text dataset에서 텍스트는 이미지의 두드러진 부분을 설명하지만, 텍스트에 설명된 영역은 일반적으로 noisy하지않고 smooth한 부분이라는 것을 관찰했다고 한다. 따라서 smoothness를 위해 아래와 같은 regularzation term을 추가하였다. 

![TCL_7](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/629da4a9-fca9-4fdd-9dba-b2b67b9f3c15){: width="300px"}

여기서 $\| \cdot \|_{TV}$는 anisotropic TV norm이다. 
<br/><br/><br/><br/>

#### **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>5. Final Loss</span></mark>**

최종 loss는 아래와 같고, $\lambda_{TCL}, \lambda_{area}, \lambda_{tv}$ 는 hyperparameter이다.

![TCL_8](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/8118c50a-511d-4849-af55-76b9a96ee377){: width="400px"}
<br/><br/><br/><br/>

## 3. Inference Pipeline

Zero-shot inference pipeline은 CLIP과 유사하다(image-level classication 대신 pixel-level classification이라는 점만 제외하면). 최종 segmentation output은 위의 학습과정에서 Text-grounded Contrastive Learning 과정을 뺀, 즉 text-grounded mask가 된다.
<br/><br/><br/><br/>

# Experiments

---

## 1. Experiment Settings

**Unified evaluation protocol.**

Open-world semantic segmentation task에 대해서는 표준 평가 프로토콜이 아직 확립되지 않았다. 이전의 연구들은 각각 자체 프로토콜을 사용했는데, 각각의 데이터셋에 대해 서로 다른 프로토콜을 적용하기도 했다.

공정한 비교를 위해 해당 논문에서는 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>open-world scenario를 따르는 Unified evaluation protocol</span></mark>**을 제시한다. 평가 전 target data에 대한 사전 접근은 불가능하며, dataset-specific한 hyperparameter나 trick 또한 금지된다. 

예를 들어 TCL은 "사람"의 대상 클래스를 하위 개념(예: 남자, 여자, 작업자, 라이더 등)으로 확장하면 상당한 성능 향상을 얻을 수 있지만 이러한 트릭은 허용되지 않는다. 이를 고려하기 위해 논문에서는 클래스 이름 기반 트릭 없이 **MMSegmentation 기본 버전의 통합 클래스 이름**을 사용하여 모델을 평가한다. 이외에 다른 모든 평가 설정은 GroupViT 논문을 따른다. 성능 지표로  **mean intersection-over-union (mIoU)**를 사용한다.
<br/><br/><br/>

**Implementation details.**

- Grounder: 입력 이미지 크기가 224 × 224, 패치 크기가 16 × 16인 CLIP ViT-B/16 모델을 사용
- Grounding decoder: 4개의 gated convolution block, 2개의 upsampling interpolation
- Mask refinement를 위해 pixel-adaptive mask refinement (PAMR) 방법 사용
- loss weights:  $λ_TCL = 0.1, λ_area = 0.4, λ_tv = 1.0$
<br/><br/><br/>

**Benchmark dataset.**

8개의 benchmark사용, 2개의 그룹으로 나눔

i) With background class: PASCAL VOC, PASCAL Context, COCO-Object<br/>
ii) Without background class: PASCAL VOC20, PASCAL Context59, COCO-Stuff, Cityscapes, ADE20K
<br/><br/><br/>

## 2. Zero-shot Transfer to Semantic Segmentation

기존의 open-world semantic segmentation method인 GroupViT와 MaskCLIP에 대해 성능을 비교하였다. 비교 수치는 아래 표와 같다. 

![TCL_9](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/bbbe9a28-e320-4259-9492-5ffd7158968f){: width="1100px"}
<br/><br/><br/>

## 3. Qualitative Results

아래의 그림은 학습된 grounding decoder의 성능을 보여준다. Grounding decoder$(D_g)$는 TCL을 통해 region-level alignment를 학습하여 더 정확하고 미세하며 노이즈가 적은 마스크를 생성하는 것을 볼 수 있다.

![TCL_10](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/8c4d83b5-b714-42cf-871d-0c31fd7238cf){: width="600px"}
<br/><br/><br/>

아래 그림은 정성적 비교에 대한 그림이다.

![TCL_11](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/898362c5-571c-4bf4-9559-26dc34a0eb08){: width="1100px"}
<br/><br/><br/>

## 4. Ablation Studies

PASCAL VOC20 데이터셋의 일부를 이용하여 제안된 프레임워크의 개별 구성 요소의 영향을 조사했다.

표(a)는 Baseline에서 각각의 요소에 대한 누적 결과이다.<br/>
표(b)는 제안된 TCL loss의 각 구성 요소의 영향과 기존 CL loss와 비교하여 segmentation 성능에 미치는 영향에 대한 조사이다.<br/>
표(c),(d),(e)는 각각 hyperparameter에 대한 성능 변화에 대한 표이다.

![TCL_12](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/5d84f3ce-d142-49c5-bd21-14d55a9d3796){: width="500px"}
<br/><br/>


