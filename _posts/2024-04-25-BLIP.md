---
title: "[논문 리뷰] BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"
author: lunalee
date: 2024-04-25 20:14:32 +0900
categories: [AI, Paper Review]
tags: [VLP, Detection]
pin: false
math: true
---

<br/><br/>
`Salesforce Research` `ICML 2022`

- Paper: [https://arxiv.org/abs/2201.12086](https://arxiv.org/abs/2201.12086)
- Git: [https://github.com/salesforce/BLIP](https://github.com/salesforce/BLIP)
- Page: [https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/](https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/)
<br/><br/><br/><br/><br/>

# Introduction

---

**VLP(Vision-Language Pre-training)**은 최근 다양한 multimodal downstream task에 대해 성공을 거두었지만, 두 가지 한계점을 가지고 있다. 

1. Model 관점: 대부분의 방법은 encoder-based model 또는 encoder-decoder model을 사용한다. Encoder-based model은  text generation task(e.g. image captioning)로 직접 사용되기 어려운 문제가 있고, encoder-decoder model은  image-text 검색 task에서 잘 동작하지 않는 문제가 있다.
2. Data 관점: 다양한 기존 SOTA 방법들은 웹에서 수집한 image-text pair를 사용하고 있다. 이러한 데이터는 noisy하다는 문제가 있다.
<br/><br/><br/>

이러한 문제를 해결하기 위해 통합된 vision-language understanding/generation 모델인 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>BLIP: Bootstrapping Language- Image Pre-training</span></mark>**을 제안한다. BLIP은 기존의 방법보다 더 넓은 범위의 downstream task를 가능하게 하는 새로운 VLP framework이다. 주요 contribution은 아래와 같다.

1. MED(Multimodal mix of Encoder-Decoder): 효과적인 multi-task pre-training 및 유연한 transfer learning을 가능하게 하는 새로운 모델 구조 제안.
2. Captioning and Filtering (CapFilt): noisy한 image-text pair에서 학습하기 위한 새로운 데이터셋 boostrapping 방법 제안.
    
    ![BLIP_1.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/6cf84739-2265-48d6-a2e1-399f700d8d92){: width="700px"}
    
<br/><br/><br/><br/><br/><br/>

# Method

---

## 1. Model Architecture

Understanding과 generation 능력을 모두 갖춘 통합 모델을 pre-train하기 위해서, 세 가지 특징에 대해 동작하는 **multi-task** 모델 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>multimodal mixture of encoder-decoder(MED)</span></mark>**를 제안하였다. Image Encoder로는 ViT(Vision Transformer)를 사용하고 global image feature를 나타내는 [CLS] token을 추가한다.
<br/><br/>

1. **Unimodal encoder** : 이미지와 텍스틀르 별도로 인코딩. 텍스트 인코더로는 BERT에 사용. 이미지와 마찬가지로 문장 요약을 위한 [CLS] token이 text input에 추가된다.
2. **Image-grounded text encoder** : Text Encoder의 각 transformer block에서, self-attention (SA) layer와 feed forward network (FFN) 사이에 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>cross-attention (CA)을 추가</span></mark>**하여 **visual information**을 주입함. Task-specific한 [Encode] token을 text에 추가하고 [Encode]의 output embedding이 text-image pair의 multimodal representation으로 사용된다.
3. **Image-grounded text decoder** : Image-grounded text encoder의 directional self-attention layer를 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>causal self-attention layer로 대체</span></mark>**. [Decode] token은 sequence의 시작을 알리는 시그널로 사용되고 end-of-sequence token은 signal의 끝을 알리는데 사용된다.
<br/><br/><br/><br/><br/>

## 2. Pre-training Objectives

본 논문에서는 pre-training 중 두 개의 **understanding-based objective**, 하나의 **generation-based objective**로 총 세 가지 objective에 대해 학습을 진행한다. 이미지-텍스트 pair에서 이미지에 대해서는 하나의 forward pass, 텍스트에 대해서는 세 개의 forward pass가 존재한다.

![BLIP_2.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/8bd710fc-4245-43af-9c29-ba3139835e05){: width="1100px"}

<br/><br/>

#### 1. Image-Text Contrastive Loss (ITC)

ITC는 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>unimodal encoder</span></mark>**를 활성화한다. 즉, Image encoder의 임베딩과 Text encoder의 임베딩을 사용한다. Image-text pair가 positive pair인 경우에는 similarity가 높아지고 negative pair인 경우에는 similarity가 낮아지는 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Contrastive Learning</span></mark>**을 활용하였다. 이 학습 방법으로 image transformer와 text transformer의 feature space를 잘 align하도록 학습한다.
<br/><br/>

[ALBEF📄](https://arxiv.org/abs/2107.07651) 논문의 ITC Loss를 사용하며, 해당 논문에서 noisy한 웹 데이터를 처리하기 위한 방법으로 제안한 **momentum encoder**를 도입하여 soft label을 생성, 학습에 활용했다.
<br/><br/>

> **Momentum Encoder**<br/>
web 데이터는 positive pair에 대해서 weakly-correlated 인 경우, negative text임에도 image와 매칭되는 등 noisy한 문제가 많다. 이러한 문제를 위해 momentum 모델을 사용하여 **pseudo-target**을 만들어, 기존 ont-hot label의 문제점을 보완한다.<br/>
momentum 모델은 기존 학습된 모델(unimodel and multimodal encoder)을 가져와 exponential-moving-average(EMA)로 학습한 버전이다. ITC Loss를 변형하여 target에 대해 **soft label**을 생성하도록 한다. 자세한 내용은 논문을 참조해보자.[📄](https://arxiv.org/abs/2107.07651)
> 

<br/><br/>

#### 2. Image-Text Matching Loss (ITM)

ITM은 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>image-grounded text encoder</span></mark>**를 활성화한다. Vision과 language 사이의 세밀한 alignment를 포착하는 image-text multimodal representation을 학습하는 것을 목표로 한다. 

ITM은 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>binary classification task</span></mark>** 이다. Multimodal feature가 주어지면,  ITM head(linear layer)를 사용하여 image-text pair가 positive(matched)인지 negative (unmatched)인지 예측한다. 

**hard negative mining strategy**를 사용했다.
<br/><br/><br/>

#### 3. Language Modeling Loss (LM)

LM은 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>image-grounded text decoder</span></mark>**를 활성화하여, 이미지가 주어졌을 때 textual description을 생성하는 것을 목표로 한다. Autoregressive 방식으로, Image 임베딩에 대해 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>텍스트의 likelihood를 maximize</span></mark>** 하도록 모델을 학습하는 cross entropy loss를 사용한다. Loss를 계산할 때 label smoothing 0.1을 적용한다.

일반적으로 VLP에 많이 사용되는 MLM loss와 비교하여 LM은 모델이 generalization 능력을 갖게 하며, 그 결과 visual information을 일관된 caption으로 변환할 수 있다.
<br/><br/><br/><br/>

Multi-task learning에서 효율적인 pre-training을 수행하기 위해 text encoder와 text decoder는 SA layer를 제외한 모든 매개변수를 공유한다. SA layer에서 parameter sharing을 수행하지 않는 이유는 Encoding 작업과 Decoding 작업의 차이가 SA layer에서 가장 잘 포착되기 때문이라고 한다. 반면 embedding layer, CA layer, FFN은 두 작업에서 유사하게 동작하므로 공유를 통해 학습 효율성을 향상시킨다. 

- Encoder SA layer: **현재** 입력 token에 대한 representation을 생성하기 위한 **bi-directional **self-attention**
- Decoder SA layer: **다음** token을 예측하기 위한 **causal self-attention**
<br/><br/><br/><br/><br/>

## 3. CapFilt

고품질의 human-annotated image-text pair $\{(I_h, T_h)\}$는 수가 제한되어 있으므로 최근의 연구에서는 웹에서 자동으로 수집하는 대용량의 image-text pair $\{(I_w, T_w)\}$를 활용한다. 그러나 웹에서 수집되는 텍스트는 **noisy**한 경우가 많아 vision-language alignment에 적합하지 않다.
<br/><br/>

![BLIP_3.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/7c8ee9de-4404-446f-a859-5a92110d2e09){: width="1200px"}

저자는 text 데이터의 품질을 향상시키기 위한 새로운 방법 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Captioning and Filtering (CapFilt)</span></mark>**을 제안하였다. CapFilt는 위의 그림과 같이 두 가지 모듈로 구성되어있다.

- **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>Captioner</span></mark>**: 웹 이미지에 대한 caption을 생성
    - image-grounded text decoder
    - 이미지가 주어졌을 때 텍스트를 decoding 하기 위해 **LM objective**로 pre-train됨
    - 웹 이미지 $I_w$가 주어지면 합성 캡션 $T_w$ 생성
- **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>Filter</span></mark>**: noise가 있는 image-text pair 제거
    - image-grounded text encoder
    - 텍스트가 이미지와 매칭되는지 여부를 알아보기 위해 **ITC, ITM objective**로 pre-train됨
    - 원본 웹 텍스트 $T_w$와 합성 텍스트 $T_s$ 모두에서 noisy한 텍스트를 제거: ITM head가 텍스트를 이미지와 일치하지 않는 것으로 예측하면 텍스트가 noisy하다고 판단
    - 마지막으로 필터링된 image-text pair를 human-annotated pair와 결합하여 새로운 데이터 세트를 형성하고, 모델을 새로 pre-train하는 데 사용함
<br/><br/>

captioner와 filter는 모두 pre-trained MED 모델에서 initialize 되고 COCO 데이터셋을 사용하여 개별적으로 fine-tuning 된다.
<br/><br/><br/><br/><br/><br/>

# Experiments and Discussions

---

## 1. Pre-training Details

- Image transformer: ImageNet에서 pre-train된 ViT로 initialize 됨
- Text transformer: BERTbase에서 initialize 됨
- pre-training 중에 224 × 224 resolution으로 random image crop 적용, fine-tuning 중에 image resolution을 384 × 384로 높임
- pre-training dataset: 총 14M 이미지, 2가지 human-annotated 데이터셋(COCO, Visual Genome)과 3가지 web 데이터셋(Conceptual Captions, Conceptual 12M, SBU captions)을 포함
- 더 noisy한 text가 포함된 115M 이미지를 포함하는 추가 web 데이터셋 LAION에 대해서도 실험함
<br/><br/><br/><br/><br/>

## 2. Effect of CapFilt

![BLIP_4.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/4a266343-98cb-4ede-8d4e-a6f776159687){: width="900px"}

위의 표는 image-text retrieval(검색) 및  image captioning을 포함한 down-stream task에서 CapFilt의 효율성을 보여주기 위해 다양한 데이터 세트에 대해 pre-train된 모델을 비교한 결과이다.

14M 이미지가 포함된 데이터셋에 captioner나 filter 중 하나를 적용하는 것만으로도 성능이 향상되는 것을 확인 할 수 있다. 둘 다 적용하면 효과가 보완되어 상당한 개선이 이루어진다. 
<br/><br/>

위의 그림은 captioner가 새로운 textual description을 생성하는 효과와 원본 web 텍스트와 합성 텍스트 모두에서 noise caption을 제거하는 filter의 효과에 대한 이미지이다.
<br/><br/><br/><br/><br/>

## 3. Parameter Sharing and Decoupling

![BLIP_5.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/4e7f7ae8-c06c-4ce0-9ea7-8824408d3335){: width="800px"}

Pre-train 중 text encoder와 decoder는 self-attention layer를 제외한 모든 paremeter를 공유한다. 위의 표에서는 web 텍스트가 포함된 14M 이미지에 대해 **다양한 parameter sharing strategy**으로 pre-train된 모델을 평가한 결과이다.
<br/><br/><br/><br/><br/><br/>

# Comparison with State-of-the-arts

---

### 1. Image-Text Retrieval

![BLIP_6.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/eb5cf993-fae3-4d91-b61c-3b0c4ba1fda6){: width="1000px"}

![BLIP_7.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/1d59fbb5-2e55-4933-840d-d8ac0b2649b0){: width="500px"}

COCO, Flickr30K 데이터셋에서 image-to-text 검색(TR), text-to-image 검색(IR)에 대해 BLIP을 평가했다.  ITC 및 ITM Loss을 사용하여 pre-train된 모델을 fine-tuning했다. 표 5에서 볼 수 있듯이 BLIP은 기존 모델과 비교하여 상당한 성능 향상을 달성했다. 

또한 COCO에서 fine-tuning된 모델을 Flickr30K로 transfer하여 zero-shot retrieva을 수행했다. 결과는 표 6에 나와 있으며, 여기서도 BLIP은 기존 방법보다 훨씬 뛰어난 성능을 보이고 있다.
<br/><br/><br/><br/><br/>

### 2. Image Captioning

![BLIP_8.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/9ae9bac4-33c0-4e23-ac5e-a3da90deac5d){: width="900px"}

NoCaps와 COCO 데이터셋을 사용하여 평가를 진행했다. LM loss로 COCO에 대해 fine-tuning된 모델을 사용하여 평가했다. 표 7에서 볼 수 있듯이 14M pre-training 이미지를 사용하는 BLIP은 비슷한 양의 pre-training 데이터를 사용하는 방법보다 훨씬 더 성능이 뛰어나다. 
<br/><br/><br/><br/><br/>

### 3. Visual Question Answering (VQA)

![BLIP_9.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/97f2eddc-9e3b-4e7f-a80c-4b138538d79a){: width="450px"}

VQA는 모델이 주어진 이미지와 질문에 대한 답변을 예측하는 task이다. VQA를 multi-answer classification task로 보는 대신 open-ended VQA를 가능하게 하는 answer generation task로 간주했다.

그림 5(a)에서 볼 수 있듯이 fine-tuning 중에 image-question이 먼저 multimodal 임베딩으로 인코딩된 다음 answer decoder에 들어가도록 pre-train 모델을 재배열했다. VQA 모델은 ground-truth answer을 target으로 사용하여 LM loss로 fine-tuning되었다.
<br/><br/>

결과는 아래 표와 같다.  BLIP은 13배 더 많은 pre-training 데이터와 더 큰 vision backbone을 사용하는 SimVLM보다 더 나은 성능을 달성했다.

![BLIP_10.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/485f2890-a9ae-4973-ac8e-69d2aff0b510){: width="450px"}
<br/><br/><br/><br/><br/>

### 4. Natural Language Visual Reasoning ($\mathrm{NLVR}^2$)

$\mathrm{NLVR}^2$는 주어진 sentence가 이미지 pair에 대해 설명하는 것인지 모델이 예측하도록 하는 task이다. 두 개의 이미지에 대한 추론을 가능하게 하기 위해 pre-train된 모델을 간단히 수정하여 계산 효율성이 높은 아키텍처로 구현했다. 

그림 5(b)와 같이  image-grounded text encoder의 각 transformer block에는 두 개의 입력 이미지를 처리하기 위한 두 개의 cross-attention layer가 있으며 해당 출력은 merge되어 FFN에 들어간다. Merge layer는 encoder의 처음 6개 layer에서 단순 average pooling을 수행하고, layer 6-12에서는 concat 후 linear projection을 수행했다. MLP classifier는 [Encode] token의 출력 임베딩에 적용되었다. 결과는 위의 표 8과 같다.
<br/><br/><br/><br/><br/>

### 5. Visual Dialog (VisDial)

![BLIP_11.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/58ee9a84-b719-4835-bd0b-70ac5c16741c){: width="500px"}

자연스러운 대화 환경으로 **확장된 VQA**를 수행한다. 모델은  image-question pair뿐만 아니라 dialog history와 이미지의 caption을 고려하여 답변을 예측해야 한다. 여기서 답변 후보 풀의 순위를 매기는 setting을 사용했다. 그림 5(c)와 같이 image와 caption 임베딩을 concat하고 cross-attention를 통해 dialog encoder에 전달한다. 

Dialog encoder는 ITM loss로 학습되어, 전체 dialog history과 이미지 caption 임베딩을 고려할 때 질문에 대한 답변이 참인지 거짓인지 구별한다. 위의 표에서 볼 수 있듯이 BLIP은 SOTA 성능을 달성했다.
<br/><br/><br/><br/><br/>

### 6. Zero-shot Transfer to Video-Language Tasks

![BLIP_12.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/8e051b82-6f59-4c2c-b56c-3af706739750){: width="450px"}

논문의 image-language 모델은 video-language task에 대한 강력한 generalization 능력을 가지고 있다고 한다. 표 10과 표 11에서는 **text-to-video retrieval** 및 **video question answering**으로 zero-shot transfer를 수행하고, 여기서 각각 COCO-retrieval 및 VQA에 대해 학습된 모델을 직접 평가했다. Video 입력을 처리하기 위해 video당 n개의 frame(retrieval의 경우 n = 8, QA의 경우 n = 16)을 균일하게 샘플링하고 frame feature를 single sequence로 concat했다.

위 표의 결과를 확인하면 domain 차이와 temporal modeling 부족에도 불구하고 BLIP은 두 video-language task에서 모두에서 SOTA를 달성했다.
<br/><br/><br/><br/><br/>
