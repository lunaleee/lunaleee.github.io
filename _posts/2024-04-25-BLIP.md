---
title: "[ë…¼ë¬¸ ë¦¬ë·°] BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"
author: lunalee
date: 2024-04-25 20:14:32 +0900
categories: [AI, Paper Review]
tags: [VLP, Detection]
pin: false
math: true
---

<br/><br/>
`Salesforce Research` `ICML 2022`

- Paper: [https://arxiv.org/abs/2201.12086](https://arxiv.org/abs/2201.12086)
- Git: [https://github.com/salesforce/BLIP](https://github.com/salesforce/BLIP)
- Page: [https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/](https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/)
<br/><br/><br/><br/><br/>

# Introduction

---

**VLP(Vision-Language Pre-training)**ì€ ìµœê·¼ ë‹¤ì–‘í•œ multimodal downstream taskì— ëŒ€í•´ ì„±ê³µì„ ê±°ë‘ì—ˆì§€ë§Œ, ë‘ ê°€ì§€ í•œê³„ì ì„ ê°€ì§€ê³  ìˆë‹¤. 

1. Model ê´€ì : ëŒ€ë¶€ë¶„ì˜ ë°©ë²•ì€ encoder-based model ë˜ëŠ” encoder-decoder modelì„ ì‚¬ìš©í•œë‹¤. Encoder-based modelì€  text generation task(e.g. image captioning)ë¡œ ì§ì ‘ ì‚¬ìš©ë˜ê¸° ì–´ë ¤ìš´ ë¬¸ì œê°€ ìˆê³ , encoder-decoder modelì€  image-text ê²€ìƒ‰ taskì—ì„œ ì˜ ë™ì‘í•˜ì§€ ì•ŠëŠ” ë¬¸ì œê°€ ìˆë‹¤.
2. Data ê´€ì : ë‹¤ì–‘í•œ ê¸°ì¡´ SOTA ë°©ë²•ë“¤ì€ ì›¹ì—ì„œ ìˆ˜ì§‘í•œ image-text pairë¥¼ ì‚¬ìš©í•˜ê³  ìˆë‹¤. ì´ëŸ¬í•œ ë°ì´í„°ëŠ” noisyí•˜ë‹¤ëŠ” ë¬¸ì œê°€ ìˆë‹¤.
<br/><br/><br/>

ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í†µí•©ëœ vision-language understanding/generation ëª¨ë¸ì¸ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>BLIP: Bootstrapping Language- Image Pre-training</span></mark>**ì„ ì œì•ˆí•œë‹¤. BLIPì€ ê¸°ì¡´ì˜ ë°©ë²•ë³´ë‹¤ ë” ë„“ì€ ë²”ìœ„ì˜ downstream taskë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ìƒˆë¡œìš´ VLP frameworkì´ë‹¤. ì£¼ìš” contributionì€ ì•„ë˜ì™€ ê°™ë‹¤.

1. MED(Multimodal mix of Encoder-Decoder): íš¨ê³¼ì ì¸ multi-task pre-training ë° ìœ ì—°í•œ transfer learningì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ìƒˆë¡œìš´ ëª¨ë¸ êµ¬ì¡° ì œì•ˆ.
2. Captioning and Filtering (CapFilt): noisyí•œ image-text pairì—ì„œ í•™ìŠµí•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ boostrapping ë°©ë²• ì œì•ˆ.
    
    ![BLIP_1.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/6cf84739-2265-48d6-a2e1-399f700d8d92){: width="700px"}
    
<br/><br/><br/><br/><br/><br/>

# Method

---

## 1. Model Architecture

Understandingê³¼ generation ëŠ¥ë ¥ì„ ëª¨ë‘ ê°–ì¶˜ í†µí•© ëª¨ë¸ì„ pre-trainí•˜ê¸° ìœ„í•´ì„œ, ì„¸ ê°€ì§€ íŠ¹ì§•ì— ëŒ€í•´ ë™ì‘í•˜ëŠ” **multi-task** ëª¨ë¸ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>multimodal mixture of encoder-decoder(MED)</span></mark>**ë¥¼ ì œì•ˆí•˜ì˜€ë‹¤. Image Encoderë¡œëŠ” ViT(Vision Transformer)ë¥¼ ì‚¬ìš©í•˜ê³  global image featureë¥¼ ë‚˜íƒ€ë‚´ëŠ” [CLS] tokenì„ ì¶”ê°€í•œë‹¤.
<br/><br/>

1. **Unimodal encoder** : ì´ë¯¸ì§€ì™€ í…ìŠ¤í‹€ë¥´ ë³„ë„ë¡œ ì¸ì½”ë”©. í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¡œëŠ” BERTì— ì‚¬ìš©. ì´ë¯¸ì§€ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ë¬¸ì¥ ìš”ì•½ì„ ìœ„í•œ [CLS] tokenì´ text inputì— ì¶”ê°€ëœë‹¤.
2. **Image-grounded text encoder** : Text Encoderì˜ ê° transformer blockì—ì„œ, self-attention (SA) layerì™€ feed forward network (FFN) ì‚¬ì´ì— **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>cross-attention (CA)ì„ ì¶”ê°€</span></mark>**í•˜ì—¬ **visual information**ì„ ì£¼ì…í•¨. Task-specificí•œ [Encode] tokenì„ textì— ì¶”ê°€í•˜ê³  [Encode]ì˜ output embeddingì´ text-image pairì˜ multimodal representationìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.
3. **Image-grounded text decoder** : Image-grounded text encoderì˜ directional self-attention layerë¥¼ **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>causal self-attention layerë¡œ ëŒ€ì²´</span></mark>**. [Decode] tokenì€ sequenceì˜ ì‹œì‘ì„ ì•Œë¦¬ëŠ” ì‹œê·¸ë„ë¡œ ì‚¬ìš©ë˜ê³  end-of-sequence tokenì€ signalì˜ ëì„ ì•Œë¦¬ëŠ”ë° ì‚¬ìš©ëœë‹¤.
<br/><br/><br/><br/><br/>

## 2. Pre-training Objectives

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” pre-training ì¤‘ ë‘ ê°œì˜ **understanding-based objective**, í•˜ë‚˜ì˜ **generation-based objective**ë¡œ ì´ ì„¸ ê°€ì§€ objectiveì— ëŒ€í•´ í•™ìŠµì„ ì§„í–‰í•œë‹¤. ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ pairì—ì„œ ì´ë¯¸ì§€ì— ëŒ€í•´ì„œëŠ” í•˜ë‚˜ì˜ forward pass, í…ìŠ¤íŠ¸ì— ëŒ€í•´ì„œëŠ” ì„¸ ê°œì˜ forward passê°€ ì¡´ì¬í•œë‹¤.

![BLIP_2.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/8bd710fc-4245-43af-9c29-ba3139835e05){: width="1100px"}

<br/><br/>

#### 1. Image-Text Contrastive Loss (ITC)

ITCëŠ” **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>unimodal encoder</span></mark>**ë¥¼ í™œì„±í™”í•œë‹¤. ì¦‰, Image encoderì˜ ì„ë² ë”©ê³¼ Text encoderì˜ ì„ë² ë”©ì„ ì‚¬ìš©í•œë‹¤. Image-text pairê°€ positive pairì¸ ê²½ìš°ì—ëŠ” similarityê°€ ë†’ì•„ì§€ê³  negative pairì¸ ê²½ìš°ì—ëŠ” similarityê°€ ë‚®ì•„ì§€ëŠ” **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Contrastive Learning</span></mark>**ì„ í™œìš©í•˜ì˜€ë‹¤. ì´ í•™ìŠµ ë°©ë²•ìœ¼ë¡œ image transformerì™€ text transformerì˜ feature spaceë¥¼ ì˜ aligní•˜ë„ë¡ í•™ìŠµí•œë‹¤.
<br/><br/>

[ALBEFğŸ“„](https://arxiv.org/abs/2107.07651) ë…¼ë¬¸ì˜ ITC Lossë¥¼ ì‚¬ìš©í•˜ë©°, í•´ë‹¹ ë…¼ë¬¸ì—ì„œ noisyí•œ ì›¹ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ì œì•ˆí•œ **momentum encoder**ë¥¼ ë„ì…í•˜ì—¬ soft labelì„ ìƒì„±, í•™ìŠµì— í™œìš©í–ˆë‹¤.
<br/><br/>

> **Momentum Encoder**<br/>
web ë°ì´í„°ëŠ” positive pairì— ëŒ€í•´ì„œ weakly-correlated ì¸ ê²½ìš°, negative textì„ì—ë„ imageì™€ ë§¤ì¹­ë˜ëŠ” ë“± noisyí•œ ë¬¸ì œê°€ ë§ë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ìœ„í•´ momentum ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ **pseudo-target**ì„ ë§Œë“¤ì–´, ê¸°ì¡´ ont-hot labelì˜ ë¬¸ì œì ì„ ë³´ì™„í•œë‹¤.<br/>
momentum ëª¨ë¸ì€ ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸(unimodel and multimodal encoder)ì„ ê°€ì ¸ì™€ exponential-moving-average(EMA)ë¡œ í•™ìŠµí•œ ë²„ì „ì´ë‹¤. ITC Lossë¥¼ ë³€í˜•í•˜ì—¬ targetì— ëŒ€í•´ **soft label**ì„ ìƒì„±í•˜ë„ë¡ í•œë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ë…¼ë¬¸ì„ ì°¸ì¡°í•´ë³´ì.[ğŸ“„](https://arxiv.org/abs/2107.07651)
> 

<br/><br/>

#### 2. Image-Text Matching Loss (ITM)

ITMì€ **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>image-grounded text encoder</span></mark>**ë¥¼ í™œì„±í™”í•œë‹¤. Visionê³¼ language ì‚¬ì´ì˜ ì„¸ë°€í•œ alignmentë¥¼ í¬ì°©í•˜ëŠ” image-text multimodal representationì„ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. 

ITMì€ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>binary classification task</span></mark>** ì´ë‹¤. Multimodal featureê°€ ì£¼ì–´ì§€ë©´,  ITM head(linear layer)ë¥¼ ì‚¬ìš©í•˜ì—¬ image-text pairê°€ positive(matched)ì¸ì§€ negative (unmatched)ì¸ì§€ ì˜ˆì¸¡í•œë‹¤. 

**hard negative mining strategy**ë¥¼ ì‚¬ìš©í–ˆë‹¤.
<br/><br/><br/>

#### 3. Language Modeling Loss (LM)

LMì€ **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>image-grounded text decoder</span></mark>**ë¥¼ í™œì„±í™”í•˜ì—¬, ì´ë¯¸ì§€ê°€ ì£¼ì–´ì¡Œì„ ë•Œ textual descriptionì„ ìƒì„±í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. Autoregressive ë°©ì‹ìœ¼ë¡œ, Image ì„ë² ë”©ì— ëŒ€í•´ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>í…ìŠ¤íŠ¸ì˜ likelihoodë¥¼ maximize</span></mark>** í•˜ë„ë¡ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” cross entropy lossë¥¼ ì‚¬ìš©í•œë‹¤. Lossë¥¼ ê³„ì‚°í•  ë•Œ label smoothing 0.1ì„ ì ìš©í•œë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ VLPì— ë§ì´ ì‚¬ìš©ë˜ëŠ” MLM lossì™€ ë¹„êµí•˜ì—¬ LMì€ ëª¨ë¸ì´ generalization ëŠ¥ë ¥ì„ ê°–ê²Œ í•˜ë©°, ê·¸ ê²°ê³¼ visual informationì„ ì¼ê´€ëœ captionìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.
<br/><br/><br/><br/>

Multi-task learningì—ì„œ íš¨ìœ¨ì ì¸ pre-trainingì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ text encoderì™€ text decoderëŠ” SA layerë¥¼ ì œì™¸í•œ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ë¥¼ ê³µìœ í•œë‹¤. SA layerì—ì„œ parameter sharingì„ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ” ì´ìœ ëŠ” Encoding ì‘ì—…ê³¼ Decoding ì‘ì—…ì˜ ì°¨ì´ê°€ SA layerì—ì„œ ê°€ì¥ ì˜ í¬ì°©ë˜ê¸° ë•Œë¬¸ì´ë¼ê³  í•œë‹¤. ë°˜ë©´ embedding layer, CA layer, FFNì€ ë‘ ì‘ì—…ì—ì„œ ìœ ì‚¬í•˜ê²Œ ë™ì‘í•˜ë¯€ë¡œ ê³µìœ ë¥¼ í†µí•´ í•™ìŠµ íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¨ë‹¤. 

- Encoder SA layer: **í˜„ì¬** ì…ë ¥ tokenì— ëŒ€í•œ representationì„ ìƒì„±í•˜ê¸° ìœ„í•œ **bi-directional **self-attention**
- Decoder SA layer: **ë‹¤ìŒ** tokenì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ **causal self-attention**
<br/><br/><br/><br/><br/>

## 3. CapFilt

ê³ í’ˆì§ˆì˜ human-annotated image-text pair $\{(I_h, T_h)\}$ëŠ” ìˆ˜ê°€ ì œí•œë˜ì–´ ìˆìœ¼ë¯€ë¡œ ìµœê·¼ì˜ ì—°êµ¬ì—ì„œëŠ” ì›¹ì—ì„œ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” ëŒ€ìš©ëŸ‰ì˜ image-text pair $\{(I_w, T_w)\}$ë¥¼ í™œìš©í•œë‹¤. ê·¸ëŸ¬ë‚˜ ì›¹ì—ì„œ ìˆ˜ì§‘ë˜ëŠ” í…ìŠ¤íŠ¸ëŠ” **noisy**í•œ ê²½ìš°ê°€ ë§ì•„ vision-language alignmentì— ì í•©í•˜ì§€ ì•Šë‹¤.
<br/><br/>

![BLIP_3.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/7c8ee9de-4404-446f-a859-5a92110d2e09){: width="1200px"}

ì €ìëŠ” text ë°ì´í„°ì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²• **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Captioning and Filtering (CapFilt)</span></mark>**ì„ ì œì•ˆí•˜ì˜€ë‹¤. CapFiltëŠ” ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë‘ ê°€ì§€ ëª¨ë“ˆë¡œ êµ¬ì„±ë˜ì–´ìˆë‹¤.

- **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>Captioner</span></mark>**: ì›¹ ì´ë¯¸ì§€ì— ëŒ€í•œ captionì„ ìƒì„±
    - image-grounded text decoder
    - ì´ë¯¸ì§€ê°€ ì£¼ì–´ì¡Œì„ ë•Œ í…ìŠ¤íŠ¸ë¥¼ decoding í•˜ê¸° ìœ„í•´ **LM objective**ë¡œ pre-trainë¨
    - ì›¹ ì´ë¯¸ì§€ $I_w$ê°€ ì£¼ì–´ì§€ë©´ í•©ì„± ìº¡ì…˜ $T_w$ ìƒì„±
- **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>Filter</span></mark>**: noiseê°€ ìˆëŠ” image-text pair ì œê±°
    - image-grounded text encoder
    - í…ìŠ¤íŠ¸ê°€ ì´ë¯¸ì§€ì™€ ë§¤ì¹­ë˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ ì•Œì•„ë³´ê¸° ìœ„í•´ **ITC, ITM objective**ë¡œ pre-trainë¨
    - ì›ë³¸ ì›¹ í…ìŠ¤íŠ¸ $T_w$ì™€ í•©ì„± í…ìŠ¤íŠ¸ $T_s$ ëª¨ë‘ì—ì„œ noisyí•œ í…ìŠ¤íŠ¸ë¥¼ ì œê±°: ITM headê°€ í…ìŠ¤íŠ¸ë¥¼ ì´ë¯¸ì§€ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ ì˜ˆì¸¡í•˜ë©´ í…ìŠ¤íŠ¸ê°€ noisyí•˜ë‹¤ê³  íŒë‹¨
    - ë§ˆì§€ë§‰ìœ¼ë¡œ í•„í„°ë§ëœ image-text pairë¥¼ human-annotated pairì™€ ê²°í•©í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„° ì„¸íŠ¸ë¥¼ í˜•ì„±í•˜ê³ , ëª¨ë¸ì„ ìƒˆë¡œ pre-trainí•˜ëŠ” ë° ì‚¬ìš©í•¨
<br/><br/>

captionerì™€ filterëŠ” ëª¨ë‘ pre-trained MED ëª¨ë¸ì—ì„œ initialize ë˜ê³  COCO ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ê°œë³„ì ìœ¼ë¡œ fine-tuning ëœë‹¤.
<br/><br/><br/><br/><br/><br/>

# Experiments and Discussions

---

## 1. Pre-training Details

- Image transformer: ImageNetì—ì„œ pre-trainëœ ViTë¡œ initialize ë¨
- Text transformer: BERTbaseì—ì„œ initialize ë¨
- pre-training ì¤‘ì— 224 Ã— 224 resolutionìœ¼ë¡œ random image crop ì ìš©, fine-tuning ì¤‘ì— image resolutionì„ 384 Ã— 384ë¡œ ë†’ì„
- pre-training dataset: ì´ 14M ì´ë¯¸ì§€, 2ê°€ì§€ human-annotated ë°ì´í„°ì…‹(COCO, Visual Genome)ê³¼ 3ê°€ì§€ web ë°ì´í„°ì…‹(Conceptual Captions, Conceptual 12M, SBU captions)ì„ í¬í•¨
- ë” noisyí•œ textê°€ í¬í•¨ëœ 115M ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ëŠ” ì¶”ê°€ web ë°ì´í„°ì…‹ LAIONì— ëŒ€í•´ì„œë„ ì‹¤í—˜í•¨
<br/><br/><br/><br/><br/>

## 2. Effect of CapFilt

![BLIP_4.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/4a266343-98cb-4ede-8d4e-a6f776159687){: width="900px"}

ìœ„ì˜ í‘œëŠ” image-text retrieval(ê²€ìƒ‰) ë°  image captioningì„ í¬í•¨í•œ down-stream taskì—ì„œ CapFiltì˜ íš¨ìœ¨ì„±ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ pre-trainëœ ëª¨ë¸ì„ ë¹„êµí•œ ê²°ê³¼ì´ë‹¤.

14M ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ë°ì´í„°ì…‹ì— captionerë‚˜ filter ì¤‘ í•˜ë‚˜ë¥¼ ì ìš©í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ë‘˜ ë‹¤ ì ìš©í•˜ë©´ íš¨ê³¼ê°€ ë³´ì™„ë˜ì–´ ìƒë‹¹í•œ ê°œì„ ì´ ì´ë£¨ì–´ì§„ë‹¤. 
<br/><br/>

ìœ„ì˜ ê·¸ë¦¼ì€ captionerê°€ ìƒˆë¡œìš´ textual descriptionì„ ìƒì„±í•˜ëŠ” íš¨ê³¼ì™€ ì›ë³¸ web í…ìŠ¤íŠ¸ì™€ í•©ì„± í…ìŠ¤íŠ¸ ëª¨ë‘ì—ì„œ noise captionì„ ì œê±°í•˜ëŠ” filterì˜ íš¨ê³¼ì— ëŒ€í•œ ì´ë¯¸ì§€ì´ë‹¤.
<br/><br/><br/><br/><br/>

## 3. Parameter Sharing and Decoupling

![BLIP_5.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/4e7f7ae8-c06c-4ce0-9ea7-8824408d3335){: width="800px"}

Pre-train ì¤‘ text encoderì™€ decoderëŠ” self-attention layerë¥¼ ì œì™¸í•œ ëª¨ë“  paremeterë¥¼ ê³µìœ í•œë‹¤. ìœ„ì˜ í‘œì—ì„œëŠ” web í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ 14M ì´ë¯¸ì§€ì— ëŒ€í•´ **ë‹¤ì–‘í•œ parameter sharing strategy**ìœ¼ë¡œ pre-trainëœ ëª¨ë¸ì„ í‰ê°€í•œ ê²°ê³¼ì´ë‹¤.
<br/><br/><br/><br/><br/><br/>

# Comparison with State-of-the-arts

---

### 1. Image-Text Retrieval

![BLIP_6.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/eb5cf993-fae3-4d91-b61c-3b0c4ba1fda6){: width="1000px"}

![BLIP_7.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/1d59fbb5-2e55-4933-840d-d8ac0b2649b0){: width="500px"}

COCO, Flickr30K ë°ì´í„°ì…‹ì—ì„œ image-to-text ê²€ìƒ‰(TR), text-to-image ê²€ìƒ‰(IR)ì— ëŒ€í•´ BLIPì„ í‰ê°€í–ˆë‹¤.  ITC ë° ITM Lossì„ ì‚¬ìš©í•˜ì—¬ pre-trainëœ ëª¨ë¸ì„ fine-tuningí–ˆë‹¤. í‘œ 5ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ BLIPì€ ê¸°ì¡´ ëª¨ë¸ê³¼ ë¹„êµí•˜ì—¬ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í–ˆë‹¤. 

ë˜í•œ COCOì—ì„œ fine-tuningëœ ëª¨ë¸ì„ Flickr30Kë¡œ transferí•˜ì—¬ zero-shot retrievaì„ ìˆ˜í–‰í–ˆë‹¤. ê²°ê³¼ëŠ” í‘œ 6ì— ë‚˜ì™€ ìˆìœ¼ë©°, ì—¬ê¸°ì„œë„ BLIPì€ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ í›¨ì”¬ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆë‹¤.
<br/><br/><br/><br/><br/>

### 2. Image Captioning

![BLIP_8.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/9ae9bac4-33c0-4e23-ac5e-a3da90deac5d){: width="900px"}

NoCapsì™€ COCO ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ í‰ê°€ë¥¼ ì§„í–‰í–ˆë‹¤. LM lossë¡œ COCOì— ëŒ€í•´ fine-tuningëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í‰ê°€í–ˆë‹¤. í‘œ 7ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ 14M pre-training ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ëŠ” BLIPì€ ë¹„ìŠ·í•œ ì–‘ì˜ pre-training ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë³´ë‹¤ í›¨ì”¬ ë” ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ë‹¤. 
<br/><br/><br/><br/><br/>

### 3. Visual Question Answering (VQA)

![BLIP_9.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/97f2eddc-9e3b-4e7f-a80c-4b138538d79a){: width="450px"}

VQAëŠ” ëª¨ë¸ì´ ì£¼ì–´ì§„ ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì˜ˆì¸¡í•˜ëŠ” taskì´ë‹¤. VQAë¥¼ multi-answer classification taskë¡œ ë³´ëŠ” ëŒ€ì‹  open-ended VQAë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” answer generation taskë¡œ ê°„ì£¼í–ˆë‹¤.

ê·¸ë¦¼ 5(a)ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ fine-tuning ì¤‘ì— image-questionì´ ë¨¼ì € multimodal ì„ë² ë”©ìœ¼ë¡œ ì¸ì½”ë”©ëœ ë‹¤ìŒ answer decoderì— ë“¤ì–´ê°€ë„ë¡ pre-train ëª¨ë¸ì„ ì¬ë°°ì—´í–ˆë‹¤. VQA ëª¨ë¸ì€ ground-truth answerì„ targetìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ LM lossë¡œ fine-tuningë˜ì—ˆë‹¤.
<br/><br/>

ê²°ê³¼ëŠ” ì•„ë˜ í‘œì™€ ê°™ë‹¤.  BLIPì€ 13ë°° ë” ë§ì€ pre-training ë°ì´í„°ì™€ ë” í° vision backboneì„ ì‚¬ìš©í•˜ëŠ” SimVLMë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆë‹¤.

![BLIP_10.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/485f2890-a9ae-4973-ac8e-69d2aff0b510){: width="450px"}
<br/><br/><br/><br/><br/>

### 4. Natural Language Visual Reasoning ($\mathrm{NLVR}^2$)

$\mathrm{NLVR}^2$ëŠ” ì£¼ì–´ì§„ sentenceê°€ ì´ë¯¸ì§€ pairì— ëŒ€í•´ ì„¤ëª…í•˜ëŠ” ê²ƒì¸ì§€ ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ë„ë¡ í•˜ëŠ” taskì´ë‹¤. ë‘ ê°œì˜ ì´ë¯¸ì§€ì— ëŒ€í•œ ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸° ìœ„í•´ pre-trainëœ ëª¨ë¸ì„ ê°„ë‹¨íˆ ìˆ˜ì •í•˜ì—¬ ê³„ì‚° íš¨ìœ¨ì„±ì´ ë†’ì€ ì•„í‚¤í…ì²˜ë¡œ êµ¬í˜„í–ˆë‹¤. 

ê·¸ë¦¼ 5(b)ì™€ ê°™ì´  image-grounded text encoderì˜ ê° transformer blockì—ëŠ” ë‘ ê°œì˜ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë‘ ê°œì˜ cross-attention layerê°€ ìˆìœ¼ë©° í•´ë‹¹ ì¶œë ¥ì€ mergeë˜ì–´ FFNì— ë“¤ì–´ê°„ë‹¤. Merge layerëŠ” encoderì˜ ì²˜ìŒ 6ê°œ layerì—ì„œ ë‹¨ìˆœ average poolingì„ ìˆ˜í–‰í•˜ê³ , layer 6-12ì—ì„œëŠ” concat í›„ linear projectionì„ ìˆ˜í–‰í–ˆë‹¤. MLP classifierëŠ” [Encode] tokenì˜ ì¶œë ¥ ì„ë² ë”©ì— ì ìš©ë˜ì—ˆë‹¤. ê²°ê³¼ëŠ” ìœ„ì˜ í‘œ 8ê³¼ ê°™ë‹¤.
<br/><br/><br/><br/><br/>

### 5. Visual Dialog (VisDial)

![BLIP_11.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/58ee9a84-b719-4835-bd0b-70ac5c16741c){: width="500px"}

ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” í™˜ê²½ìœ¼ë¡œ **í™•ì¥ëœ VQA**ë¥¼ ìˆ˜í–‰í•œë‹¤. ëª¨ë¸ì€  image-question pairë¿ë§Œ ì•„ë‹ˆë¼ dialog historyì™€ ì´ë¯¸ì§€ì˜ captionì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€ì„ ì˜ˆì¸¡í•´ì•¼ í•œë‹¤. ì—¬ê¸°ì„œ ë‹µë³€ í›„ë³´ í’€ì˜ ìˆœìœ„ë¥¼ ë§¤ê¸°ëŠ” settingì„ ì‚¬ìš©í–ˆë‹¤. ê·¸ë¦¼ 5(c)ì™€ ê°™ì´ imageì™€ caption ì„ë² ë”©ì„ concatí•˜ê³  cross-attentionë¥¼ í†µí•´ dialog encoderì— ì „ë‹¬í•œë‹¤. 

Dialog encoderëŠ” ITM lossë¡œ í•™ìŠµë˜ì–´, ì „ì²´ dialog historyê³¼ ì´ë¯¸ì§€ caption ì„ë² ë”©ì„ ê³ ë ¤í•  ë•Œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ì°¸ì¸ì§€ ê±°ì§“ì¸ì§€ êµ¬ë³„í•œë‹¤. ìœ„ì˜ í‘œì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ BLIPì€ SOTA ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆë‹¤.
<br/><br/><br/><br/><br/>

### 6. Zero-shot Transfer to Video-Language Tasks

![BLIP_12.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/8e051b82-6f59-4c2c-b56c-3af706739750){: width="450px"}

ë…¼ë¬¸ì˜ image-language ëª¨ë¸ì€ video-language taskì— ëŒ€í•œ ê°•ë ¥í•œ generalization ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆë‹¤ê³  í•œë‹¤. í‘œ 10ê³¼ í‘œ 11ì—ì„œëŠ” **text-to-video retrieval** ë° **video question answering**ìœ¼ë¡œ zero-shot transferë¥¼ ìˆ˜í–‰í•˜ê³ , ì—¬ê¸°ì„œ ê°ê° COCO-retrieval ë° VQAì— ëŒ€í•´ í•™ìŠµëœ ëª¨ë¸ì„ ì§ì ‘ í‰ê°€í–ˆë‹¤. Video ì…ë ¥ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ videoë‹¹ nê°œì˜ frame(retrievalì˜ ê²½ìš° n = 8, QAì˜ ê²½ìš° n = 16)ì„ ê· ì¼í•˜ê²Œ ìƒ˜í”Œë§í•˜ê³  frame featureë¥¼ single sequenceë¡œ concatí–ˆë‹¤.

ìœ„ í‘œì˜ ê²°ê³¼ë¥¼ í™•ì¸í•˜ë©´ domain ì°¨ì´ì™€ temporal modeling ë¶€ì¡±ì—ë„ ë¶ˆêµ¬í•˜ê³  BLIPì€ ë‘ video-language taskì—ì„œ ëª¨ë‘ì—ì„œ SOTAë¥¼ ë‹¬ì„±í–ˆë‹¤.
<br/><br/><br/><br/><br/>
