---
title: "[논문 리뷰] LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models (LMD)"
author: lunalee
date: 2024-05-23 20:33:21 +0900
categories: [AI, Paper Review]
tags: [LLM, Diffusion, Generation]
pin: false
math: true
---

<br/><br/>
`TMLR 2024`

- Paper: [https://arxiv.org/abs/2305.13655](https://arxiv.org/abs/2305.13655)
- Git: [https://github.com/TonyLianLong/LLM-groundedDiffusion](https://github.com/TonyLianLong/LLM-groundedDiffusion)
- Page: [https://llm-grounded-diffusion.github.io](https://llm-grounded-diffusion.github.io/)
<br/><br/><br/>

#### 📖 핵심 훑어보기 !!

- Text-to-Image generation에서 복잡한 text prompt(e.g. 개체 수, 공간적 속성) 내용을 정확하게 생성해내기 위해 2-stage generation 방법을 도입하였다.
    1. **Text-grounded layout generation**: in-context learning을 활용하여 prompt에 맞는 이미지 layout을 먼저 생성
    2. **Layout-grounded image generation**: 1 단계에서 생성한 layout을 바탕으로 이미지 생성
- Layout-grounded image generation 단계에서 기존의 diffusion 모델을 활용하는 **training-free 방법** 제안. Denoising process에서 cross-attention map을 활용하여 layout에 맞는 위치에 object를 생성하도록 가이드를 주는 방식을 사용.
<br/><br/><br/><br/>

# Introduction

---

Text-to-Image generation은 Diffusion 모델의 등장 이후 현실적이고 다양한 이미지를 생성해내며 많은 발전을 이루었다. 하지만 diffusion 모델은 복잡한 text prompt의 내용을 정확하게 생성해내는데 어려움을 겪고 있다. 

예를 들어, 아래 그림을 살펴보자. 바나나 없는 테이블을 생성하라고 했지만 테이블 위의 바나나 그림을 생성했고, 고양이 3마리를 생성하라고 했지만 4마리의 고양이를 생성했다. 이와 같이 모델의 결과가 특정 수의 객체 생성에 실패했고, ‘without’과 같은 부정 어휘를 인식하지 못했다. 이 외에도 공간적인 속성을 객체와 올바르게 연관짓는데 실패한 결과를 보여주고 있다. 

![LMD_1.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/7f3fc350-f472-463e-9625-8e8e04834556){: width="1100px"}
<br/><br/>

이러한 문제를 해결하기 위해 복잡한 캡션을 포함하는 multimodal 데이터셋을 수집하고 대용량 text-image 데이터를 학습시키는 방법이 있지만, 이러한 방법을 위해서는 데이터셋을 큐레이션 하는데 상당한 시간과 자원이 필요하다. 
<br/><br/>

![LMD_2.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/b6212f4a-10e6-4faf-96ff-539ae8f86a36){: width="1100px"}

따라서 본 논문에서는 더 높은 수준의 prompt 이해를 위한 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>grounding</span></mark>**을 제공하는 LLM을 사용하는 새로운  training-free 방법을 제안했다. 이 방법은 **L**L**M**-grounded **D**iffusion (LMD) 이라고 하며, 2-stage generation process로 구성되어있다.

1. **Text-grounded layout generation:** in-context learning을 통해 LLM을 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>text-grounded layout generator</span></mark>**로 학습하는 과정. 이미지에 관한 prompt가 주어지면 LLM이 이를 고려하여 bbox 형태로 scene 레이아웃을 생성함
2. **Layout-grounded image generation**: 첫 번째 단계에서 생성된 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>grounding 레이아웃을 따라 diffusion 모델이 이미지를 생성</span></mark>**하게 하는 새로운 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>controller</span></mark>** 도입
<br/><br/><br/>

특히 위의 두 단계는 모두 기존에 학습된 frozen pretrained 모델을 사용하여, LLM과 diffusion 모델 재학습 없이 독립적으로 학습된 각각의 모델을 사용하여 적용할 수 있다. 
<br/><br/><br/><br/><br/><br/>

# Method (LLM-grounded Diffusion)

---

## 1. LLM-based Layout Generation

![LMD_3.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/008d9884-b62c-4eaf-9839-e9365ada9e5f){: width="1300px"}

논문에서는 이미지 레이아웃을 생성하기 위해 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>In-Context Learning</span></mark>***을 활용한다. 위 그림과 같이 입력 caption(prompt)을 특정한 templete에 맞게 바꿔준 뒤, LLM에게 query하여 LLM Completion을 획득한다. 이 때 LLM에 query하는 내용은 prompt 문장을 보고 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>Layout representation</span></mark>**을 생성하는 것이다. Layout representation은 다음과 같이 2개(또는 3개)의 요소로 구성되어 있다. 

1. caption에 포함되어 있는 각각의 foreground object에 대한 bounding box: (x, y, width, height) **format으로 구성된 coordinate 정보.
2. 이미지 background를 설명하는 간결한 caption.

(3. negative prompt: 생성된 이미지에 포함되지 말아야할 것. 선택적으로 존재하는 경우에만. 이외에는 비워둔다.)
<br/><br/>

> **In-Context Learning*** <br/>
> In-Context Learning은 Meta Learning의 일종으로, fine-tuning과 같은 별도의 모델을 학습을 거치지 않고, inference 단계에서 prompt를 잘 생성하여줌으로서 맥락적인 의미를 모델이 파악하게 하여 답변을 생성하게 하는 것을 의미한다.
> 

<br/><br/><br/>

LLM에 query하기 위해 caption을 특정 templete으로 변환하는 과정을 살펴보자. Templete에는 몇가지 지시사항(instruction)을 추가한다. Instruction은 두개의 부분으로 구성되어있다.<br/>
![LMD_4.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/0068a4e4-4bc0-44c9-9611-aa751a0cd700){: width="1300px"}<br/>
1. Task specification: Task에 대한 설명
2. Supporting details: 이미지 형식 등의 상세 정보
<br/><br/><br/><br/>

#### In-Context Learning.

저자는 위의 task description 뒤에 아래와 같이 수동으로 큐레이션한 예제를 추가하여 LLM에 제공한다. 이를 통해 LLM이 모호함을 극복하도록 하고 명확한 답을 생성할 수 있게 한다.<br/>
![LMD_5.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/b96cda2d-541f-49b2-9ce4-e2daff83d9e8){: width="1200px"}
<br/><br/><br/>

여기에 보다 정확한 layout control을 위해, 저자는 2가지 원칙을 추가하였다. 

1. 각각의 object instance는 단일 bounding box로 표현된다.
(e.g. prompt에 사과 4개가 언급되면 “사과” caption이 있는 bbox 4개를 생성한다.)
2. 모든 foreground object가 레이아웃에 포함되도록 하고 background caption에 남겨두지 않는다.
<br/><br/><br/>

⇒ 이렇게 생성된 full prompt에 대한 예시 (LLM input)<br/>
![LMD_6.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/36ba9513-20bb-4678-8ba3-fc739a0db082){: width="1000px"}<br/>
위와 같은 prompt를 제공하면 LLM은 “Objects:” 부터 Completion을 시작한다.
<br/><br/><br/><br/>

#### LLM Completion.

in-context 예제가 제공된 후, LLM Completion은 아래와 같은 형식으로 시작된다.<br/>
![LMD_7.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/5c6188ae-9571-4f6d-a885-2b4346723e64){: width="1300px"}<br/>
LLM Completion의 결과 레이아웃은 parsing되고 후속 이미지 생성 프로세스에 사용된다.
<br/><br/><br/>

⇒ In-context 결과 예시 (LLM Completion)<br/>
![LMD_8.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/81639c38-8e3f-4130-a8b6-5681fd706c93){: width="1000px"}
<br/><br/><br/><br/><br/>

## 2. Layout-grounded Stable Diffusion

먼저, LDM의 generation 방법을 살펴보기 전에, 2023년 발표된 BoxDiff 논문의 일부를 살펴보자.
<br/><br/>

> [**BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion**📄](https://arxiv.org/abs/2307.10816)<br/>
> 해당 논문에서는 Box와 같은 사용자가 제공하는 조건에 맞는 이미지를 생성하는 것을 목표로 한다. 주어진 spatial condition을 준수하는 이미지를 생성하기 위해, 합성 이미지에서 object와 context를 제어하는 training-free method를 제안한다.<br/>
> ![LMD_9.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/ec7f2dea-e8aa-4001-8560-ea912fb62676){: width="600px"}
>
> 먼저, box condition을 constraint로 주기 위해, 해당 논문에서는 text와 image feature의 cross-attention map에 주목한다. <br/>
> 위의 그림에서 볼 수 있듯이 Stable Diffusion 모델의 denoising step에서, cross-attention map의 높은 response region의 위치와 크기는 디코딩된 이미지 $x$의 object(panda, snowboard)와 지각적으로 동일하다. 이는 이미지 $x$에서 대상 object의 생성을 제어하기 위해 cross-attention에 제약 조건을 추가할 수 있다는 동기를 부여한다.
> 
> <br/>
> ![LMD_10.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/165a6566-7db1-40dc-94a5-698cb7db20c8){: width="1400px"}
>
> 따라서 합성된 객체의 위치와 크기가 box condition에 충족하게 하기 위해, latent $z_t$를 점진적으로 업데이트하는 과정에서 cross-attention map에 대해 세 가지 spatial constraint를 제안한다.
> 
> **1. Inner-Box Constraint**: object가 box 위치에 생성되도록, cross-attention의 높은 response가 마스크 영역에만 있도록 상자 내의 response를 최대화하는 조건.<br/>
> **2. Outer-Box Constraint**: object가 box 영역 밖으로 생성되는 것을 방지하기 위해, cross-attention의 높은 response가 마스크 영역 밖에 생성되는 것을 최소화하는 조건(1-Mask).<br/>
> **3. Corner Constraint**: object의 경계 픽셀을 좀 더 명확하게 제한하기 위한 조건.
> <br/>
> 
> LMD에서는 마찬가지로 In-Context로 생성된 box 안에 객체가 생성되는 것을 목적으로 하므로, Box Diff의 Inner-Box Constraint와 Outer-Box Constraint를 활용한다.
> 

<br/><br/><br/>

이제 Layout-grounded image generation stage를 살펴보자. 이 단계에서는 LLM에서 생성된 레이아웃을 기반으로 이미지 생성을 구축하는 컨트롤러를 소개한다. 

Diffusion 모델은 instance-level 구분을 위한 세밀한 control 능력은 부족하지만, 하나의 instance에 대한 생성에 유리하다. 따라서 논문에서는 instance-level grounding을 위해 한 번에 하나의 foreground box를 처리한다. Generation stage 각 step별로 자세한 과정을 살펴보자.
<br/><br/>

![LMD_11.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/34fc4651-9b42-457f-bd18-425206f3ce89){: width="1000px"}

1. 위의 그림과 같이, 각각의 foreground object $i$ 에 대해, noise image $\mathbf{z}^{(i)}_T$에서 $\mathbf{z}^{(i)}_0$로 noise를 제거하여 하나의 instance에 대한 이미지를 생성하는 denoising process를 진행한다. 이를 위해 먼저 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Gaussian noise $\mathbf{z}^{(i)}_T$를 생성</span></mark>**한다.<br/>
이 때 생성한 initial noise는 모든 foreground object 이미지 생성(bbox)에 공유됨으로써 이미지의 일관성을 유지한다.(즉, $z^{(i)}_T = z_T$)
<br/><br/>
2. Denoising process의 noise 제거를 위한 **caption**은 다음과 같이 사용한다.
”*[background prompt] with [box caption]*"           (예: “*a realistic image of an indoor scene with a gray cat”*)
<br/><br/>
3. 앞서 언급한 것 처럼 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Cross attetion map</span></mark>**을 사용하여 bbox 안에 object가 생성되도록 하기 위해, cross attention map을 구한다. Spatial location $u$에서의 image feature $\mathbf{q}_u$, token index $v$에서 text feature $\mathbf{k}_v$에 대한 cross attention map $\mathbf{A}^{(i)}$는 다음과 같이 계산된다. 각각의 map은 pixel과 text token의 친화도를 나타낸다.
    
    $$
    \mathbf{A}^{(i)}_{uv} = \text{softmax}(\mathbf{q}^{T}_u \mathbf{k}_v)
    $$

    <br/>    
4. 위에서 언급한 BoxDiff의 constraint를 사용하여 bbox 안의 pixel은 caption과 관련된 token으로 cross-attention을 강화하고, bbox 밖의 pixel에서 cross-attention을 약화하는 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>energy function</span></mark>**을 사용한다. (Inner-Box, Outer-Box constraint)
($\text{Topk}_u$: average of top-k values across the spatial dimension $u$)
    
    $$
    E(\mathbf{A}^{(i)}, i, v) = - \text{Topk}_u(\mathbf{A}_{uv}\ \cdot \ \mathbf{b}^{(i)}) + w \ \text {Topk}_u(\mathbf{A}_{uv} \cdot (1-\mathbf{b}^{(i)}))
    $$

    <br/>    
5. 각각의 denoising step 전에 latent를 업데이트하여 energy function을 최소화 한다.
($\eta$는 guidance strength; $\text{Denoise}(\cdot)$은 denoising step)
    
    $$
    \mathbf{z}^{(i)}_t \gets \mathbf{z}^{(i)}_t - \eta \nabla_{\mathbf{z}^{(i)}_t} \sum_{u\ \in V_i} E(\mathbf{A}^{(i)}, i, v ) \\
    \mathbf{z}^{(i)}_{t-1} \gets \text{Denoise}(\mathbf{z}^{(i)}_t)
    $$

    <br/>    
6. 생성 단계가 끝나면, box caption에 해당하는 **cross-attention map**을 얻는다. 이 map은 각 object에 해당하는 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>saliency mask 역할</span></mark>**을 한다. 여기서 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>SAM</span></mark>**(segmentation 모델, [SAM 논문 리뷰](https://lunaleee.github.io/posts/SAM/) 참조)을 추가적으로 사용하여 mask 품질을 향상시킨다. 위의 그림과 같이 Per-box cross-attention map을 SAM에 넣으면 높은 saliency를 가지는 위치를 기준으로 refine된 mask를 얻을 수 있다. 만약 SAM을 사용하지 않는다면 saliency mask에서 단순 thresholding으로 대체할 수도 있다. <br/>
하나의 foreground instance에 대한 mask $\mathbf{m}^{(i)}$가 생성되면, 각 denoising step의 latent와 mask를 element-wise multiplication 하여 *masked* instance latents $(\hat{\text{z}}^{(i)})^T_{t=0}$를 얻는다. <br/>
**<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>masked instance latent</span></mark>**: 위의 그림과 같이 instance에 해당하는 영역에 대한 latent image.
    
    $$
    \hat{\mathbf{z}}^{(i)}_t = \mathbf{z}^{(i)}_t ⊗ \mathbf{m^{(i)}}
    $$
    
    <br/>

   ![LMD_12.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/2673f654-4631-404b-9b37-a331b9cdd89f){: width="1200px"}

7. masked instance latents $(\hat{\text{z}}^{(i)})^T_{t=0}$ 는 전체 이미지 생성에 일종의 instance-level 힌트 역할을 한다. 위의 그림과 같이, 각각 instance의 masked latent는 instance 이미지 생성이 아닌 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>전체 이미지를 생성하는 denoising process 과정에 주입</span></mark>**된다. 각 denoising time step에서의 latent $\mathbf{z}^{(\text{comp})}_t$에 대해, masked instance latents $\hat{\mathbf{z}}^{(i)}_t$에 해당하는 위치는 해당 mask를 배치한다.<br/>
앞서 언급했 듯 통일성을 위해 $\mathbf{z}^{(\text{comp})}_T$는 $\mathbf{z}_T$로 initialize된다. <br/>
Diffusion 모델은 초기 denoising step에서 object의 위치를 생성하고 이후 step에서 디테일한 부분을 생성하는 경향이 있으므로, **초기 단계에만 해당 과정을 적용**했다.
    
    $$
    \mathbf{z}^{(\text{comp})}_t \gets \text{LatentCompose}(\mathbf{z}^{(\text{comp})}_t, \hat{\mathbf{z}}^{(i)}_t, \mathbf{m}^{(i)}) \quad \forall i
    $$
    
    <br/>
8. 이 때 가이드를 더욱 강력하게 하기 위해, per-box generation에서 생성된 cross-attention map을 energy function을 이용하여 해당 영역으로 추가로 전송한다.
    
    $$
    E^{\text{(comp)}} = (\mathbf{A}^{(\text{comp})}, \mathbf{A}^{(i)}, i, v) = E(\mathbf{A}^{(\text{comp})}, i, v) + \lambda \sum_{u \in V^{'}_i } ∣\mathbf{A}^{(\text{comp})}_{uv} - \mathbf{A}^{(i)}_{uv}∣
    $$

9. 마지막으로 **diffusion image decoder**를 사용하여 latent $\mathbf{z}^{(\text{comp})}_0$를 pixel $\mathbf{x}_0$로 디코딩한다.

<br/><br/><br/>
논문에 제안된 training-free 방법은 instance-annotated external dataset을 활용하기 위해 GLIGEN과 같은 training-based method에 적용할 수도 있다. 
<br/><br/><br/><br/><br/>

## 3. Additional Capabilities of LMD

LMD pipeline은 추가적인 학습과정 없이도 두 가지 추가 기능을 제공한다.
<br/><br/>

#### 1. Instruction-based scene specification

![LMD_13.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/e983d39e-0167-4289-a83d-1ab3b3376300){: width="800px"}

Multi-round dialog를 지원하는 LLM(e.g. GPT-3.5/4)을 활용하여 LMD는 위의 그림과 같이 초기 prompt 이후에 여러 명령어로 추가적인 이미지 합성이 가능하다. 초기 이미지 생성 이후 사용자가 LLM에 설명이나 추가적인 수정을 요청하면, 생성된 이미지에서 새로운 레이아웃으로 이미지를 생성할 수 있다. 해당 기능을 통해 사용자가 전체 이미지 스타일, 레이아웃을 유지하면서 다양한 세부 조정을 가능하게 한다.
<br/><br/><br/>

#### 2. Supporting more languages

![LMD_14.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/53b3da22-2d53-4012-bf4d-fa6bfd6c3188){: width="800px"}

영어가 아닌 다른 언어를 사용하는 사용자에게 영어 layout output을 제공한다. 이를 통해 LLM layout generator는 non-English 유저의 prompt를 이해하고 영어 caption과 함께 layout을 출력한다. 
<br/><br/><br/><br/><br/><br/>

# Experiments

---

### 1. Qualitative Comparison

![LMD_15.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/e64bcbdc-17f9-4fa1-b57c-4a64df20cc67){: width="1200px"}

논문에서는 LMD를 Stable Diffusion과 질적으로 비교했다. LDM은 추가 학습 없이 다양한 diffusion 모델에 적용가능하므로 가장 큰 Stable Diffusion 모델인 SDXL을 LMD의 기본 모델로 사용하였고, SDXL baseline과 비교하였다. 결과적으로 위 그림과 같이 2단계 text-to-image generation 방식이 LLM의 layout과 일치하는 이미지를 생성함으로써 baseline 모델에 비해 prompt에 충실한 것을 관찰했다.
<br/><br/><br/>

![LMD_16.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/f460a499-39da-4c74-bd42-7c88630ede49){: width="1200px"}

다른 LLM-based image generator와 비교를 수행했다. VisualChatGPT, GILL 모델도 LLM을 활용하고 Stable Diffusion 기반으로 이미지를 생성한다. 두 방법은 여전히 기본 SD 모델에서 text embedding에 대한 제어가 충분하지 않은 문제가 있다. 위의 그림에서 볼 수 있듯이 두 방법 모두 LMD에 비해 text prompt를 정확하게 따르지 않는 것을 확인할 수 있다.
<br/><br/><br/><br/>

### 2. Quantitative evaluation

저자는 다음 4가지 task를 포함하는 text-to-image evaluation benchmark를 제안했다. 

- Negation / generative numeracy: 특정 개수의 object를 생성하거나 특정 객체를 생성하지 않는 것
- Attribute binding: prompt에 여러 object가 있을 때 올바른 object에 올바른 속성(attribute)을 할당하는 작업
- Spatial reasoning: object의 상대적 위치를 설명하는 단어를 이해하는 것
<br/><br/><br/>

#### Detection-based evaluation.

저자는 관심 있는 object에 대한 bounding box를 얻기 위해 open-vocabulary object detector, OWL-ViT를 활용했다. 그 다음 생성된 각 이미지가 prompt의 요구 사항을 충족하는지 확인했다. 결과는 아래 표와 같이 4개 task에서 SD에 비해 1.3배에서 3.6배까지 생성 정확도가 크게 향상되고 평균 정확도가 두 배 증가했다.

또한 in-domain instance-annotated 데이터를 활용하기 위해 GLIGEN을 파이프라인에 추가로 통합하면(LMD+) 추가 개선을 달성하는 것을 확인할 수 있다.

![LMD_17.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/cd439948-c573-4b92-8168-0401bebf85bf){: width="1100px"}
<br/><br/><br/><br/>

### 3. Ablation Study

#### Layout-to-image stage.

![LMD_18.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/30d395ca-66d9-44c8-9abe-7542573c0561){: width="900px"}

다른 layout-to-image method와 비교를 수행했다. Semantic-level grounding을 수행하는 training-free layout-to-image generation 방법과 비교하여 제안된 LMD는 훨씬 더 나은 instance-level grounding 성능을 보인다.
<br/><br/><br/>

![LMD_19.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/c5323bdb-4e4e-4241-afdd-98dc40763d1f){: width="350px"}

위에서 언급했듯 SAM을 사용하여 mask를 얻는 대신 추가 segmentation 모듈이 필요하지 않은 접근 방식에 대한 실험을 진행했다. Attention 값이 가장 높은 상위 75% 픽셀을 각 bbox의 mask로 선택했다. 결과는 위 표와 같다.
<br/><br/><br/><br/>
