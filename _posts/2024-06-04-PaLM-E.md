---
title: "[논문 리뷰] PaLM-E: An Embodied Multimodal Language Model"
author: lunalee
date: 2024-06-04 21:01:41 +0900
categories: [AI, Paper Review]
tags: [LLM, Multi-modal, Multi-task]
pin: false
math: true
toc: ture
---

<br/><br/>
`Robotics at Google` `TU Berlin` `Google Research` `arXiv 2023`

- Paper: [https://arxiv.org/abs/2303.03378](https://arxiv.org/abs/2303.03378)
- Git: [https://github.com/kyegomez/PALM-E](https://github.com/kyegomez/PALM-E)
- Page: [https://palm-e.github.io](https://palm-e.github.io/)
<br/><br/><br/>

#### 📖 핵심 훑어보기 !!

- pre-trained LLM(PaLM)을 기반으로 multi-modal 입력을 처리하기 위한 embodied language model 제안
- 다양한 continuous observation(입력)을 language embedding space X로 매핑하도록 modality 별로 개별 encoder를 학습시키고, embedding vector LLM의 text와 함께 prefix로 사용
- ViT, OSRT등 다양한 입력을 처리하기 위한 encoder를 사용
<br/><br/><br/><br/>

# Introduction

---

LLM은 다양한 도메인에서 강력한 추론 능력을 보여주고 있다. 방대한 양의 text 데이터에서 LLM을 학습하면 실제 세계와 관련된 representation을 이끌어 낼 수 있지만, visual 또는 physical sensor 같은 modality에 연결하여 더 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>광범위한 “grounded” real-world 문제</span></mark>**를 해결하는 것이 필요하다. 특히 기존의 일반적인 visual-language task에 대해 학습된 모델은 robotic task를 직접 해결하지 못했다.
<br/><br/><br/>

![PaLM-E_1.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/3665dfcf-46ea-4385-975c-7169d4b038b6){: width="1300px"}

본 논문에서는 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>“embodied language model”</span></mark>**을 제안한다. 다양한 sensor modality에서 continuous한 입력을 통합하여, LLM이 real-world에서 순차적인 의사 결정을 위해 더욱 ground된 추론을 할 수 있도록 한다. 이미지, state estimate와 같은 입력은 language token과 동일한 latent embedding으로 처리되고 text와 동일한 방식으로 transformer 기반 LLM의 self-attention layer에서 처리된다. 
<br/><br/>

논문의 방법에 대해 조사하기 위해 세 가지 robotic manipulation domain, VQA, image captioning, language task에 대해 평가했다. 평과 결과에 다르면 multi-task training은 개별 task에 대한 training 모델에 비해 성능이 향상되었다고 한다. 이러한 task간 transfer는 robotics task에 높은 데이터 효율성으로 이어진다. 예를 들어 적은 학습 데이터로 학습 성공률을 올리고, one-shot 또는 zero-shot generalization을 보여준다.
<br/><br/><br/><br/><br/><br/>

# Method

---
## 1. PaLM-E: An Embodied Multimodal Language Model

먼저 PaLM-E의 메인 아이디어는 이미지나 state estimates, 여러 sensor modality의 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>continuous, embodied observation들을 language embedding space로 변환</span></mark>**하여 pre-trained language 모델에 넣는 것이다. 이를 위해 continuous observation을 vector sequence로 인코딩한다. 이 때 vector sequence는 language token의 embedding space dimension과 동일하게 인코딩되므로 language token과 유사한 방식으로 모델에 주입될 수 있다. 
<br/><br/>

PaLM-E 모델은 다음과 같이 요약된다.
<br/><br/>

- LLM 모델 PaLM을 pre-trained language 모델로 사용하여 “Embodied”하게 만듦.
- Prefix 또는 prompt가 주어지면 텍스트 완성(textual completion)을 autoregressive하게 생성하는 **decoder-only LLM**.
- Prefix 또는 prompt $w_{1:n}$은 LLM이 후속  token $w_{n+1:L}$을 계속 예측하는 데 기반이 되는 context를 제공함.
    
    $$
    p(w_{n+1:L}|w_{1:n}) = \prod^L_{l=n+1} p_{LM}(w_l|w_{1:l-1})
    $$
    
- 입력으로는 text, (여러개의) continuous observation으로 구성됨. Continuous observation을 language embedding space X로 매핑하도록 encoder $\phi$를 학습시킴$(\phi : \mathcal{O} → \mathcal{X}^q)$.
- Observation은 vector sequence 형태로 변환되고 text token과 함께 LLM의 prefix를 형성.
- Observation embedding은 고정된 위치에 삽입되지 않고 text 주변에 동적으로 배치됨.
- 출력은 모델에 의해 생성된 텍스트로, **질문에 대한 답변**이거나 **로봇이 실행해야 하는 텍스트 형태의 decision**이 될 수 있음.
- 모델이 decision이나 계획을 세우는 task를 수행할 때, 모델의 decision을 low-level action으로 바꿔주는 low-level policy 또는 planner가 있다고 가정함.
<br/><br/><br/><br/><br/><br/><br/>

## 2. Input & Scene Representations for Different Sensor Modalities

다음으로는 PaLM-E에 통합되는 개별 modality와 해당 encoder에 대해 살펴보자. 각 encoder $(\phi : \mathcal{O} → \mathcal{X}^q)$에 대해 서로 다른 아키텍처를 사용하여 해당 modaliy를 language embedding space에 매핑한다.
<br/><br/><br/>

#### (1) State estimation vectors: MLP

- Robot이나 object에 대한 상태 추정과 같은 state vector는 PaLM-E의 입력으로 가장 간단한 형태임.
- 장면에서 object의 상태를 설명하는 vector $s ∈ ℝ^S$ 는 Object의 포즈, 크기, 색상 등을 포함한다.
- MLP $\phi_{\text{state}}$ 를 사용하여 $s$를 language embedding space로 매핑
<br/><br/><br/><br/>

#### (2) Image: Vision Transformer(ViT)

- ViT $\tilde \phi_\text{ViT}$를 사용하여 Image $I$를 token embedding $\tilde x_{1:m} = \tilde \phi_{\text{ViT}}(I) ∈ R^{m×\tilde k }$ 로 매핑.
- 3가지 ViT architecture 사용: ViT-4B, ViT-22B, ViT+TL (ViT token learner)
- ViT embedding 차원 $\tilde{k}$는 language 모델 차원과 동일하지 않아도 됨. 각각의 embedding은 아래와 같이 ViT를 거친 뒤 affine transformation을 거쳐 인코딩 됨.

$$
x_i = \phi_{ViT}(I)_i = \psi(\tilde{\phi}_{ViT}(I)_i) \qquad \psi: \text{affine transformation}
$$

<br/><br/><br/>

#### (3) Object-centric representations.

![PaLM-E_2.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/a813c7a4-c03d-4377-b7d2-2c5a9111cb97){: width="1400px"}

Language 모델은 언어의 문법적 구조와 의미론적 상호작용을 이해하고 모델링한다. 하지만 visual input에 대해 학습 과정의 featrue를 살펴보면, 이미지 내에 존재하는 object instance간의 관계보다는 주로 위 그림의 (a)와 같은 패턴, static grid에 가깝다. 

Language과 image 모델의 이러한 차이점은, ViT를 pre-train된 LLM과 인터페이스 할 때, object간의 상호작용이 필요한 추론을 해결하는 데 어려움을 준다. 따라서 LLM에 image embedding을 주입하기 전, visual input을 개별 object로 분리하는 구조화된 Encoder를 탐색한다.

Ground-truth object instance mask $M_j$가 주어지면, object $j$에 대해 ViT의 representation을 학습할 수 있다.

$$
x^j_{1:m} = \phi_{\text{ViT}}(M_j \ \circ \ I)
$$

<br/><br/><br/>

#### (4) Object Scene Representation Transformer (OSRT).

Object-centric representation으로 변환할 때, GT segmentation이 필요하지 않은 대안으로 [OSRT📄]([https://arxiv.org/abs/2206.06922](https://arxiv.org/abs/2206.06922))가 있다. 논문에서는 이미지를 object, 기하학적인 구조 측면으로 이해하는 것을 목적으로 하며, 이를 위해 Slot Attention*이라고 하는 방법을 사용한다. OSRT에서 제안한 view synthesis task를 통해 3D-centric neural scene representation을 학습힌다. 

Scene representation은 object slot $o_j = \phi_{\text{OSRT}}(I_{1:v})_j \in ℝ^k$ 으로 구성되어 있다. 이러한 각각의 slot을 MLP $\psi$ 로 project 한다. 개별 object는 항상 여러개의 embedding으로 토큰화 된다.$(\psi : ℝ^{\bar{k}} \to ℝ^{m \times k})$

$$
x^j_{1:m} = \psi(\phi_{\text{OSRT}}(I_{1:v})_j)
$$

<br/>

> [**Object-Centric Learning with Slot Attention)**📄](https://arxiv.org/abs/2206.06922)
>
> ![PaLM-E_3.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/b81c40ce-7678-43eb-af66-ed0c808455c3){: width="1100px"}
> 

<br/><br/><br/>

#### Entity referrals.

Embodied planning task와 같은 경우, PaLM-E는 생성된 계획에서 object를 참조할 수 있어야한다. 대부분의 경우에는 object의 고유한 속성을 토해 자연어로 식별하능하지만, 일부의 경우 몇개의 단어로 식별할 수 없는 경우도 있다. 예를 들어 테이블 위에 같은 색상의 블록이 여러개 존재하는 경우, OSRT는  object에 대해 다음과 같이 label을 지정한다

$$
\text{Object 1 is <obj 1>. ... Object }j\text{ is <obj }j\text{>}
$$

<br/><br/><br/><br/><br/>

## 3. Training Recipes

- PaLM-E의 학습 데이터는 각 example i에 대해 다음과 같은 형태를 가진다. 여기서 $I^i_j$는 $u_i$-many continuous observations, $w^i_{1:L_i}$는 text, $n_i$는 index이다.
    
    $$
    {\lbrace ( I^i_{1:u_i}, w^i_{1:L_i}, n_i) \rbrace }^N_{i=1}
    $$
    
- index $n_i$ 까지는 multi-modal 문장 형식의 prefix 부분이고, 그 이후는 text token만 포함하는 prediction target이다.
- loss function은 개별 non-prefix token $w^i_{n_i +1:L_i}$에 대한 cross-entropy loss 평균이다.
- multi-modal 문장을 형성하기 위해, text에 special token이 존재한다. Encoder의 embedding vector가 존재할 때, special token을 embedding vector로 대체해주는 방식으로 사용한다.
<br/><br/><br/>

#### Variation with Model freezing.

논문의 구조는 인코더 $\tilde {\phi}$, 프로젝터 $\psi$, LLM $p_{LM}$의 세 부분으로 구성된다. 전체 모델에 대해 동시에 parameter update를 진행하며 학습할 수 도 있지만, 적절한 prompt가 주어졌을 때 LLM의 추론능력은 증명되었다. 따라서 LLM을 frozen하고 입력 encoder만 학습할 수 있는지, 그렇다면 다른 modality encoder를 어떻게 비교하는지에 대해 조사했다. 

이 경우, encoder는 frozen된 LLM이 observation에 기반을 두도록 embedding vector를 생성해야 하며, observation에 대한 정보를 LLM에 전파해야 한다. 이러한 encoding을 학습하는 것은 soft prompt*의 한 형태로 이해할 수 있다.
<br/><br/>

> **Soft prompt***<br/>
> hard prompt: 사람이 해석 가능한 토큰이 추가된 형태<br/>
> soft prompt: 사람이 해석 불가한 실수값, 연속적인 벡터값으로 이루어진 토큰이 추가된 형태<br/>
> 
> ![PaLM-E_4.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/6d1a53e1-30b4-4e3f-a99b-23f7f319591e){: width="600px"}<br/>
> 참조: [https://velog.io/@hyeda/About-prompt-learning](https://velog.io/@hyeda/About-prompt-learning)
> 

<br/><br/>

#### Co-training across tasks.

실험에서 다양한 다양한 데이터에 대한 모델 co-training의 효과를 조사했다. 
<br/><br/><br/><br/><br/><br/>

# Experiments

---

저자는 robotic (mobile) manipulation task에 대한 실험을 진행했다. 또한 visual-question-answering(VQA), image captioning, established language modeling task와 같은 일반적인 vision-language task에서도 PaLM-E를 평가했다.
<br/><br/>

논문의 세 가지 robot environment에는 로봇이 물체를 manipulate(잡고 쌓기)해야 하는 Task and Motion Planning (TAMP) domain, table-top pushing environment, mobile manipulation domain이 포함된다. 각 도메인에서 PaLM-E는 해당 domain의 데이터로 학습된다. 

![PaLM-E_5.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/8e670753-7df2-4163-aefa-da20f19a888d){: width="500px"}

저자는 다양한 task과 robot embodiment에서 여러 데이터셋을 혼합하여 학습한 모델이 모든 task에서 동시에 높은 성능을 달성할 수 있음을 보여주고 있다. 위의 그림을 살펴보면, 다른 task와 embodiment에도 불구하고, 개별 task에 대한 성능은 작업 mixture에 대한 학습을 통해 증가했다.
<br/><br/><br/><br/>

#### TAMP Environment.

![PaLM-E_6.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/488bd219-623e-468b-b3d3-48160e9400fd){: width="1000px"}

TAMP 환경에 대한 계획 성공률과 VQA 성능에 대한 실험을 진행했다.. LLM은 pre-train된 상태로 frozen되었고 input representation은 TMAP 데이터셋에 대해서만 학습되었다(데이터 혼합 없음). 결과는 위 표와 같다.
<br/><br/><br/><br/>

#### Language-Table Environment.

![PaLM-E_7.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/ce580401-2cc7-47c6-992e-c2ceb29aed71){: width="1100px"}

위의 그림을 통해  PaLM-E는 적대적인 교란에 강인함을 유지하면서도 multi-stage tabletop manipulation task을 통해 실제 로봇을 안내할 수 있음을 볼 수 있다. 
<br/><br/><br/><br/>

#### Performance on General Language Tasks.

![PaLM-E_8.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/033d1ab0-6a16-4ec0-ad0a-67b78beef5f2){: width="550px"}

OKVQ, VQA v2, COCO caption을 포함한 general vision-language task에 대한 결과는 위와 같다.

<br/><br/><br/><br/>
