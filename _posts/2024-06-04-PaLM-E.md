---
title: "[ë…¼ë¬¸ ë¦¬ë·°] PaLM-E: An Embodied Multimodal Language Model"
author: lunalee
date: 2024-06-04 21:01:41 +0900
categories: [AI, Paper Review]
tags: [LLM, Multi-modal, Multi-task]
pin: false
math: true
toc: ture
---

<br/><br/>
`Robotics at Google` `TU Berlin` `Google Research` `arXiv 2023`

- Paper: [https://arxiv.org/abs/2303.03378](https://arxiv.org/abs/2303.03378)
- Git: [https://github.com/kyegomez/PALM-E](https://github.com/kyegomez/PALM-E)
- Page: [https://palm-e.github.io](https://palm-e.github.io/)
<br/><br/><br/>

#### ğŸ“– í•µì‹¬ í›‘ì–´ë³´ê¸° !!

- pre-trained LLM(PaLM)ì„ ê¸°ë°˜ìœ¼ë¡œ multi-modal ì…ë ¥ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ embodied language model ì œì•ˆ
- ë‹¤ì–‘í•œ continuous observation(ì…ë ¥)ì„ language embedding space Xë¡œ ë§¤í•‘í•˜ë„ë¡ modality ë³„ë¡œ ê°œë³„ encoderë¥¼ í•™ìŠµì‹œí‚¤ê³ , embedding vector LLMì˜ textì™€ í•¨ê»˜ prefixë¡œ ì‚¬ìš©
- ViT, OSRTë“± ë‹¤ì–‘í•œ ì…ë ¥ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ encoderë¥¼ ì‚¬ìš©
<br/><br/><br/><br/>

# Introduction

---

LLMì€ ë‹¤ì–‘í•œ ë„ë©”ì¸ì—ì„œ ê°•ë ¥í•œ ì¶”ë¡  ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤. ë°©ëŒ€í•œ ì–‘ì˜ text ë°ì´í„°ì—ì„œ LLMì„ í•™ìŠµí•˜ë©´ ì‹¤ì œ ì„¸ê³„ì™€ ê´€ë ¨ëœ representationì„ ì´ëŒì–´ ë‚¼ ìˆ˜ ìˆì§€ë§Œ, visual ë˜ëŠ” physical sensor ê°™ì€ modalityì— ì—°ê²°í•˜ì—¬ ë” **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>ê´‘ë²”ìœ„í•œ â€œgroundedâ€ real-world ë¬¸ì œ</span></mark>**ë¥¼ í•´ê²°í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤. íŠ¹íˆ ê¸°ì¡´ì˜ ì¼ë°˜ì ì¸ visual-language taskì— ëŒ€í•´ í•™ìŠµëœ ëª¨ë¸ì€ robotic taskë¥¼ ì§ì ‘ í•´ê²°í•˜ì§€ ëª»í–ˆë‹¤.
<br/><br/><br/>

![PaLM-E_1.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/3665dfcf-46ea-4385-975c-7169d4b038b6){: width="1300px"}

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>â€œembodied language modelâ€</span></mark>**ì„ ì œì•ˆí•œë‹¤. ë‹¤ì–‘í•œ sensor modalityì—ì„œ continuousí•œ ì…ë ¥ì„ í†µí•©í•˜ì—¬, LLMì´ real-worldì—ì„œ ìˆœì°¨ì ì¸ ì˜ì‚¬ ê²°ì •ì„ ìœ„í•´ ë”ìš± groundëœ ì¶”ë¡ ì„ í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. ì´ë¯¸ì§€, state estimateì™€ ê°™ì€ ì…ë ¥ì€ language tokenê³¼ ë™ì¼í•œ latent embeddingìœ¼ë¡œ ì²˜ë¦¬ë˜ê³  textì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ transformer ê¸°ë°˜ LLMì˜ self-attention layerì—ì„œ ì²˜ë¦¬ëœë‹¤. 
<br/><br/>

ë…¼ë¬¸ì˜ ë°©ë²•ì— ëŒ€í•´ ì¡°ì‚¬í•˜ê¸° ìœ„í•´ ì„¸ ê°€ì§€ robotic manipulation domain, VQA, image captioning, language taskì— ëŒ€í•´ í‰ê°€í–ˆë‹¤. í‰ê³¼ ê²°ê³¼ì— ë‹¤ë¥´ë©´ multi-task trainingì€ ê°œë³„ taskì— ëŒ€í•œ training ëª¨ë¸ì— ë¹„í•´ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆë‹¤ê³  í•œë‹¤. ì´ëŸ¬í•œ taskê°„ transferëŠ” robotics taskì— ë†’ì€ ë°ì´í„° íš¨ìœ¨ì„±ìœ¼ë¡œ ì´ì–´ì§„ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì ì€ í•™ìŠµ ë°ì´í„°ë¡œ í•™ìŠµ ì„±ê³µë¥ ì„ ì˜¬ë¦¬ê³ , one-shot ë˜ëŠ” zero-shot generalizationì„ ë³´ì—¬ì¤€ë‹¤.
<br/><br/><br/><br/><br/><br/>

# Method

---
## 1. PaLM-E: An Embodied Multimodal Language Model

ë¨¼ì € PaLM-Eì˜ ë©”ì¸ ì•„ì´ë””ì–´ëŠ” ì´ë¯¸ì§€ë‚˜ state estimates, ì—¬ëŸ¬ sensor modalityì˜ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>continuous, embodied observationë“¤ì„ language embedding spaceë¡œ ë³€í™˜</span></mark>**í•˜ì—¬ pre-trained language ëª¨ë¸ì— ë„£ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ìœ„í•´ continuous observationì„ vector sequenceë¡œ ì¸ì½”ë”©í•œë‹¤. ì´ ë•Œ vector sequenceëŠ” language tokenì˜ embedding space dimensionê³¼ ë™ì¼í•˜ê²Œ ì¸ì½”ë”©ë˜ë¯€ë¡œ language tokenê³¼ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì— ì£¼ì…ë  ìˆ˜ ìˆë‹¤. 
<br/><br/>

PaLM-E ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ìš”ì•½ëœë‹¤.
<br/><br/>

- LLM ëª¨ë¸ PaLMì„ pre-trained language ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ì—¬ â€œEmbodiedâ€í•˜ê²Œ ë§Œë“¦.
- Prefix ë˜ëŠ” promptê°€ ì£¼ì–´ì§€ë©´ í…ìŠ¤íŠ¸ ì™„ì„±(textual completion)ì„ autoregressiveí•˜ê²Œ ìƒì„±í•˜ëŠ” **decoder-only LLM**.
- Prefix ë˜ëŠ” prompt $w_{1:n}$ì€ LLMì´ í›„ì†  token $w_{n+1:L}$ì„ ê³„ì† ì˜ˆì¸¡í•˜ëŠ” ë° ê¸°ë°˜ì´ ë˜ëŠ” contextë¥¼ ì œê³µí•¨.
    
    $$
    p(w_{n+1:L}|w_{1:n}) = \prod^L_{l=n+1} p_{LM}(w_l|w_{1:l-1})
    $$
    
- ì…ë ¥ìœ¼ë¡œëŠ” text, (ì—¬ëŸ¬ê°œì˜) continuous observationìœ¼ë¡œ êµ¬ì„±ë¨. Continuous observationì„ language embedding space Xë¡œ ë§¤í•‘í•˜ë„ë¡ encoder $\phi$ë¥¼ í•™ìŠµì‹œí‚´$(\phi : \mathcal{O} â†’ \mathcal{X}^q)$.
- Observationì€ vector sequence í˜•íƒœë¡œ ë³€í™˜ë˜ê³  text tokenê³¼ í•¨ê»˜ LLMì˜ prefixë¥¼ í˜•ì„±.
- Observation embeddingì€ ê³ ì •ëœ ìœ„ì¹˜ì— ì‚½ì…ë˜ì§€ ì•Šê³  text ì£¼ë³€ì— ë™ì ìœ¼ë¡œ ë°°ì¹˜ë¨.
- ì¶œë ¥ì€ ëª¨ë¸ì— ì˜í•´ ìƒì„±ëœ í…ìŠ¤íŠ¸ë¡œ, **ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€**ì´ê±°ë‚˜ **ë¡œë´‡ì´ ì‹¤í–‰í•´ì•¼ í•˜ëŠ” í…ìŠ¤íŠ¸ í˜•íƒœì˜ decision**ì´ ë  ìˆ˜ ìˆìŒ.
- ëª¨ë¸ì´ decisionì´ë‚˜ ê³„íšì„ ì„¸ìš°ëŠ” taskë¥¼ ìˆ˜í–‰í•  ë•Œ, ëª¨ë¸ì˜ decisionì„ low-level actionìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” low-level policy ë˜ëŠ” plannerê°€ ìˆë‹¤ê³  ê°€ì •í•¨.
<br/><br/><br/><br/><br/><br/><br/>

## 2. Input & Scene Representations for Different Sensor Modalities

ë‹¤ìŒìœ¼ë¡œëŠ” PaLM-Eì— í†µí•©ë˜ëŠ” ê°œë³„ modalityì™€ í•´ë‹¹ encoderì— ëŒ€í•´ ì‚´í´ë³´ì. ê° encoder $(\phi : \mathcal{O} â†’ \mathcal{X}^q)$ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ modaliyë¥¼ language embedding spaceì— ë§¤í•‘í•œë‹¤.
<br/><br/><br/>

#### (1) State estimation vectors: MLP

- Robotì´ë‚˜ objectì— ëŒ€í•œ ìƒíƒœ ì¶”ì •ê³¼ ê°™ì€ state vectorëŠ” PaLM-Eì˜ ì…ë ¥ìœ¼ë¡œ ê°€ì¥ ê°„ë‹¨í•œ í˜•íƒœì„.
- ì¥ë©´ì—ì„œ objectì˜ ìƒíƒœë¥¼ ì„¤ëª…í•˜ëŠ” vector $s âˆˆ â„^S$ ëŠ” Objectì˜ í¬ì¦ˆ, í¬ê¸°, ìƒ‰ìƒ ë“±ì„ í¬í•¨í•œë‹¤.
- MLP $\phi_{\text{state}}$ ë¥¼ ì‚¬ìš©í•˜ì—¬ $s$ë¥¼ language embedding spaceë¡œ ë§¤í•‘
<br/><br/><br/><br/>

#### (2) Image: Vision Transformer(ViT)

- ViT $\tilde \phi_\text{ViT}$ë¥¼ ì‚¬ìš©í•˜ì—¬ Image $I$ë¥¼ token embedding $\tilde x_{1:m} = \tilde \phi_{\text{ViT}}(I) âˆˆ R^{mÃ—\tilde k }$ ë¡œ ë§¤í•‘.
- 3ê°€ì§€ ViT architecture ì‚¬ìš©: ViT-4B, ViT-22B, ViT+TL (ViT token learner)
- ViT embedding ì°¨ì› $\tilde{k}$ëŠ” language ëª¨ë¸ ì°¨ì›ê³¼ ë™ì¼í•˜ì§€ ì•Šì•„ë„ ë¨. ê°ê°ì˜ embeddingì€ ì•„ë˜ì™€ ê°™ì´ ViTë¥¼ ê±°ì¹œ ë’¤ affine transformationì„ ê±°ì³ ì¸ì½”ë”© ë¨.

$$
x_i = \phi_{ViT}(I)_i = \psi(\tilde{\phi}_{ViT}(I)_i) \qquad \psi: \text{affine transformation}
$$

<br/><br/><br/>

#### (3) Object-centric representations.

![PaLM-E_2.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/a813c7a4-c03d-4377-b7d2-2c5a9111cb97){: width="1400px"}

Language ëª¨ë¸ì€ ì–¸ì–´ì˜ ë¬¸ë²•ì  êµ¬ì¡°ì™€ ì˜ë¯¸ë¡ ì  ìƒí˜¸ì‘ìš©ì„ ì´í•´í•˜ê³  ëª¨ë¸ë§í•œë‹¤. í•˜ì§€ë§Œ visual inputì— ëŒ€í•´ í•™ìŠµ ê³¼ì •ì˜ featrueë¥¼ ì‚´í´ë³´ë©´, ì´ë¯¸ì§€ ë‚´ì— ì¡´ì¬í•˜ëŠ” object instanceê°„ì˜ ê´€ê³„ë³´ë‹¤ëŠ” ì£¼ë¡œ ìœ„ ê·¸ë¦¼ì˜ (a)ì™€ ê°™ì€ íŒ¨í„´, static gridì— ê°€ê¹ë‹¤. 

Languageê³¼ image ëª¨ë¸ì˜ ì´ëŸ¬í•œ ì°¨ì´ì ì€, ViTë¥¼ pre-trainëœ LLMê³¼ ì¸í„°í˜ì´ìŠ¤ í•  ë•Œ, objectê°„ì˜ ìƒí˜¸ì‘ìš©ì´ í•„ìš”í•œ ì¶”ë¡ ì„ í•´ê²°í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ì¤€ë‹¤. ë”°ë¼ì„œ LLMì— image embeddingì„ ì£¼ì…í•˜ê¸° ì „, visual inputì„ ê°œë³„ objectë¡œ ë¶„ë¦¬í•˜ëŠ” êµ¬ì¡°í™”ëœ Encoderë¥¼ íƒìƒ‰í•œë‹¤.

Ground-truth object instance mask $M_j$ê°€ ì£¼ì–´ì§€ë©´, object $j$ì— ëŒ€í•´ ViTì˜ representationì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

$$
x^j_{1:m} = \phi_{\text{ViT}}(M_j \ \circ \ I)
$$

<br/><br/><br/>

#### (4) Object Scene Representation Transformer (OSRT).

Object-centric representationìœ¼ë¡œ ë³€í™˜í•  ë•Œ, GT segmentationì´ í•„ìš”í•˜ì§€ ì•Šì€ ëŒ€ì•ˆìœ¼ë¡œ [OSRTğŸ“„]([https://arxiv.org/abs/2206.06922](https://arxiv.org/abs/2206.06922))ê°€ ìˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ë¯¸ì§€ë¥¼ object, ê¸°í•˜í•™ì ì¸ êµ¬ì¡° ì¸¡ë©´ìœ¼ë¡œ ì´í•´í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•˜ë©°, ì´ë¥¼ ìœ„í•´ Slot Attention*ì´ë¼ê³  í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. OSRTì—ì„œ ì œì•ˆí•œ view synthesis taskë¥¼ í†µí•´ 3D-centric neural scene representationì„ í•™ìŠµíŒë‹¤. 

Scene representationì€ object slot $o_j = \phi_{\text{OSRT}}(I_{1:v})_j \in â„^k$ ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. ì´ëŸ¬í•œ ê°ê°ì˜ slotì„ MLP $\psi$ ë¡œ project í•œë‹¤. ê°œë³„ objectëŠ” í•­ìƒ ì—¬ëŸ¬ê°œì˜ embeddingìœ¼ë¡œ í† í°í™” ëœë‹¤.$(\psi : â„^{\bar{k}} \to â„^{m \times k})$

$$
x^j_{1:m} = \psi(\phi_{\text{OSRT}}(I_{1:v})_j)
$$

<br/>

> [**Object-Centric Learning with Slot Attention)**ğŸ“„](https://arxiv.org/abs/2206.06922)
>
> ![PaLM-E_3.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/b81c40ce-7678-43eb-af66-ed0c808455c3){: width="1100px"}
> 

<br/><br/><br/>

#### Entity referrals.

Embodied planning taskì™€ ê°™ì€ ê²½ìš°, PaLM-EëŠ” ìƒì„±ëœ ê³„íšì—ì„œ objectë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆì–´ì•¼í•œë‹¤. ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì—ëŠ” objectì˜ ê³ ìœ í•œ ì†ì„±ì„ í† í•´ ìì—°ì–´ë¡œ ì‹ë³„í•˜ëŠ¥í•˜ì§€ë§Œ, ì¼ë¶€ì˜ ê²½ìš° ëª‡ê°œì˜ ë‹¨ì–´ë¡œ ì‹ë³„í•  ìˆ˜ ì—†ëŠ” ê²½ìš°ë„ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ í…Œì´ë¸” ìœ„ì— ê°™ì€ ìƒ‰ìƒì˜ ë¸”ë¡ì´ ì—¬ëŸ¬ê°œ ì¡´ì¬í•˜ëŠ” ê²½ìš°, OSRTëŠ”  objectì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ labelì„ ì§€ì •í•œë‹¤

$$
\text{Object 1 is <obj 1>. ... Object }j\text{ is <obj }j\text{>}
$$

<br/><br/><br/><br/><br/>

## 3. Training Recipes

- PaLM-Eì˜ í•™ìŠµ ë°ì´í„°ëŠ” ê° example iì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¥¼ ê°€ì§„ë‹¤. ì—¬ê¸°ì„œ $I^i_j$ëŠ” $u_i$-many continuous observations, $w^i_{1:L_i}$ëŠ” text, $n_i$ëŠ” indexì´ë‹¤.
    
    $$
    {\lbrace ( I^i_{1:u_i}, w^i_{1:L_i}, n_i) \rbrace }^N_{i=1}
    $$
    
- index $n_i$ ê¹Œì§€ëŠ” multi-modal ë¬¸ì¥ í˜•ì‹ì˜ prefix ë¶€ë¶„ì´ê³ , ê·¸ ì´í›„ëŠ” text tokenë§Œ í¬í•¨í•˜ëŠ” prediction targetì´ë‹¤.
- loss functionì€ ê°œë³„ non-prefix token $w^i_{n_i +1:L_i}$ì— ëŒ€í•œ cross-entropy loss í‰ê· ì´ë‹¤.
- multi-modal ë¬¸ì¥ì„ í˜•ì„±í•˜ê¸° ìœ„í•´, textì— special tokenì´ ì¡´ì¬í•œë‹¤. Encoderì˜ embedding vectorê°€ ì¡´ì¬í•  ë•Œ, special tokenì„ embedding vectorë¡œ ëŒ€ì²´í•´ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.
<br/><br/><br/>

#### Variation with Model freezing.

ë…¼ë¬¸ì˜ êµ¬ì¡°ëŠ” ì¸ì½”ë” $\tilde {\phi}$, í”„ë¡œì í„° $\psi$, LLM $p_{LM}$ì˜ ì„¸ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ëœë‹¤. ì „ì²´ ëª¨ë¸ì— ëŒ€í•´ ë™ì‹œì— parameter updateë¥¼ ì§„í–‰í•˜ë©° í•™ìŠµí•  ìˆ˜ ë„ ìˆì§€ë§Œ, ì ì ˆí•œ promptê°€ ì£¼ì–´ì¡Œì„ ë•Œ LLMì˜ ì¶”ë¡ ëŠ¥ë ¥ì€ ì¦ëª…ë˜ì—ˆë‹¤. ë”°ë¼ì„œ LLMì„ frození•˜ê³  ì…ë ¥ encoderë§Œ í•™ìŠµí•  ìˆ˜ ìˆëŠ”ì§€, ê·¸ë ‡ë‹¤ë©´ ë‹¤ë¥¸ modality encoderë¥¼ ì–´ë–»ê²Œ ë¹„êµí•˜ëŠ”ì§€ì— ëŒ€í•´ ì¡°ì‚¬í–ˆë‹¤. 

ì´ ê²½ìš°, encoderëŠ” frozenëœ LLMì´ observationì— ê¸°ë°˜ì„ ë‘ë„ë¡ embedding vectorë¥¼ ìƒì„±í•´ì•¼ í•˜ë©°, observationì— ëŒ€í•œ ì •ë³´ë¥¼ LLMì— ì „íŒŒí•´ì•¼ í•œë‹¤. ì´ëŸ¬í•œ encodingì„ í•™ìŠµí•˜ëŠ” ê²ƒì€ soft prompt*ì˜ í•œ í˜•íƒœë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤.
<br/><br/>

> **Soft prompt***<br/>
> hard prompt: ì‚¬ëŒì´ í•´ì„ ê°€ëŠ¥í•œ í† í°ì´ ì¶”ê°€ëœ í˜•íƒœ<br/>
> soft prompt: ì‚¬ëŒì´ í•´ì„ ë¶ˆê°€í•œ ì‹¤ìˆ˜ê°’, ì—°ì†ì ì¸ ë²¡í„°ê°’ìœ¼ë¡œ ì´ë£¨ì–´ì§„ í† í°ì´ ì¶”ê°€ëœ í˜•íƒœ<br/>
> 
> ![PaLM-E_4.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/6d1a53e1-30b4-4e3f-a99b-23f7f319591e){: width="600px"}<br/>
> ì°¸ì¡°: [https://velog.io/@hyeda/About-prompt-learning](https://velog.io/@hyeda/About-prompt-learning)
> 

<br/><br/>

#### Co-training across tasks.

ì‹¤í—˜ì—ì„œ ë‹¤ì–‘í•œ ë‹¤ì–‘í•œ ë°ì´í„°ì— ëŒ€í•œ ëª¨ë¸ co-trainingì˜ íš¨ê³¼ë¥¼ ì¡°ì‚¬í–ˆë‹¤. 
<br/><br/><br/><br/><br/><br/>

# Experiments

---

ì €ìëŠ” robotic (mobile) manipulation taskì— ëŒ€í•œ ì‹¤í—˜ì„ ì§„í–‰í–ˆë‹¤. ë˜í•œ visual-question-answering(VQA), image captioning, established language modeling taskì™€ ê°™ì€ ì¼ë°˜ì ì¸ vision-language taskì—ì„œë„ PaLM-Eë¥¼ í‰ê°€í–ˆë‹¤.
<br/><br/>

ë…¼ë¬¸ì˜ ì„¸ ê°€ì§€ robot environmentì—ëŠ” ë¡œë´‡ì´ ë¬¼ì²´ë¥¼ manipulate(ì¡ê³  ìŒ“ê¸°)í•´ì•¼ í•˜ëŠ” Task and Motion Planning (TAMP) domain, table-top pushing environment, mobile manipulation domainì´ í¬í•¨ëœë‹¤. ê° ë„ë©”ì¸ì—ì„œ PaLM-EëŠ” í•´ë‹¹ domainì˜ ë°ì´í„°ë¡œ í•™ìŠµëœë‹¤. 

![PaLM-E_5.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/8e670753-7df2-4163-aefa-da20f19a888d){: width="500px"}

ì €ìëŠ” ë‹¤ì–‘í•œ taskê³¼ robot embodimentì—ì„œ ì—¬ëŸ¬ ë°ì´í„°ì…‹ì„ í˜¼í•©í•˜ì—¬ í•™ìŠµí•œ ëª¨ë¸ì´ ëª¨ë“  taskì—ì„œ ë™ì‹œì— ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤. ìœ„ì˜ ê·¸ë¦¼ì„ ì‚´í´ë³´ë©´, ë‹¤ë¥¸ taskì™€ embodimentì—ë„ ë¶ˆêµ¬í•˜ê³ , ê°œë³„ taskì— ëŒ€í•œ ì„±ëŠ¥ì€ ì‘ì—… mixtureì— ëŒ€í•œ í•™ìŠµì„ í†µí•´ ì¦ê°€í–ˆë‹¤.
<br/><br/><br/><br/>

#### TAMP Environment.

![PaLM-E_6.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/488bd219-623e-468b-b3d3-48160e9400fd){: width="1000px"}

TAMP í™˜ê²½ì— ëŒ€í•œ ê³„íš ì„±ê³µë¥ ê³¼ VQA ì„±ëŠ¥ì— ëŒ€í•œ ì‹¤í—˜ì„ ì§„í–‰í–ˆë‹¤.. LLMì€ pre-trainëœ ìƒíƒœë¡œ frozenë˜ì—ˆê³  input representationì€ TMAP ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œë§Œ í•™ìŠµë˜ì—ˆë‹¤(ë°ì´í„° í˜¼í•© ì—†ìŒ). ê²°ê³¼ëŠ” ìœ„ í‘œì™€ ê°™ë‹¤.
<br/><br/><br/><br/>

#### Language-Table Environment.

![PaLM-E_7.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/ce580401-2cc7-47c6-992e-c2ceb29aed71){: width="1100px"}

ìœ„ì˜ ê·¸ë¦¼ì„ í†µí•´  PaLM-EëŠ” ì ëŒ€ì ì¸ êµë€ì— ê°•ì¸í•¨ì„ ìœ ì§€í•˜ë©´ì„œë„ multi-stage tabletop manipulation taskì„ í†µí•´ ì‹¤ì œ ë¡œë´‡ì„ ì•ˆë‚´í•  ìˆ˜ ìˆìŒì„ ë³¼ ìˆ˜ ìˆë‹¤. 
<br/><br/><br/><br/>

#### Performance on General Language Tasks.

![PaLM-E_8.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/033d1ab0-6a16-4ec0-ad0a-67b78beef5f2){: width="550px"}

OKVQ, VQA v2, COCO captionì„ í¬í•¨í•œ general vision-language taskì— ëŒ€í•œ ê²°ê³¼ëŠ” ìœ„ì™€ ê°™ë‹¤.

<br/><br/><br/><br/>
