---
title: "[논문리뷰] CLIP, Learning Transferable Visual Models From Natural Language Supervision"
author: lunalee
date: 2024-01-15 20:30:28 +0500
categories: [AI, Paper Review]
tags: [Multi-modal, Zero-shot]
pin: false
math: true
---

<br/>
`Open AI` `ICML 2021`

- Paper: [https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020)
- Git: [https://github.com/OpenAI/CLIP](https://github.com/OpenAI/CLIP)
- Project: [https://openai.com/research/clip](https://openai.com/research/clip)
<br/><br/><br/>

# Introduction and Motivating Work

---

 Raw-text로부터 pre-traing하는 방법은 NLP분야에서 큰 성능을 거두었다. Text-to-text의 발달은 Task-agnostic한 구조에서 별다른 커스터마이즈 없이 down-stream dataset으로의 zero-shot transfer를 가능하게 했다. GPT-3와 같은 시스템은 특정한 학습 데이터 없이도 많은 task에서 뛰어난 성능을 보여주고 있다.

이것은 NLP에서는 이러한 방법이 high-quality crowd-labeled 데이터셋을 능가한다는 것을 의미한다. 하지만 컴퓨터 비전과 같은 분야에서는 여전히 ImageNet과 같이 사전에 라벨이 지정된 데이터셋에서 모델을 pre-train하는 것이 일반적이다. 그렇다면 NLP와 같은 방법이 컴퓨터 비전에서도 유사한 혁신을 가져올 수 있을까?

이전에도 image representation learning을 위해 natural language supervision을 사용하는 다양한 시도가 있었지만, 일반적인 벤치마크에서 입증된 성능이 다른 접근 방식보다 훨씬 낮다. 예를 들어, Li et al. (2017)은  zero-shot setting에서 ImageNet의 정확도가 11.5%에 불과했다. 이는 현재 SOTA 88.4%보다 훨씬 낮다. 대신 weak supervision의 방법들을 사용하여 성능을 개선시켰지만, 이 또한 유연성이 떨어져 zero-shot 성능을 제한시킨다.

본 연구에서는 이러한 격차를 줄이고 natural language supervision을 통해 학습된 image classifier를 대규모로 연구했다. 공개적으로 사용 가능한 대량의 데이터를 활용하여 4억 개의(이미지, 텍스트) 쌍으로 구성된 새로운 데이터 세트를 생성했으며, CLIP이라는 모델을 from scratch로 학습했다.  CLIP은 다양한 task로 확장될 수 있음을 증명하고, 특히 공개적으로 사용 가능한 최고의 ImageNet 모델을 능가하는 동시에 계산 효율성도 더 높다는 것을 증명한다. 또한 zero-shot CLIP 모델이 동등한 supervised ImageNet model보다 강력하다는 것을 보였다.
<br/><br/><br/><br/>

# Method

---

## 1. Natural Language Supervision

본 논문의 접근 방법의 핵심은 natural language에 포합된 supervision으로부터 perception을 학습한다는 것이다. Unsupervised, self-supervised, weakly supervised 등 다양한 방법으로 학습되어온 이 개념은, 공통적으로 자연어를 training signal로 인식한다는 점이다. 이러한 모든 접근 방식은 **natural language supervision** 을 통해 학습된다.

자연어를 통한 학습은 다른 학습 방법에 비해 몇 가지 잠재적인 장점이 있다. 인터넷의 방대한 양의 텍스트에 포함된 supervision을 통해 수동적으로 학습할 수 있으므로 고전적인 방법과 같은 “gold label”이 필요하지 않아 확장성이 뛰어나다(crowed-source label이 필요하지 않음). 또한, 자연어를 통한 학습은 해당 representation을 language에 연결하여 유연한 zero-shot을 가능하게 한다는 점에서 대부분의 unsupervised, self-supervised 접근 방식에 비해 이점이 있다.
<br/><br/><br/><br/>

## 2. Creating a Sufficiently Large Dataset

기존의 연구에서는 MS-COCO, Visual Genome, YFCC100M 세가지 데이터를 주로 사용했다. 이 데이터들은 현대 표준에 따르면 규모가 작거나, 품질이 떨어지는 문제가 있다.

자연어 감독의 주요 동기는 인터넷에서 공개적으로 사용할 수 있는 대량의 데이터인데, 기존의 데이터셋은 이 장점을 제대로 반영하고 있지 않다. 따라서 저자는 인터넷에서 공개적으로 이용 가능한 다양한 소스로부터 수집된 4억 쌍(이미지, 텍스트)의 새로운 데이터 세트를 구축했다. 
<br/><br/><br/><br/>

## 3. Selecting an Efficient Pre-Training Method

SOTA 컴퓨터 비전 시스템은 매우 많은 양의 컴퓨팅을 사용한다. 이 시스템이 1000개의 ImageNet 클래스만 예측하도록 훈련되었다는 점을 고려해, 저자는 training 효율성이 natural language supervision을 성공적으로 확장하는데 핵심이라고 생각했다. 
<br/><br/>

저자는 초기에 VirTex와 유사한 방식으로 이미지 캡션을 예측하기 위해 이미지 CNN + 텍스트 Transforemer를 동시에 훈련했다. 그러나 이 방법을 효율적으로 scaling하는데는 어려움이 있었다.

![CLIP_1](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/a696a223-d122-4363-8a8c-1b26957633a2){: width="600px"}

위 그림을 보면 ResNet-50 이미지 인코더의 계산량을 2배로 사용하는 transforemr 모델이 훨씬 단순한 baseline 모델보다 3배나 느리게 ImageNet class를 학습하고 있었다. 이 방식은 각 이미지의 **정확한** 단어를 예측(predict)하려고 헀다. 하지만 정확한 단어를 예측하는 것은 묘사방법의 다양성 등의 문제로 어려운 작업이다. 따라서 본 논문에서는 해당 텍스트의 정확한 단어가 아닌 **전체 텍스트가 어떤 이미지와 쌍**을 이루는지 예측하는 더 쉬운 방법으로 학습을 진행헀다**(predictive object → contrastive object**). 위 그림에서도 확인할 수 있듯이 contrastive 방법으로 교체하고 zero-shot transfer에서 효율성이 4배 향상되었다. 
<br/><br/><br/>

![CLIP_2](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/6ee488ce-9081-4459-9ef3-e152546398ec){: width="700px"}

N개 pair(이미지, 텍스트)의 배치가 주어지면 CLIP은 배치 전체에서 가능한 $N × N$ pair(이미지, 텍스트) 중 맞는 pair를 찾도록 학습된다. 이를 위해 CLIP은 Image Encoder와 Text Encoder를 공동으로 training하여 batch에서 N개의 실제 쌍의 Image와 Text 임베딩의 Consine similarity를 최대화하고, $N^2 - N$개의 잘못된 쌍에 대해서는 Consine similarity를  최소화함으로써 multi-modal embedding space를 학습한다(symmetric cross entropy 사용). 

<br/><br/><br/>
![CLIP_3](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/4ff6cdb1-2128-4019-9644-e80342dcb7c1){: width="500px"}

symmetric cross entropy loss를 구하는 pseudo code는 위와 같다.

저자는 representation과 contrastive embedding space 사이에 non-linear projection을 사용하지 않고, 대신 encoder의 representation을 multi-modal embedding space에 매핑하는 linear projection만 사용한다. 또한 데이터셋이 단일 문장이기 때문에 text transformeation 도 제거했고, Image transformer도 단순화했다. Data augmentation으로는 random square crop만을 사용했다.
<br/><br/><br/><br/>

## 4. Choosing and Scaling a Model

**Image Encoder**

1. **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>ResNet-50</span></mark>** <br/>
   Original 버전에서 수정된 ResNet-D를 사용했다. Antialiased rect-2 blur pooling를 사용했으며, global average pooling를 Attention pooling으로 교체했다.
2. **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Vision Transformer(ViT)</span></mark>** <br/>
   추가적인 Layer normalization 이외에 기존의 모델을 거의 수정하지 않고 사용했다.

<br/><br/>

**Text Encoder**

1. Transformer 
  BPE(Byte Pair Encoding) representation을 사용했으며, Text sequence는 [SOS]와 [EOS]를 묶어서 사용했다. 계산 효율성을 위해 max sequence length는 76으로 제한했다.
<br/>
<details>
  <summary>BPE(Byte pair encoding)</summary>
  <div markdown="1">
  BPE(Byte pair encoding) 알고리즘은 데이터 압축 알고리즘이다.  연속적으로 가장 많이 등장한 글자의 쌍을 찾아서 하나의 글자로 병합하는 방식을 수행하는 알고리즘이지만, 자연어 처리에서는 서브워드 분리 알고리즘으로 응용되었다.<br/>

  자연어 처리에서의 BPE는 서브워드 분리(subword segmentation) 알고리즘으로, 기존에 있던 단어를 분리한다는 의미이다. BPE을 요약하면, 글자(charcter) 단위에서 점차적으로 단어 집합(vocabulary)을 만들어 내는 Bottom up 방식의 접근을 사용한다. 우선 훈련 데이터에 있는 단어들을 모든 글자(chracters) 또는 유니코드(unicode) 단위로 단어 집합(vocabulary)를 만들고, 가장 많이 등장하는 유니그램을 하나의 유니그램으로 통합하는 방법이다.<br/>

  참조:[https://wikidocs.net/22592](https://wikidocs.net/2259)
  </div>
</details>

<br/><br/>

기존 Computer Vision 연구들은 모델의 Depth 또는  Width를 늘려 모델의 scale을 증가시켰지만 하나의 차원만 증가시키는 것 보다  Width, Depth, Resolution 전체에 걸쳐 scaling 하는 것이 더 뛰어나다는 성능을 발견했다고 한다. 반면 Text Encoder는 모델의 Width만 증가시키고 Depth는 증가시키지 않았다. 방대한 양의 텍스트 데이터로 인해 CLIP의 성능이 Text Encoder의 성능에 덜 민감했기 때문이다.
<br/><br/><br/><br/>

## 5. Training

 5개의 ResNet과 3개의 Vision Transfermer를 학습했다. ResNet의 경우 ResNet-50, ResNet-101, EfficientNet 스타일 모델,  ResNet-50 컴퓨팅의 약 4배, 16배, 64배를 사용하는 3개를 더  학습했다. Vision Transformer의 경우 ViT-B/32, ViT-B/16 및 ViT-L/14를 학습했다.

 Adam Optimizer 사용, Cosine Scheduler를 사용하여 learning rate를 감소시켰다.
<br/><br/><br/><br/>

# Experiments

---

## 1. Zero-Shot Transfer

Computer Vision에서 Zero-shot learning은 일반적으로 Image classification에서 본 적이 없는 카데고리(unseen object categoty)에 대한 예측을 의미한다. 이 논문에서는 좀 더 넓은 범위로 한번도 보지 못한 데이터(unseen data)로 의미를 확장하였다.
<br/><br/>

![CLIP_4](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/5a785993-443d-4a53-aae5-a44c2fba9288){: width="700px"}

CLIP은 데이터셋에서 이미지와 텍스트가 쌍을 이루는지 예측하도록 사전 학습되었다. 저자들은 zero-shot 분류를 수행하기 위해 이 방법을 다시 사용하였다. 데이터셋의 모든 클래스와 이미지(이미지 임베딩) - 텍스트(텍스트 임베딩) 쌍에 대해 코사인 유사성은 계산해 가장 높은 확률을 가진 쌍을 출력한다. 
<br/><br/><br/>

![CLIP_5](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/f3355603-7292-4a3b-9a7d-8a86bd5e652b){: width="500px"}

대부분의 데이터 세트는 label의 숫자 ID만으로 annotation한다. 또한 단어의 다의성 때문에 클래스 이름 외에 추가적인 정보가 없는 경우 컨텍스트가 부족하여 어떤 단어 의미를 의미하는지 구별할 수 없다. 저자는 이러한 문제를 위해 class 이름을 그대로 넣지 않고, “A photo of a {label}” 과 같은 문장 형식으로 label을 구성했다. 이렇게 Task에 맞는 prompt를 고려하는 경우, 더 좋은 성능을 보여준다.
<br/><br/><br/>

![CLIP_6](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/ce79dc38-bb9d-4910-8ad5-ee13d7372929){: width="500px"}

위 그림은 이러한 텍스트 인코더, 이미지 인코더의 구조의 CLIP이 label이 모두 주어진 학습과 비교해서 경쟁력있는 성능을 보여주고 있는것을 나타낸다. Zero-shot CLIP이 27개 데이터 세트 중 16개에서 높은 성능을 보이고 있지만, 보다 복잡한 작업에서 제로샷 CLIP의 성능이 떨어지는 것을 보여주고 있다.
<br/><br/><br/>

![CLIP_7](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/0e8ba7e9-ba02-4962-8bea-143e968dcec2){: width="500px"}

Zero-shot CLIP이 Few-shot Learning 작업을 수행한 다른 모델보다 훨씬 뛰어난 성능을 보여주는 것을 볼 수 있다.
<br/><br/><br/><br/>

## 2. Representation Learning

이전 섹션에서 보았던 결과만으로는 CLIP이 좋은 representation을 학습했는지 평가하기는 어렵다. 일반적으로는 추출한 representation을 가지로 linear classifier를 학습시켜 성능을 측정하는 것이 일반적이기 때문이다. 대안으로 CLIP의 Feature extractor 부분은 frozen하고 Classifier 부분에 대해 fine-tuing(Linear Probing)하여 성능을 측정했다. 
<br/><br/>

![CLIP_8](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/c4051ccb-4a9b-49c2-96c5-5008062a6d79){: width="800px"}
<br/>

위의 결과에서 증명하듯 대부분의 모델보다 CLIP이 나은 성능을 보여주고 있다. 이를 통해 CLIP이 효과적인 representation을 학습하고 있다고 볼 수 있다. 
<br/><br/><br/><br/>

## 3. Robustness to Natural Distribution Shift

![CLIP_9](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/1d615fd0-9fb4-4220-b27b-f2c543b16f30){: width="800px"}
<br/>

다음으로 저자들은 Robustness에 관한 실험을 진행했다. 직관적으로 제로샷 모델은 특정한 데이터셋 분포에 대해 훈련되지 않았기 때문에 특정 분포에만 적용되는 허위 correlation이나 pattern을 활용할 수 없어야 한다. 위의 그래프를 통해 CLIP이 transfer 성능이 다른 모델들보다 우수한 것을 볼 수 있다.
<br/><br/><br/>

![CLIP_10](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/1c14df95-ec75-4096-85c0-39e65ae22b22){: width="800px"}
<br/>

Natural distribution shifts에 대한 Zero-shot CLIP의 성능을 기존 ImageNet 모델과 비교했다. CLIP의 robust함을 증명하기 위해 ImageNet에 변형이 가해진 데이터셋에 대한 실험을 진행했다. 그래프를 보면 ResNet 모델보다 CLIP의 Zero-shot 성능이 더 높은 점수를 보이고 있다. 특히 이미지의 형태가 크게 바뀌는 데이터셋에서 ResNet은 큰 성능 감소를 보이는 반면 CLIP은 비교적 잘 작동하는 것을 볼 수 있다.

<br/><br/><br/>
