---
title: "[ë…¼ë¬¸ ë¦¬ë·°] BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing"
author: lunalee
date: 2024-05-16 19:41:11 +0900
categories: [AI, Paper Review]
tags: [Multi-modal, VLP]
pin: false
math: true
---

<br/><br/>
`Salesforce AI Research`  `arXiv 2023`

- Paper: [https://arxiv.org/abs/2305.14720](https://arxiv.org/abs/2305.14720)
- Git: [https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion)
- Page: [https://dxli94.github.io/BLIP-Diffusion-website/](https://dxli94.github.io/BLIP-Diffusion-website/)
<br/><br/><br/>

#### ğŸ“– í•µì‹¬ í›‘ì–´ë³´ê¸° !!

- Subject-driven text-to-image generationì„ ëª©ì ìœ¼ë¡œ, ì¼ë°˜ì ì¸ subject representationì„ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ëŠ” BLIP-Diffusion ëª¨ë¸ ì œì•ˆ
- ì¼ë°˜ì ì¸ subject representationì„ í•™ìŠµí•˜ê¸° ìœ„í•œ two-stage pre-training ì „ëµ ì œì‹œ
    1.  multimodal representation learning: BLIP-2ë¥¼ ì ìš©, ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ text-aligned visual featureë¥¼ ìƒì„±
    2. subject representation learning: 1-stageì—ì„œ ìƒì„±í•œ subject representationì„ ì´ìš©í•´ diffusion ëª¨ë¸ì´ ìƒˆë¡œìš´ ë³€í˜• ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµ
- Pre-trainëœ BLIP-Diffusion ëª¨ë¸ì„ foundation generation ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ê³ , ë‹¤ì–‘í•œ ê¸°ì¡´ ëª¨ë¸ì„ í†µí•©í•˜ì—¬ ì¶”ê°€ì ì¸ í•™ìŠµ ì—†ì´ ë‹¤ì–‘í•œ taskì— ì ìš©(ControlNet, Prompt-to-prompt)
<br/><br/><br/><br/>

# Introduction

---

í•´ë‹¹ ë…¼ë¬¸ì€ ìƒë‹¹ ë¶€ë¶„ BLIP-2ì— ê¸°ë°˜í•˜ë¯€ë¡œ BLIP-2ì— ëŒ€í•œ ì‚¬ì „ ì§€ì‹ì´ í•„ìš”í•˜ì‹  ë¶„ì€ ë¸”ë¡œê·¸ ë‚´ì˜ [[BLIP-2 ë…¼ë¬¸ ë¦¬ë·°](https://lunaleee.github.io/posts/BLIP-2/)] ê¸€ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”!
<br/><br/>

![BLIP-Diffusion_1.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/b339e350-415b-4c50-bc8e-8b636fd165f2){: width="900px"}

**<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Subject-driven(ì£¼ì œ ê¸°ë°˜) generation</span></mark>**ì€ Text-to-image generation task ì¤‘ í•˜ë‚˜ë¡œ **ì…ë ¥ subject(ì£¼ì œ)ì˜ ëª¨ì–‘ì„ ìœ ì§€**í•˜ë©´ì„œ **ì´ë¯¸ì§€ë¥¼ ìƒˆë¡­ê²Œ ë³€í˜•í•˜ëŠ” ê²ƒ**ì„ ì˜ë¯¸í•œë‹¤(ìœ„ ê·¸ë¦¼ ì°¸ì¡°). ì¼ë°˜ì ìœ¼ë¡œ Pre-trainëœ text-image genration ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬, íŠ¹ì • text embeddingì„ ë³€ê²½í•´ê°€ë©° ì´ì— í•´ë‹¹í•˜ëŠ” image ì§‘í•©ì„ ì¬êµ¬ì„±í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•œë‹¤. ì´ì™€ ê°™ì€ ë°©ë²•ì€ ê°ê°ì˜ subjectì— ëŒ€í•´ fine-tuning stepì„ ê±°ì³í•˜ê¸° ë•Œë¬¸ì—, ê´‘ë²”ìœ„í•œ subjectì— ëŒ€í•´ í™•ì¥í•˜ê¸° ì–´ë ¤ìš´ ë¬¸ì œê°€ ìˆë‹¤. 
<br/><br/>

ì €ìëŠ” ì´ëŸ¬í•œ ë¬¸ì œê°€ ëŒ€ë¶€ë¶„ì˜ pre-trainëœ text-to-image ëª¨ë¸ì´ imageì™€ text ëª¨ë‘ë¥¼ control ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” **multimodal controlì´ ë¶ˆê°€ëŠ¥**í•˜ê¸° ë•Œë¬¸ì´ë¼ê³  í•œë‹¤. Subject visualì„ ë†’ì€ ì¶©ì‹¤ë„ë¡œ ìº¡ì²˜í•˜ë©´ì„œ, text spaceì™€ ì˜ alignë˜ëŠ” subject representationì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì–´ë µë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>subject-driven text-to-image generationì„ ìœ„í•œ BLIP-Diffusion</span></mark>**ì„ ì œì•ˆí•œë‹¤. BLIP-Diffusionì€ zero-shot ë˜ëŠ” few-step fine-tuningë§Œìœ¼ë¡œë„ subject-driven generationì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>generic subject representation</span></mark>**ì„ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤*.* 
<br/><br/><br/>

BLIP-Diffusionì—ì„œëŠ” generic subject representation í•™ìŠµì„ ìœ„í•œ two-stage pre-training ì „ëµì„ ì œì‹œí–ˆë‹¤. 

1. BLIP-2ë¥¼ ì ìš©, ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ text-aligned visual featureë¥¼ ìƒì„±í•˜ëŠ” **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>multimodal representation learning</span></mark>** ìˆ˜í–‰
2. Diffusion ëª¨ë¸ì´ subject ê¸°ë°˜ ìƒˆë¡œìš´ ë³€í˜• ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ëŠ” **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>subject representation learning task</span></mark>** ì„¤ê³„
<br/><br/><br/><br/><br/><br/>

# Method

---

BLIP-Diffusionì€ pre-trainëœ subject representationì„ í†µí•´ multimodal controlì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ëª¨ë¸ì´ë‹¤. ì´ë¥¼ ìœ„í•´ subject-specific visual appearanceë¥¼ í¬ì°©í•˜ëŠ” ë™ì‹œì— text promptì™€ ì¼ì¹˜í•˜ëŠ” subject representationì„ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í•œ ëŒ€ë¡œ two-stage pre-training ì „ëµì„ ì‚¬ìš©í–ˆë‹¤.
<br/><br/>

## 1. Multimodal Representation Learning with BLIP-2

![BLIP-Diffusion_2.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/b4c2200c-6c20-495d-a75f-68c128634bfa){: width="400px"}

ë¨¼ì € ìƒì„±ëª¨ë¸ë¡œ **Stable Diffusion** ëª¨ë¸(ì°¸ì¡°: [Stable Diffusion ë…¼ë¬¸ ë¦¬ë·°](https://lunaleee.github.io/posts/StableDiffusion/))ì„ ì‚¬ìš©í•œë‹¤. ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ text embeddingì€ CLIPì—ì„œ ìƒì„±ë˜ì–´ ì „ë‹¬ëœë‹¤. ì´ ë•Œ promptë¡œ ì‚¬ìš©ë˜ëŠ” subject representationê³¼ textê°€ ì„œë¡œ ì˜ align ë˜ì–´ìˆëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. 

ë…¼ë¬¸ì—ì„œëŠ” vision-language pre-trained ëª¨ë¸ì¸ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>BLIP-2</span></mark>**ë¥¼ ì‚¬ìš©í•˜ì—¬ high-quality **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>text-aligned visual representation</span></mark>**ë¥¼ ìƒì„±í•œë‹¤.
<br/><br/><br/>

![BLIP-Diffusion_3.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/63899b32-eb88-44c4-9008-3f9b6fbb0799){: width="500px"}

êµ¬ì²´ì ìœ¼ë¡œëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´, **frozen pre-trained Image Encoder**ì™€ **Q-Former**(multi-modal encoder, BLIP-2 ë…¼ë¬¸ë¦¬ë·° ì°¸ì¡°)ë¡œ êµ¬ì„±ë˜ì–´ìˆë‹¤. 

BLIP-2ì™€ ë§ˆì°¬ê°€ì§€ë¡œ Q-FormerëŠ” ê³ ì •ëœ ìˆ˜ì˜ learnable query tokenì„ ì‚¬ìš©í•œë‹¤. Query ì§‘í•©ì€ self-attention layerë¥¼ í†µí•´ textì™€ ìƒí˜¸ì‘ìš©í•˜ê³ , cross-attention layerë¥¼ í†µí•´ image featureì™€ ìƒí˜¸ì‘ìš©í•˜ì—¬ ì¶œë ¥ìœ¼ë¡œ text-aligned image featureë¥¼ ìƒì„±í•œë‹¤(BLIP-2ì™€ ë™ì¼).

ì €ìëŠ” ê¸°ì¡´ BLIP-2ì™€ ê°™ì´ queryë¥¼ 32ê°œë¡œ êµ¬í˜„í•˜ë©´(=output featureë„ 32ê°œ), output featureê°€ CLIP text embeddingì— ë¹„í•´ ë„ˆë¬´ ê°•í•´ì ¸ ì´ë¯¸ì§€ ìƒì„±ì— ì ì ˆí•˜ê²Œ ì¡°í•©ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ, query tokenì˜ ìˆ˜ë¥¼ ì ˆë°˜ì¸ 16ê°œë¡œ êµ¬í˜„í–ˆë‹¤ê³  í•œë‹¤.
<br/><br/>

ë˜í•œ BLIP-2 pre-training ë°©ë²•ê³¼ ë™ì¼í•˜ê²Œ ITC, ITG, ITM lossë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í–ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ì¼ë°˜ì ì¸ image-text paired dataì— ëŒ€í•œ multimodal representation learningì„ í†µí•´ ëª¨ë¸ì´ ë‹¤ì–‘í•œ visual, textual conceptì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.
<br/><br/><br/><br/><br/><br/>

## 2. Subject Representation Learning with Stable Diffusion

Multimodal representation learningì„ í†µí•´ ì…ë ¥ ì´ë¯¸ì§€ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì˜ë¯¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì˜€ìœ¼ë¯€ë¡œ, ë‹¤ìŒìœ¼ë¡œëŠ” Diffusion ëª¨ë¸ì´ ì´ëŸ¬í•œ visual representationì„ í™œìš©í•˜ì—¬ subjectì˜ ë³€í˜• ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.
<br/><br/><br/>

#### Model Architecture.

![BLIP-Diffusion_4.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/e7c87f11-89ab-44e0-9e16-cec0bd98b657){: width="700px"}

ì „ì²´ì ì¸ ëª¨ë¸ êµ¬ì¡°ëŠ” ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ë‹¤. Subject Representation Learningì€ ì•„ë˜ì™€ ê°™ì€ ë‹¨ê³„ë¥¼ ê±°ì³ í•™ìŠµëœë‹¤. 
<br/><br/>

- (Multimodal representation learning stage) BLIP-2ì˜ Q-Former(multi-modal encoder)ëŠ” pre-train ê³¼ì • ì¤‘ subject ì´ë¯¸ì§€, subject categoryê°€ í¬í•¨ëœ textë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ category-aware subject visual representationì„ ìƒì„±í–ˆë‹¤.
- Q-Formerì˜ ì¶œë ¥, ì¦‰ subject representationì€ feed-forward layer(ë‘ ê°œì˜ Linear layerì™€ GELUë¡œ êµ¬ì„±)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€í™˜ëœë‹¤.
- ë³€í™˜ëœ featureëŠ” text prompt token embeddingì— **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>soft visual subject prompt</span></mark>**ë¡œ ì¶”ê°€ëœë‹¤. ì´ ë•Œ ì•„ë˜ì™€ ê°™ì€ templateì„ ì‚¬ìš©í•œë‹¤.
<br/>

$$
\text{â€œ[text prompt], the [subject text] is [subject prompt]"}
$$

- ë§ˆì§€ë§‰ìœ¼ë¡œ, ê²°í•©ëœ text / subject embeddingì€ CLIP text encoderë¡œ ì „ë‹¬ë˜ì–´ diffusion ëª¨ë¸ì´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” guidance ì—­í• ì„ í•œë‹¤.

<blockquote class="prompt-info"> ì—¬ê¸°ì„œ soft visual promptëŠ” ê¸°ë³¸ diffusion ëª¨ë¸ì— ìµœì†Œí•œì˜ êµ¬ì¡°ì  ë³€ê²½ì„ ê°€í•˜ì—¬ subject representationì„ ì£¼ì…í•˜ëŠ” ë™ì‹œì—, ê¸°ë³¸ diffusion ëª¨ë¸ì˜modeling capabilityë¥¼ ë¬¼ë ¤ë°›ì„ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ë¼ê³  í•œë‹¤.</blockquote>
<br/><br/><br/>

#### Subject-generic Pre-training with Prompted Context Generation.

ì…ë ¥ ì´ë¯¸ì§€ì—ì„œ genericí•œ subjectë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì€ multi-modal encoder(ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Q-Former)ì˜ ì…ë ¥ê³¼ diffusion modelì˜ ì…ë ¥ìœ¼ë¡œ ë™ì¼í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í–ˆë‹¤. ì €ìê°€ ì§„í–‰í•œ ì‚¬ì „ ì‹¤í—˜ì—ì„œ ì´ì™€ ê°™ì€ ë°©ë²•ì€ ì…ë ¥ì˜ ë°°ê²½ì— ì˜í•´ í¬ê²Œ ê°„ì„­ë°›ê±°ë‚˜, ì…ë ¥ê³¼ ë³„ë°˜ ë‹¤ë¥¼ë°” ì—†ëŠ” ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ë‚´ëŠ” trivial solutionìœ¼ë¡œ ì´ì–´ì§€ëŠ” ë¬¸ì œê°€ ìˆì—ˆë‹¤ê³  í•œë‹¤. 
<br/><br/>

ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ BLIP-DIffusionì—ì„œëŠ” ìƒˆë¡œìš´ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>prompted context generation</span></mark>**ì„ ì œì•ˆí–ˆë‹¤. Randomí•œ ë°°ê²½ì— subject ì´ë¯¸ì§€ë¥¼ í•©ì„±í•˜ì—¬ input-target training pairë¥¼ ìƒì„±í•˜ê³ , ëª¨ë¸ì€ ì´ í•©ì„±ëœ subject ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì€ ë’¤ text promptì— ë”°ë¼ ì›ë³¸ subject ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì€ ë‹¨ê³„ë¥¼ ê±°ì¹œë‹¤.
<br/><br/>

- Subjectê°€ í¬í•¨ëœ ì´ë¯¸ì§€ì™€ í•´ë‹¹ category textë¥¼ **text-prompted segmentation ëª¨ë¸**ì¸ [CLIPSegğŸ“„](https://arxiv.org/abs/2112.10003)ì— ë„£ëŠ”ë‹¤.
- CLIPSeg ì¶œë ¥ segmentation mapì—ì„œ ë” ë†’ì€ confidenceë¥¼ ê°€ì§„ ë¶€ë¶„ì„ known foreground, ë‚®ì€ confidenceë¥¼ uncertain region, ë‚˜ë¨¸ì§€ë¥¼ known backgroundë¡œ ì„¤ì •í•˜ì—¬ **trimap**ì„ ìƒì„±í•œë‹¤.
- Trimapì´ ì£¼ì–´ì§€ë©´, closed-form mattingì„ ì‚¬ìš©í•˜ì—¬ foreground, ì¦‰ subjectë¥¼ ì¶”ì¶œí•œë‹¤.
- ì¶”ì¶œëœ subjectë¥¼ alpha blendingì„ ì‚¬ìš©í•˜ì—¬ randomí•œ background ì´ë¯¸ì§€ì— í•©ì„±í•œë‹¤.
- í•©ì„± ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ì›ë˜ subject ì´ë¯¸ì§€ë¥¼ ì¶œë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ í•˜ë‚˜ì˜ í•™ìŠµ ì´ë¯¸ì§€ pairë¡œ ì‚¬ìš©í•œë‹¤.
<br/><br/><br/>

![BLIP-Diffusion_5.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/d4f9f8b5-c79f-4dfc-a53e-8164453606d8){: width="600px"}

ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ í•©ì„± pairëŠ” subjectì™€ backgroundë¥¼ ë¶„ë¦¬í•˜ì—¬ subjectì™€ ë¬´ê´€í•œ ì •ë³´ê°€ promptì— ì¸ì½”ë”©ë˜ëŠ” ê²ƒì„ ë°©ì§€í•œë‹¤. ìœ„ ë°©ì‹ìœ¼ë¡œ diffusion ëª¨ë¸ì€ subject prompt, text promptë¥¼ í•¨ê»˜ ê³ ë ¤í•œ pre-train ëª¨ë¸ë¡œ í•™ìŠµëœë‹¤.

Pre-train ì¤‘ì— image encoderë¥¼ frození•˜ê³  diffusion ëª¨ë¸ì˜ text encoder(CLIP)ì™€ U-Net, BLIP-2ì˜ Q-Formerë¥¼ ê³µë™ìœ¼ë¡œ í•™ìŠµí•œë‹¤. ì›ë˜ì˜ text-to-image generation ê¸°ëŠ¥ì„ ë” ì˜ ë³´ì¡´í•˜ê¸° ìœ„í•´, diffusion guideë¡œ text promptë§Œ ì‚¬ìš©í•˜ë©´ì„œ subject promptë¥¼ 15% í™•ë¥ ë¡œ random ì‚­ì œí–ˆì„ ë•Œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤ê³  í•œë‹¤.
<br/><br/><br/><br/><br/><br/>

## 3. Fine-tuning and Controllable Inference

ì´ë ‡ê²Œ pre-trainëœ subject representationì€ zero-shot generation ë¿ ì•„ë‹ˆë¼ íŠ¹ì • custom subjectì— ëŒ€í•œ fine-tuningë„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤. ë¿ë§Œ ì•„ë‹ˆë¼ ê¸°ì¡´ diffusion ëª¨ë¸ì˜ ê¸°ëŠ¥ì„ í™œìš©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, BLIP-Diffusion ëª¨ë¸ì„ foundation generation ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ê³  ì¶”ê°€ì ìœ¼ë¡œ image generation / editing ê¸°ìˆ ì„ í™œìš©í•  ìˆ˜ë„ ìˆë‹¤. 
<br/><br/>

#### Subject-specific Fine-tuning and Inference.

ë¨¼ì €  pre-trained generic subject representationì„ ì‚¬ìš©í•˜ì—¬ ê°œë³„ì ì¸ subjectì— ëŒ€í•œ fine-tuning ê³¼ì •ì— ëŒ€í•´ ì•Œì•„ë³´ì. ì•„ë˜ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ, ë‹¨ì¼ A100 GPUì—ì„œ 20~40ì´ˆ ì •ë„ì˜ fine-tuning ì‹œê°„ì´ ê±¸ë¦°ë‹¤ê³  í•œë‹¤.

- ëª‡ ê°œì˜ subject ì´ë¯¸ì§€ì™€ subject category textê°€ ì£¼ì–´ì§€ë©´, multi-modal encoder(Q-Former)ë¥¼ ì‚¬ìš©í•˜ì—¬  ê°œë³„ì ì¸ subject representationì„ ì–»ëŠ”ë‹¤.
- ê·¸ ë‹¤ìŒ ëª¨ë“  subject ì´ë¯¸ì§€ì˜ subject representationì˜ í‰ê· ìœ¼ë¡œ subject prompt embeddingì„ ì´ˆê¸°í™”í•œë‹¤. ì´ ë°©ì‹ìœ¼ë¡œ fine-tuning ê³¼ì • ì¤‘ì—ëŠ” multi-modal encoderì˜ í•™ìŠµì€ í•„ìš”í•˜ì§€ ì•Šë‹¤.
- Diffusion ëª¨ë¸ì€ text prompt embeddingê³¼ í‰ê·  subject embeddingì„ ì‚¬ìš©í•˜ì—¬ target ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë„ë¡ fine-tuning ëœë‹¤.
- ì—¬ê¸°ì„œ diffusion ëª¨ë¸ì˜ text encoder(CLIP)ì€ frozen í•˜ì—¬ subject ì´ë¯¸ì§€ì— ëŒ€í•œ overfittingì„ ë§‰ëŠ”ë‹¤.
<br/><br/><br/><br/>

#### Structure-controlled Generation with ControlNet.

![BLIP-Diffusion_6.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/38bba073-997c-42d1-95c7-00cfb1c170f1){: width="500px"}

ì¶”ê°€ì ìœ¼ë¡œ subject-controlì„ ìœ„í•œ multimodal conditioning mechanismì„ ë„ì…í–ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>ControlNet</span></mark>**ì„ í†µí•©í•˜ì—¬ structure-controlledê³¼ subject-controlled generationì„ ë™ì‹œì— ê°€ëŠ¥í•˜ê²Œ í–ˆë‹¤.

ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ pre-train ëœ ControlNetì˜ U-Net êµ¬ì¡°ë¥¼ BLIP-Diffusionì— ì—°ê²°í•œë‹¤. ControlNet ì—°ê²°ì„ í†µí•´ ëª¨ë¸ì€ subjectì— ê´€í•œ ë‹¨ì„œ ë¿ ì•„ë‹ˆë¼ inputì˜ structure condition(e.g. edge map, depth map) ë˜í•œ ê³ ë ¤í•  ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ ë‹¤ì–‘í•œ ê¸°ì¡´ ëª¨ë¸ì„ í†µí•©í•˜ì—¬ ì¶”ê°€ì ì¸ í•™ìŠµ ì—†ì´ ì´ë¯¸ì§€ ìƒì„±ì´ ê°€ëŠ¥í•œ ê²ƒì´ BLIP-diffusionì˜ ì¥ì ì´ë¼ê³  í•œë‹¤.
<br/><br/>

> [**ControlNet**ğŸ“„](https://arxiv.org/abs/2302.05543)<br/>
> Stable Diffusionì—ì„œ ì¢€ ë” ì„¸ë°€í•œ ì œì–´ë¥¼ ìœ„í•´ ì œì•ˆëœ ëª¨ë¸. ì´ë¯¸ì§€ ìƒì„± ê³¼ì •ì—ì„œ ê³µê°„ì ì¸ context(e.g. edge maps, segmentation maps, depth ë“±)ë¥¼ conditionìœ¼ë¡œ ì£¼ì–´ ë” ì„¸ë¶€ì ì¸ ì œì–´ê°€ ê°€ëŠ¥í•˜ë‹¤.<br/>
> êµ¬ì¡°ëŠ” ëŒ€ëµì ìœ¼ë¡œ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë‘ ê°œì˜ ëª¨ë¸ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. í•˜ë‚˜ëŠ” ê¸°ì¡´ì— pre-trainëœ ìƒì„± ëª¨ë¸(stable diffusion)ì´ê³ , í•˜ë‚˜ëŠ” ì´ë¥¼ ì œì–´í•˜ê¸° ìœ„í•œ ì¡°ê±´ë¶€ ëª¨ë¸ì´ë‹¤. ì¡°ê±´ë¶€ ëª¨ë¸ì€ ì‚¬ìš©ìê°€ ì…ë ¥í•œ conditionì— ë”°ë¼ ìƒì„± ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ì¡°ì •í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤. ìì„¸í•œ ëª¨ë¸ì˜ êµ¬ì¡° ë° ì›ë¦¬ëŠ” ë…¼ë¬¸ì„ ì°¸ì¡°í•´ë³´ì.
>
> ![BLIP-Diffusion_7.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/04446a79-65f5-4eb8-b047-d28e54d6ede9){: width="450px"}
> 

<br/><br/><br/>

#### Subject-driven Editing with Attention Control.

![BLIP-Diffusion_8.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/58d3fcc7-0013-4c0e-9418-bc77af9d4b98){: width="650px"}
_ê·¸ë¦¼ì—ì„œ Text EncoderëŠ” Diffusion ëª¨ë¸ì˜ CLIP, U-Netì€ Diffusion ëª¨ë¸(forward/backward process)._

BLIP-Diffusionì—ì„œëŠ” multimodal controlled generationì„ ìœ„í•´ subject prompt embeddingê³¼ text prompt embeddingì„ ê²°í•©í•œë‹¤. ì—¬ê¸°ì„œ prompt tokenì˜ cross-attention mapì„ ì¡°ì‘í•˜ì—¬ **subject-driven image editing**ì„ ìˆ˜í–‰í•˜ê²Œ í–ˆë‹¤. **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>prompt-to-prompt</span></mark>** ë…¼ë¬¸ì—ì„œ ì˜ê°ì„ ë°›ì€ ë°©ì‹ìœ¼ë¡œ, **cross-attention control technique**ì„ ì‚¬ìš©í–ˆë‹¤.
<br/><br/>

ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ subject-specific ì´ë¯¸ì§€ë¡œ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ í¸ì§‘í•œë‹¤. í¸ì§‘ ê³¼ì •ì€ ì•„ë˜ì˜ ë‹¨ê³„ë¥¼ ê±°ì³ ì§„í–‰ëœë‹¤.

- ë¨¼ì € í¸ì§‘í•  text token(e.g. dog)ì„ ì§€ì •í•œë‹¤.
- ê·¸ ë‹¤ìŒ ì§€ì •ëœ tokenì˜ cross-attention mapì„ ì‚¬ìš©í•˜ì—¬ í¸ì§‘í•  ì˜ì—­ì— ëŒ€í•œ maskë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•œë‹¤.
- í¸ì§‘ë˜ì§€ ì•ŠëŠ” ì˜ì—­ì€ ë³´ì¡´í•˜ê¸° ìœ„í•´, subject embeddingì— ëŒ€í•œ attention mapì„ ìƒì„±í•˜ëŠ” ë™ì•ˆ ì›ë³¸ ìƒì„± attention mapì€ ìœ ì§€í•œë‹¤.
- ì¶”ì¶œëœ editing maskë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° stepì—ì„œ denoising latentë¥¼ í˜¼í•©í•œë‹¤. ì¦‰, í¸ì§‘ë˜ì§€ ì•Šì€ ì˜ì—­ì˜ latentëŠ” ì›ë³¸ ìƒì„±ì—ì„œ ë‚˜ì˜¨ ê²ƒì´ê³ , í¸ì§‘ëœ ì˜ì—­ì˜ latentëŠ” subject-driven ìƒì„±ì—ì„œ ë‚˜ì˜¨ ê²ƒì´ë‹¤.
<br/><br/>

> [**Prompt-to-prompt**ğŸ“„](https://arxiv.org/abs/2208.01626)
> 

<br/><br/><br/><br/><br/>

# Experiments

---

## 1. Pre-training Datasets and Details

- Multimodal representation learning
    - BLIP-2 êµ¬ì¡°ë¥¼ ì‚¬ìš©.
    - 129M image-text pairì— ëŒ€í•´ pre-train í•¨. ì—¬ê¸°ì—ëŠ” CapFilt captionì´ ìˆëŠ”  LAION, COCO, Visual Genome, Conceptual Captionsì˜ 115M image-text pairê°€ í¬í•¨ë¨.
    - CLIPì—ì„œì˜ image encoder ì‚¬ìš©, BERTbaseë¡œ Q-Former ì´ˆê¸°í™”.
- Subject representation learning
    - OpenImage-V6 ë°ì´í„°ì…‹ì—ì„œ íŠ¹ì • subjectë¥¼ í¬í•¨í•˜ëŠ” 292Kì˜ subsetì„ ì‚¬ìš©(ì¸ê°„ê³¼ ê´€ë ¨ìˆëŠ” subject ì œì™¸).
    - BLIP-2 OPTë¥¼ ì‚¬ìš©í•˜ì—¬ captionì„ text promptë¡œ ìƒì„±
    - webì—ì„œ 59Kì˜ background ì´ë¯¸ì§€ë¥¼ ì–»ì–´ subjectì™€ í•©ì„±í•¨
    - Stable Diffusion v1-5ë¥¼ ê¸°ë³¸ diffusion ëª¨ë¸ë¡œ ì‚¬ìš©

<br/><br/><br/>

## 2. Experimental Results

#### Main Qualitative Results.

ì•„ë˜ ê·¸ë¦¼ì—ì„œ BLIP-Diffusionì˜ ì •ì„±ì  ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.

- row #1: pre-trainëœ subject representationì„ ì´ìš©í•œ zero-shot subject-driven generation
- row #3-6: íš¨ìœ¨ì ì¸ fine-tuningì´ ê°€ëŠ¥í•˜ë¯€ë¡œ ë‹¤ì–‘í•œ taskì— ëŒ€í•´ì„œë„ ë†’ì€ ì¶©ì‹¤ë„ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±
- row #7-8: ControlNetê³¼ ê²°í•©í•˜ì—¬ êµ¬ì¡°ì™€ subjectë¥¼ ë™ì‹œì— ì œì–´
- row #9-10: subject ì •ë³´ë¥¼ ì´ë¯¸ì§€ í¸ì§‘ íŒŒì´í”„ë¼ì¸ì— ë„ì…í•˜ì—¬ íŠ¹ì • subject ì´ë¯¸ì§€ë¡œ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ í¸ì§‘í•  ìˆ˜ ìˆìŒ<br/>
![BLIP-Diffusion_9.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/5103f512-124e-4149-aa22-a5765f30e231){: width="700px"}
<br/><br/><br/><br/>

#### Comparisons on DreamBooth Dataset.

BLIP-Diffusionê³¼ ë‹¤ì–‘í•œ SOTA ìƒì„±ëª¨ë¸ì„ DreamBooth ë°ì´í„°ì…‹ì— ëŒ€í•´ ë¹„êµí–ˆë‹¤. ë°ì´í„°ì…‹ì—ëŠ” 30ê°œì˜ subjectì— ëŒ€í•´ ê°ê° 4~7ê°œì˜ ì´ë¯¸ì§€ê°€ í¬í•¨ëœë‹¤. ê²°ê³¼ëŠ” ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ë‹¤.

![BLIP-Diffusion_10.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/992cbddc-f124-4664-b84b-821ad99c9c29){: width="900px"}
<br/><br/><br/>

ë‹¤ìŒìœ¼ë¡œëŠ” ì´ì— ëŒ€í•œ ì •ëŸ‰ì ì¸ ê²°ê³¼ì´ë‹¤. DINO ë° CLIP-I ì ìˆ˜ëŠ” subject alignmentì„ ì¸¡ì •í•˜ê³  CLIP-TëŠ” image-text alignmentì„ ì¸¡ì •í•˜ëŠ” ì§€í‘œì´ë‹¤. ì €ìëŠ”  ê° text promptì— ëŒ€í•´ 4ê°œì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì—¬ ëª¨ë“  subjectì— ëŒ€í•´ ì´ 3,000ê°œì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í–ˆë‹¤. ì „ë°˜ì ì¸ ê²°ê³¼ëŠ” ì •ì„±ì  ê²°ê³¼ì™€ ìœ ì‚¬í•˜ê²Œ BLIP-Diffusionì´ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. 

![BLIP-Diffusion_11.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/77658e20-07a8-4be6-8462-72b71365facd){: width="900px"}
<br/><br/><br/><br/>

#### Ablation Studies.

![BLIP-Diffusion_12.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/131cd930-a665-4e19-a0ac-43fa906edb9d){: width="900px"}

ë‹¤ìŒìœ¼ë¡œëŠ” 250K subject representation learning stepì„ ì‚¬ìš©í•˜ì—¬ ablation studyë¥¼ ìˆ˜í–‰í–ˆë‹¤. ìœ„ í‘œëŠ” zero-shot evaluation ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ê²°ê³¼ë¥¼ í†µí•´ ì €ìëŠ” ì•„ë˜ì™€ ê°™ì€ ê²°ë¡ ì„ ì–»ì—ˆë‹¤ê³  í•œë‹¤.

1. subject embeddingê³¼ text prompt embedding ê°„ì˜ representation gapì„ í•´ì†Œí•˜ê¸° ìœ„í•´ multimodal representation learningì´ ì¤‘ìš”í•˜ë‹¤.
2. Diffusion ëª¨ë¸ì˜ text encoderë¥¼ freezingë©´ subject embeddingê³¼ text prompt embedding ê°„ì˜ ìƒí˜¸ ì‘ìš©ì´ ì•…í™”ë˜ì–´ text promptê°€ ë¬´ì‹œë˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•œë‹¤.
3. subject textë¥¼ multimodal encoderì— ì œê³µí•˜ë©´ í´ë˜ìŠ¤ë³„ ì‹œê°ì  ì‚¬ì „ ì •ë³´ë¥¼ ì£¼ì…í•˜ëŠ” ë° ë„ì›€ì´ ë˜ì–´ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤.
4.  random subject embedding ì‚­ì œë¥¼ í†µí•œ pre-trainingì€ diffusion ëª¨ë¸ì˜ generation abilityì„ ë” ì˜ ë³´ì¡´í•˜ëŠ” ë° ë„ì›€ì´ ë˜ì–´ ê²°ê³¼ì— ë„ì›€ì„ ì¤€ë‹¤.
<br/><br/><br/><br/>

#### Subject Representation Visualization.

![BLIP-Diffusion_13.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/88bed676-4ed9-47f4-82e3-f5ed61f2ea8c){: width="1100px"}

ì´ë¯¸ì§€ ë‚´ì˜ í”½ì…€ì´ í•´ë‹¹ë˜ëŠ” emdeddingì— ë°˜ì‘í•œë‹¤ëŠ” ê´€ì°° ê²°ê³¼ì— ë”°ë¼ cross-attention mapì„ ê´€ì°°í•˜ì—¬ í•™ìŠµëœ subject embeddingì„ ì‹œê°í™”í•˜ì˜€ë‹¤. ìœ„ì˜ ê²°ê³¼ë¥¼ ë³´ë©´ í•™ìŠµëœ embeddingë“¤ì´ ì„¸ë°€í•˜ë©´ì„œë„ ê°ê° ë‹¤ë¥¸ ì¸¡ë©´ì„ ì¸ì½”ë”©í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤(e.g. ì¼ë¶€ embeddingì€ ë³´ë‹¤ ì§€ì—­ì ì¸ featureì—, ë‹¤ë¥¸ embedddingì€ ì „ì²´ì ì¸ featureë¥¼ ì¸ì½”ë”©). ê²°ë¡ ì ìœ¼ë¡œ ì—¬ëŸ¬ê°œì˜ subject embeddingì„ ì‚¬ìš©í•˜ëŠ” ìƒí˜¸ ë³´ì™„ì ì¸ íš¨ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.
<br/><br/><br/><br/>

#### Zero-shot Subject-driven Image Manipulation.

BLIP-Diffusionì€ ì´ë¯¸ì§€ ìƒì„±ì„ ê°€ì´ë“œí•˜ëŠ” subject featureë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆë‹¤. Subject-driven generations / editing ì™¸ì—ë„ pre-trainëœ subject representationì„ ì´ìš©í•˜ì—¬ subject-driven style transfer ë˜ëŠ” subject interpolation ë“±ì˜ taskë¥¼ ìˆ˜í–‰í•  ìˆ˜ìˆë‹¤.
<br/><br/>

- Subject-driven Style Transfer<br/>
![BLIP-Diffusion_14.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/d5a924cb-08ec-490a-98e4-97a505ac2a9d){: width="900px"}
<br/><br/>

- Subject Interpolation<br/>
![BLIP-Diffusion_15.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/4e5c4de4-d313-43f1-b580-ee93fbb2a538){: width="700px"}

<br/><br/><br/><br/>
