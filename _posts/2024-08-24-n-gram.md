---
title: "NLP 기초: N-gram 개념 정리 (feat. Statistical Language Model, SLM)"
author: lunalee
date: 2024-08-24 14:11:08 +0900
categories: [AI, Concept Note]
tags: [NLP, Basic]
pin: false
math: true
---

이번 게시물에서는 Language Model의 기본적인 N-gram 모델에 대한 개념을 짚어보자. N-gram의 정의부터, N-gram 모델이 속해있는 SLM(Statistical Language Model)의 개념까지 핵심위주로 간단하게 요약정리해보자.
<br/><br/><br/><br/><br/>

# N-gram

---

N-gram은 인접한 **N개의 연속적인 단어의 나열**을 의미한다. N이 1인 경우 uni-gram, 2인 경우 bi-gram과 같이 나타낸다. 아래 문장을 N-gram으로 분리해보면 다음과 같이 나타낼 수 있다.
<br/><br/>

$$
\text{The moon is very bright today.}
$$


|  N-gram Model  | Results |
|:--------------:| --- |
| Uni-gram (N=1) | The, moon, is, very, bright, today |
| Bi-gram (N=2)  | The moon, moon is, is very, very bright, bright today |
| Tri-gram (N=3) | The moon is, moon is very, is very bright, very bright today |
|  4-gram (N=4)  | The moon is very, moon is very bright, is very bright today |


<br/><br/><br/>

## Statistical Language Model (SLM)

N-gram 언어 모델은 SLM, 즉 **통계 기반 언어 모델(**Statistical Language Mosel)의 일종이다. 통계 기반 언어 모델은 한마디로 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>문장 즉 단어의 시퀀스를 예측하는 모델</span></mark>**이다. 단어의 확률 분포를 기반으로 전체 문장의 확률을 할당한다.
<br/><br/>

통계적 언어 모델에 의하면, $\text{The moon is very}$ 라는 단어 시퀀스가 주어졌을 때 뒤에 $\text{bright}$가 나올 확률은 조건부 확률로 표현될 수 있고, 이전 단어에 대한 다음 단어의 조건부 확률은 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>카운트에 기반하여 계산</span></mark>**된다.
<br/><br/>

$$
P(\text{bright}∣\text{The moon is very}) = \frac{\text{count(The moon is very bright)}} {\text{count(The moon is very)}}
$$

<br/>
여기서 Count란 전체 학습한 데이터 혹은 문서 데이터(=corpus)에서 해당 시퀀스가 등장한 횟수를 의미한다.
<br/><br/><br/><br/>

그렇다면 특정 문장 전체에 대한 확률 $P$는 다음과 같이 모든 단어에 대한 조건부 확률의 곱으로 표현된다.

$$
P(\text{The moon is very btight today}) = P(\text{The}) \times P(\text{moon}∣\text{The})
$$

$$
\times P(\text{is}∣\text{The moon}) \times P(\text{very}∣\text{The moon is}) \times P(\text{bright}∣\text{The moon is very})
$$

$$
\times P(\text{today}∣\text{The moon is very bright}) 
$$

<br/><br/>
즉, 수식으로 일반화하자면 다음과 같다.

$$
P(w_1, w_2, w_3, ..., w_n) = \prod^n_{n=1} P(w_n ∣ w_1, ..., w_{n-1})
$$

<br/><br/>
하지만 해당 방법에서 문제점은, $P(\text{today}∣\text{The moon is very bright})$를 구하기 위해서 학습 corpus에 $\text{The moon is very bright}$ 라는 시퀀스가 없거나, 충분히 많지 않다면 $P(\text{today}∣\text{The moon is very bright})$의 확률 분포를 근사하기 쉽지 않다는 것이다. 학습 데이터가 충분히 방대하지 않다면 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>희소 문제(sparse problem)</span></mark>**가 발생하게 된다.
<br/><br/><br/><br/>

## N-gram Language Model

N-gram 언어 모델에서는 단어 시퀀스가 corpus 내에 존재하지 않는 경우를 고려해 단어를 줄이는 방법이다. 즉, **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>특정 단어가 나올 확률을 구하기 위해 앞의 $(N-1)$개의 단어를 기반으로 예측을 수행</span></mark>**하는 것이다. 

만약, 위의 예제에서 N=3인 경우라면,  $\text{bright}$를 예측하기 위한 조건부 확률은 다음과 같다.
<br/><br/>

$$
P(\text{bright}∣ \text{is very}) = \frac{\text{count(is very bright)}} {\text{count(is very)}}
$$

<br/>
N-gram 방법을 통해 SLM에 비교하여 어느정도 희소 문제를 완화할 수 있었지만, N-gram 방법도 여전히 corpus에 해당 n-gram이 충분히 존재해야 확률 근사가 가능하므로 희소 문제(sparse problem)가 존재한다.
<br/><br/><br/><br/><br/><br/>

---

#### Reference

[1] 딥러닝을 이용한 자연어 처리 입문: [https://wikidocs.net/21692](https://wikidocs.net/21692)
