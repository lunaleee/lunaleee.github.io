---
title: "[ë…¼ë¬¸ ë¦¬ë·°] simCLR, A Simple Framework for Contrastive Learning of Visual Representations"
author: lunalee
date: 2024-07-03 18:12:02 +0900
categories: [AI, Paper Review]
tags: [Contrastive Learning, Self-supervised]
pin: false
math: true
---

<br/><br/>
`Geoffrey Hinton` `PMLR 2020`

- Paper: [https://arxiv.org/abs/2002.05709](https://arxiv.org/abs/2002.05709)
- Git: [https://github.com/google-research/simclr](https://github.com/google-research/simclr)
<br/><br/><br/>

#### ğŸ“– í•µì‹¬ í›‘ì–´ë³´ê¸° !!
- Visual representationì„ í•™ìŠµí•˜ê¸° ìœ„í•´ latent spaceì—ì„œ ë¹„ìŠ·í•œ ì´ë¯¸ì§€ëŠ” ê°€ê¹Œì›Œì§€ë„ë¡, ë‹¤ë¥¸ ì´ë¯¸ì§€ëŠ” ì„œë¡œ ë©€ì–´ì§€ë„ë¡ í•™ìŠµí•˜ëŠ” **Contrastive Learning ë°©ë²•**ì„ ì‚¬ìš©
- ë°ì´í„° sampleì— **augmentationì„ ì ìš©**í•˜ì—¬ ê°™ì€ ì´ë¯¸ì§€ì— ìƒì„±ëœ ì˜ˆì œëŠ” positive pairë¡œ, batch ë‚´ì˜ ë‚˜ë¨¸ì§€ ì˜ˆì œë¥¼ negativeë¡œ ì·¨ê¸‰í•˜ì—¬ í•™ìŠµí•¨
- ë‹¤ì–‘í•œ Augmentation, model size, projection head, batch sizeë“± ì‹¤í—˜ì„ í†µí•´ ë‹¤ì–‘í•œ parameterì˜ ì˜í–¥ì— ëŒ€í•´ ì¦ëª…í•¨


<br/><br/><br/><br/>

# Introduction

---

Human supervision ì—†ì´ íš¨ê³¼ì ì¸ visual representationì„ í•™ìŠµí•˜ëŠ” ê²ƒì€ ì˜¤ë˜ ë‹¤ë¤„ì ¸ì˜¨ ë¬¸ì œì´ë‹¤. ì¼ë°˜ì ì¸ SSL(self-supervised learning) ë°©ë²•ìœ¼ë¡œ self-supervised learningì—ì„œ ì‚¬ìš©í•˜ëŠ” object functionì„ ì‚¬ìš©í•˜ë˜, unlabeled dataì—ì„œ íŒŒìƒëœ pretext taskë¥¼ ìˆ˜í–‰í•˜ë„ë¡ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì„ ìˆ˜í–‰í•´ì™”ë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ë°©ë²•ì€ pretext taskë¥¼ ì„¤ê³„í•˜ê¸° ìœ„í•´ heuristicì— ì˜ì¡´í•  ë¿ ì•„ë‹ˆë¼ í•™ìŠµëœ representationì˜ generalityë¥¼ ì œí•œí•  ìˆ˜ ìˆë‹¤.
<br/><br/>

![simCLR_1.png](https://github.com/user-attachments/assets/8cc335ee-bb9e-40e5-8bba-c83f260ee741){: width="500px"}

ì €ìëŠ” contrastive learningì„ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì— ì˜ê°ì„ ì–»ì–´, **SimCLR**ì´ë¼ê³  ë¶€ë¥´ëŠ” **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>visual representationì˜ contrastive learning</span></mark>**ì„ ìœ„í•œ frameworkë¥¼ ì œì•ˆí•œë‹¤. SimCLRì€ ì´ì „ ì‘ì—…ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì¼ ë¿ ì•„ë‹ˆë¼ íŠ¹ìˆ˜í•œ êµ¬ì¡°ë‚˜ memory bankë¥¼ í•„ìš”ë¡œ í•˜ì§€ ì•ŠëŠ”ë‹¤. ì£¼ìš” ë°œê²¬ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
<br/><br/>

- ì—¬ëŸ¬ **data augmentation**ì˜ êµ¬ì„±ì€ íš¨ê³¼ì ì¸  contrastive prediction taskë¥¼ ì •ì˜í•˜ëŠ”ë° ì¤‘ìš”í•˜ë‹¤.
- Representationê³¼ contrastive loss ì‚¬ì´ì— í•™ìŠµ ê°€ëŠ¥í•œ **nonlinear transformation**ì„ ì¶”ê°€í•˜ë©´ í•™ìŠµëœ representationì˜ í’ˆì§ˆì´ í¬ê²Œ í–¥ìƒëœë‹¤.
- Contrastive cross entropy lossì„ ì‚¬ìš©í•œ í•™ìŠµì€ **normalized embeddingê³¼ ì¡°ì •ëœ temperature parameter**ë¡œ ë¶€í„° ì´ì ì„ ì–»ëŠ”ë‹¤.
- Supervised learningì— ë¹„í•´ í° **batch sizeì™€ ê¸´ í•™ìŠµ, ë” ê¹Šê³  ë„“ì€ ë„¤íŠ¸ì›Œí¬**ì˜ ì´ì ì„ ì–»ëŠ”ë‹¤.
<br/><br/><br/><br/><br/><br/>

# Method

---

## 1. The Contrastive Learning Framework

![simCLR_2.png](https://github.com/user-attachments/assets/4d6d5a6f-bb62-4aa2-9e19-dc1679322b1d){: width="400px"}

Contrastive Learningì€ ë§ ê·¸ëŒ€ë¡œ â€œëŒ€ì¡° í•™ìŠµâ€ìœ¼ë¡œ, representationì„ í•™ìŠµí•  ë•Œ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>latent spaceì—ì„œ ë¹„ìŠ·í•œ ì´ë¯¸ì§€ëŠ” ê°€ê¹Œì›Œì§€ë„ë¡, ë‹¤ë¥¸ ì´ë¯¸ì§€ëŠ” ì„œë¡œ ë©€ì–´ì§€ë„ë¡ í•™ìŠµí•˜ëŠ” ë°©ë²•</span></mark>**ì„ ë§í•œë‹¤. SimCLRë„ ë§ˆì°¬ê°€ì§€ë¡œ latent spaceì—ì„œ **contrastive lossë¥¼ í™œìš©í•˜ì—¬, ë™ì¼í•œ ì´ë¯¸ì§€ì—ì„œ ë‹¤ë¥¸ Augmentationì„ ì ìš©í•˜ì—¬ ê°™ì€ ì´ë¯¸ì§€ì—ì„œ ë§Œë“¤ì–´ì§„ ê²ƒì„ positive pairë¡œ ìµœëŒ€í•œ ê°€ê¹Œì›Œì§€ë„ë¡ í•™ìŠµ**í•œë‹¤. ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ simCLRì€ í¬ê²Œ ë„¤ê°€ì§€ êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤. 
<br/><br/>

- **stochastic data augmentation module**: data sampleì„ randomí•˜ê²Œ augmentationí•˜ì—¬, ë™ì¼í•œ ì´ë¯¸ì§€ì—ì„œ ìƒì„±ëœ augmented ì´ë¯¸ì§€ë¥¼ positive pairë¡œ ê°„ì£¼í•¨.
- **base encoder** $f(\cdot)$: augmented ëœ dataì—ì„œ representation vectorë¥¼ ì¶”ì¶œ.
- **projection head** $g(\cdot)$: representationì„ constrastive lossê°€ ì ìš©ë˜ëŠ” ê³µê°„ìœ¼ë¡œ ë§¤í•‘.
- **contrastive loss function**:  contrastive prediction taskì— ì˜í•´ ì •ì˜ë¨.
<br/><br/><br/>

![simCLR_3.png](https://github.com/user-attachments/assets/db500081-7699-49bd-a22c-a543a3639920){: width="600px"}
<br/><br/>

![simCLR_4.png](https://github.com/user-attachments/assets/cdd0f550-9a7b-4773-98d0-2e0500d259a4){: width="600px"}

ì´ ë•Œ random í•˜ê²Œ Nê°œì˜ ë°ì´í„° ì˜ˆì œë¡œ êµ¬ì„±ëœ minibatchë¥¼ ìƒ˜í”Œë§í•˜ê³ , augmentationì„ ì ìš©í•˜ì—¬ 2Nê°œì˜ ë°ì´í„° pointë¥¼ ìƒì„±í•œë‹¤. ê°™ì€ ì´ë¯¸ì§€ì— ìƒì„±ëœ ì˜ˆì œëŠ” positive pairê°€ ë˜ê³ , batch ë‚´ì˜ ë‚˜ë¨¸ì§€ 2(N-1)ê°œì˜ ì˜ˆì œë¥¼ negativeë¡œ ì·¨ê¸‰í•˜ì—¬ í•™ìŠµí•œë‹¤. Loss functionì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤. 
$\text{sim}(u, v) = u^\top v/âˆ¥uâˆ¥âˆ¥vâˆ¥$ëŠ” cosine similarityë¥¼ ë‚˜íƒ€ë‚´ê³ , $\tau$ëŠ” temperature parameterë¥¼ ì˜ë¯¸í•œë‹¤.

$$
\ell_{i,j} = -\log \frac{\exp(\text{sim} (z_i, z_j)/\tau)}{\sum^{2N}_{k=1} \mathbb 1_{[k \not = i]}\exp (\text{sim}(z_i, z_k)/\tau)}
$$

<br/><br/><br/>
ì•Œê³ ë¦¬ì¦˜ì€ ì•„ë˜ì™€ ê°™ì´ ìš”ì•½ë  ìˆ˜ ìˆë‹¤. 

![simCLR_5.png](https://github.com/user-attachments/assets/2e2ff509-c39c-4ddb-8ff1-709c044faed1){: width="500px"}
<br/><br/><br/><br/><br/>

## 2. Data Augmentation for Contrastive Representation Learning

Data Augmentationì€ supervised, unsupervised representation learningì—ì„œ ë§ì´ ì‚¬ìš©ë˜ì—ˆì§€ë§Œ, contrastive prediction taskë¥¼ ìœ„í•œ ì²´ê³„ì ì¸ ë°©ë²•ìœ¼ë¡œ ì‚¬ìš©ë˜ì§€ëŠ” ì•Šì•˜ë‹¤. ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹ë“¤ì€ contrastive prediction taskë¥¼ ìœ„í•´ architectureë¥¼ ë³€ê²½í•˜ì—¬, contrastive prediction taskë¥¼ ì •ì˜í–ˆë‹¤. 

ì˜ˆë¥¼ë“¤ë©´ ë„¤íŠ¸ì›Œí¬ architectureì—ì„œ receptive fieldë¥¼ ì œí•œí•˜ì—¬ global-to-local view predictionì„ ìˆ˜í–‰í•œë‹¤ë˜ì§€, context aggregation ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„í•  ì ˆì°¨ë¥¼ ìˆ˜ì •í•˜ì—¬ neighboring view predictionì„ ìˆ˜í–‰í–ˆë‹¤. 
<br/><br/>

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ê°„ë‹¨í•œ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>random cropping(with resizing)</span></mark>**ë§Œì„ ìˆ˜í–‰í•˜ì—¬ ì´ëŸ¬í•œ ë³µì¡í•œ ê³¼ì •ì„ ëŒ€ì²´í•  ìˆ˜ ìˆìŒì„ ì¦ëª…í–ˆë‹¤. 

![simCLR_6.png](https://github.com/user-attachments/assets/3423dc1e-cd9e-4b95-b25f-9a06c0b4c2df){: width="500px"}
<br/><br/><br/><br/>

#### Composition of data augmentation operations is crucial for learning good representations.

![simCLR_7.png](https://github.com/user-attachments/assets/7b90ebd4-0100-4ec9-8105-72a908406cdc){: width="900px"}

Data Augmentationì˜ ì˜í–¥ì„ ì²´ê³„ì ìœ¼ë¡œ ì—°êµ¬í•˜ê¸° ìœ„í•´ ëª‡ê°€ì§€ augmentationì„ ì ìš©í–ˆë‹¤.  

- Cropping, resizing, rotation, cutout ê°™ì€ spatial/geometric transformation
- Color distortion, Gaussian blur, Sobel filteringê³¼ ê°™ì€ appearance transformation
<br/><br/><br/><br/>

![simCLR_8.png](https://github.com/user-attachments/assets/eacbe5ab-3b5e-4097-9cae-e5f21f61fdc7){: width="600px"}

ê°œë³„ augmentationì„ ì‚¬ìš©í•˜ëŠ” íš¨ê³¼ì™€ augmentationì˜ êµ¬ì„± ì¦‰, pairë¡œ ì‚¬ìš©í•  ë•Œì˜ ì„±ëŠ¥ì„ ì¡°ì‚¬í–ˆë‹¤. ìœ„ ê·¸ë¦¼ì—ì„œ ë§ˆì§€ë§‰ ì—´ â€˜Avarageâ€™ë¥¼ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” x, yì— í•´ë‹¹í•˜ëŠ” augmentationì„ pairë¡œ ì ìš©í•œ ê²°ê³¼ì´ë‹¤(ëŒ€ê°í–‰ë ¬ì€ ê°œë³„ augmentation ê²°ê³¼ì´ë‹¤). 

ìœ„ ê²°ê³¼ë¥¼ í†µí•´ **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>ë‹¨ì¼ augmentationì„ ì ìš©í•œ ê²½ìš°, ì„±ëŠ¥ì´ í™•ì—°íˆ ì €í•˜</span></mark>**ë˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë˜í•œ, **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>random cropping + random color distortionìœ¼ë¡œ êµ¬ì„±</span></mark>**í–ˆì„ ë•Œ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì´ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
<br/><br/><br/><br/><br/>

## 3. Architectures for Encoder and Head

#### Unsupervised contrastive learning benefits (more) from bigger models.

![simCLR_9.png](https://github.com/user-attachments/assets/47cf2bc1-ac79-48a6-b580-553a6ca6377c){: width="400px"}

ë‹¤ìŒìœ¼ë¡œëŠ” ëª¨ë¸ í¬ê¸°ì— ëŒ€í•´ ì„±ëŠ¥ í–¥ìƒì„ ì¡°ì‚¬í–ˆë‹¤. ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´, ëª¨ë¸ì˜ depthì™€ widthë¥¼ ëŠ˜ë¦¬ë©´ ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì´ ë•Œ, ëª¨ë¸ì˜ í¬ê¸°ê°€ ì»¤ì§ˆ ìˆ˜ë¡ supervised learningì—ì„œë³´ë‹¤ unsupervised learningì—ì„œì˜ ì„±ëŠ¥ ê²©ì°¨ê°€ ì¤„ì–´ë“œëŠ” ê²ƒì„ ë³´ë©´ **unsupervised learningì´ ëª¨ë¸ í¬ê¸° ì¸¡ë©´ì—ì„œ ë” ë§ì€ ì´ì ì„ ì–»ëŠ”ë‹¤**ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
<br/><br/><br/><br/><br/>

#### A nonlinear projection head improves the representation quality of the layer before it.

![simCLR_10.png](https://github.com/user-attachments/assets/18944a97-9e0d-4529-a765-c2ecc7acb918){: width="600px"}

ë‹¤ìŒìœ¼ë¡œëŠ” projection head g(h)ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•´ ì—°êµ¬í–ˆë‹¤. ìœ„ì˜ ê·¸ë¦¼ì€ headì— ì„¸ ê°€ì§€ ë‹¤ë¥¸ architectureë¥¼ ì‚¬ìš©í–ˆì„ ë•Œì˜ ì„±ëŠ¥ì— ëŒ€í•´ í‰ê°€í•œ ê·¸ë˜í”„ì´ë‹¤. ê°ê° Identity mapping, linear projection, ì¶”ê°€ì ì¸ hidden layerë¥¼ ì‚¬ìš©í•œ nonlinear projectionì´ë‹¤. **Nonlinear projection**ì´ linear projection ë³´ë‹¤ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì´ê³ , projectionì´ ì—†ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
<br/><br/><br/><br/>

![simCLR_11.png](https://github.com/user-attachments/assets/19e79a5e-f052-4555-a0c8-a1d2919fa09e){: width="500px"}

ë˜í•œ nonliear projectionì„ ì‚¬ìš©í•˜ë”ë¼ë„, hidden layerê°€ **projection head ì•ì— ìœ„ì¹˜í•˜ëŠ” ê²½ìš°**$(\mathcal h)$ê°€ **ë’¤ì— ìœ„ì¹˜í•˜ëŠ” ê²½ìš°**$(z=g (\mathcal h))$ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì•˜ë‹¤. ì €ìë“¤ì€ ì´ëŸ¬í•œ í˜„ìƒì„ nonlinear projection ì•ì˜ representationì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ contrastive lossë¡œ ì¸í•´ ì •ë³´ê°€ ì†ì‹¤ë˜ê¸° ë•Œë¬¸ì´ë¼ê³  ì¶”ì¸¡í–ˆë‹¤.
<br/><br/><br/><br/><br/>

## 4. Loss Functions and Batch Size

#### Normalized cross entropy loss with adjustable temperature works better than alternatives.

![simCLR_12.png](https://github.com/user-attachments/assets/eb7bb514-47d1-4fd1-81f2-c833606d1145){: width="500px"}

ë‹¤ìŒìœ¼ë¡œëŠ” loss functionì— ë”°ë¥¸ ê²°ê³¼ì´ë‹¤. ìœ„ í‘œì™€ ê°™ì´ NT-Xent lossê°€ ê°€ì¥ ë†’ì€ Top-1 scoreë¥¼ ë³´ì´ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. 

ë˜í•œ í‘œ 5ë¥¼ í†µí•´ NT-Xent lossì—ì„œ **$\ell_2$ normalization(cosine similarity vs dot product)ê³¼ temperature $\tau$ì˜ ì¤‘ìš”ì„±**ì— ëŒ€í•´ ì‹¤í—˜í•œ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. Noramlizationê³¼ ì ì ˆí•œ temperature ì„¤ì • ì—†ì´ëŠ” ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
<br/><br/><br/><br/>

#### Contrastive learning benefits (more) from larger batch sizes and longer training.

![simCLR_13.png](https://github.com/user-attachments/assets/ffa00b8e-e733-4200-8299-1dc3d33d653b){: width="500px"}

ìœ„ì˜ ê·¸ë˜í”„ëŠ” ëª¨ë¸ì˜ epochì— ë”°ë¥¸ batch sizeì˜ ì˜í–¥ì„ ë‚˜íƒ€ë‚¸ë‹¤. **Training epochê°€ ì ì„ ë•Œì—ëŠ” batch sizeì— ë”°ë¥¸ ì„±ëŠ¥ ì°¨ì´ê°€ í¬ê³ , epochê°€ ì§„í–‰ë  ìˆ˜ë¡ batch sizeë¡œ ì¸í•œ ê²©ì°¨ê°€ ì¤„ì–´ë“œëŠ” ê²ƒ**ì„ ë³¼ ìˆ˜ ìˆë‹¤. Contrastive learningì—ì„œ ë” í° batch sizeë¥¼ ì‚¬ìš©í•˜ë©´ ë” ë§ì€ negative exampleì„ ì œê³µí•˜ë¯€ë¡œ ëª¨ë¸ì´ ìˆ˜ë ´í•˜ê¸°ì— ìš©ì´í•˜ë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ ë” ë§ì€ training epochë¥¼ ì§„í–‰í•˜ë©´ ë” ë§ì€ negative exampleì„ ì œê³µí•˜ê³ , ê²°ê³¼ê°€ ê°œì„ ëœë‹¤.
<br/><br/><br/><br/>
