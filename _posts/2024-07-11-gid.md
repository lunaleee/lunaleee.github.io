---
title: "[논문 리뷰] Generative Image Dynamics"
author: lunalee
date: 2024-07-11 19:43:12 +0900
categories: [AI, Paper Review]
tags: [Video, Generative, Diffusion]
pin: false
math: true
---

<br/><br/>
`Google Research`  `CVPR 2024` `Best paper`

- Paper: [https://arxiv.org/abs/2309.07906](https://arxiv.org/abs/2309.07906)
- Page: [https://generative-dynamics.github.io](https://generative-dynamics.github.io/)
<br/><br/><br/>

#### 📖 핵심 훑어보기 !!
- 

<br/><br/><br/><br/>

# Introduction

---

Natural world의 장면들은 겉보기에 정적인 장면에도 바람, 물의 흐름, 호흡과 같은 자연적인 리듬으로 인해 미묘한 진동을 포함하고 있다. 사람은 이러한 미묘한 진동, 즉 움직임(motion)을 쉽게 상상할 수 있지만, motion을 만들어내도록 모델링하는 것은 어떨까?<br/>
이러한 motion은 물리적인 역학(dynamic)에 의해 적용된다. 물체의 질량, 탄성 등의 힘이 물체에 적용되는 결과이므로, 측정이 쉽지 않다. 그렇다면 어떻게 모델링적인 측면에서 접근하면 좋을까?
<br/><br/>

특정 application에서는 이러한 양을 정확히 측정하지 않아도, 물체에서 직접 **관찰된 2D motion을 분석**하는 방법으로 역학을 시뮬레이션 할 수 있다. 관찰된 motion이 일종의 감독 신호 역할을 하여, 학습을 진행하는 것이다. 비록 관찰된 motion은 복잡한 물리적 효과를 기반으로 하지만 종종 예측가능하기도 하기 때문이다. (e.g.촛불은 특정 방식으로 깜빡거리고, 나무는 흔들리고, 잎은 바스락거린다.) 
<br/><br/>

![GID_1.png](https://github.com/user-attachments/assets/50a458a3-84de-4daf-ad3e-7898ce85a99d){: width="900px"}

본 논문에서는 **image-space scene motion**, 즉 이미지의 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>모든 픽셀의 motion에 대한 generative prior를 모델링</span></mark>**한다. 이 모델은 거대한 real video sequence 모음에서 자동으로 추출된 motion trajectory에 대해 학습한다. 특히 각 training video에서 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>spectral volume이라는</span></mark>** 형태로 motion을 계산한다. Spectral volume은 dense한, long-range pixel trajectory를 의미하는 **frequency-domain representation**이다. 바람에 흔들리는 꽃과 나무와 같은 진동 역학(oscillatory dynamics)에 적합하다. 
<br/><br/>

또한 저자는 spectral volume이 **diffusion output**으로 생성하기 효과적이라는 사실을 발견했다. 따라서 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>하나의 이미지를 condition으로, 학습된 distribution에서 spectral volume을 샘플링할 수 있는 generative model을 학습</span></mark>**한다. Predicted spectral volume은 **motion texture**(long-range, per-pixel motion trajectory 집합)로 변환하여 이미지를 애니메이션화 하는데 사용할 수 있다. 즉, 예측된 motion은 미래 프레임을 합성하는 데 사용한다.
<br/><br/>

바로 RGB 이미지를 예측하는, RGB 픽셀에 대한 prior와 비교할 때, **motion에 대한 prior는 더 기본적이고 lower-dimensional structure를 캡처**하여 픽셀 값의 long-range variation을 효율적으로 설명한다고한다. 따라서 프레임(각 이미지) 사이의 중간 motion을 생성하면 보다 일관된 long-term generation과 애니메이션에 대한 보다 세밀한 제어가 가능하다.
<br/><br/><br/><br/><br/><br/><br/>

# Overview

---

논문의 목표는 single picture $I_0$가 주어지면 나무, 꽃 또는 바람에 흔들리는 촛불과 같은 진동 운동을 담고 있는 비디오 $\lbrace \hat I_1, \hat I_2, … , \hat I_T \rbrace$를 생성하는 것이다. 전체 시스템은 두 개의 모듈(**<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Motion prediction module</span></mark>**, **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Image-based rendering module</span></mark>**)로 구성되어 있고, 전체적인 pipeline은 다음과 같다.
<br/><br/>

1. Latent diffusion model (LDM) 을 사용하여 입력 I에 대한 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>spectral volume</span></mark>** $\mathcal S = (S_{F_0}, S_{F_1}, … , S_{F_{K-1}})$을 예측한다.
2. 예측된 spectral volume은 **inverse discrete Fourier transform**을 통해 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>motion texture</span></mark>** $\mathcal F = (F_1, F_2, …, F_T)$로 변환된다. 이 motion은 모든 future time step에서 각 입력 픽셀의 위치를 결정한다.
3. Motion texture가 주어지면, **image-based rendering technique**을 사용하여 입력 RGB 이미지에 **애니메이션을 적용**한다.
<br/><br/><br/><br/><br/><br/><br/>  

# Predicting Motion

---

## 1. Motion representation

먼저 Motion representation에 대해 정의해보자. Motion은 두 프레임 사이의 움직임을 나타내는 것으로, 이미지와 비디오의 주된 차이라고 볼 수 있다. 논문에서는 세부적으로 아래와 같이 정의했다. 
<br/><br/>

**[Motion이란?]**<br/>
![GID_2.png](https://github.com/user-attachments/assets/0b1cd29f-3f44-4863-9c73-43ff977d6e98){: width="1100px"}

- **Motion texture**: time-varying **2D displacement map**의 sequence. $\mathcal F = \lbrace F_t∣t = 1,...,T \rbrace$
- 여기서, 입력 이미지 $I_0$의 픽셀 좌표 $\mathbf p$에서의 **2D displacement vector** $F_t(\mathbf p)$는 미래 시간 $t$에서 해당 픽셀의 위치를 정의한다.
- 시간 t에서 미래 프레임을 생성하려면 해당 displacement map $D_t$를 사용하여 $I_0$의 픽셀을 이동시켜 forward-warp된 이미지 $I'_t$를 얻을 수 있다.

$$
I'_t(\mathbf p + F_t(\mathbf p)) = I_0(\mathbf p).
$$

<br/><br/><br/>

**[왜 spectral volume 을 사용했을까?]**

Motion texture를 통해 비디오를 생성하기 위해서 입력 이미지에서 곧바로 time-domain motion texture를 생성하는 방법도 고려해볼 수 있다. 하지만 motion texture는 비디오 길이만큼 생성되어야하기 때문에, $T$개의 출력 프레임을 생성하기 위해서는 $T$개의 displacement field를 예측해야한다. 긴 비디오에 대해, 이렇게 큰 출력 representation을 예측하는 것은 결국 생성된 비디오의 long-term temporal consistency를 잃어버리는 문제를 일으키게 된다.
<br/><br/><br/>

![GID_3.png](https://github.com/user-attachments/assets/fae2a151-58e1-4c96-a54e-ced340f84ad1){: width="700px"}

반면에, 많은 natural motion은 여러 개의 각각 다른 주파수(frequency), 진폭(amplitude), 위상(phase)을 가지는 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>진동(harmonic oscillation)의 중첩</span></mark>**으로 표현될 수 있다. 이러한 motion은 quasi-periodic, 즉 준주기적이므로 **frequency domain에서 모델링**하는 것이 유리하다.
<br/><br/>

따라서 저자는 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>spectral volume</span></mark>**이라는 비디오의 motion에 대한 효율적인 frequency space representation을 채택했다.

Spectral volume은 **비디오에서 추출된 per-pixel trajectory에 대해 temporal Fourier transform을 수행한 결과**라고 볼 수 있다.
<br/><br/>

> **[Visual Vibration Analysis📄](https://www.abedavis.com/thesis.pdf)**<br/>
> Spectral volume 변환 과정. Motion texture에서 frequency domain으로의 변환을 위해 Temporal Fourier Transform을 거쳐, specral volume을 얻는다.<br/>
> *** Fourier Transform → 입력 신호(이미지)를 다양한 주파수(frequency)를 갖는 주기함수들의 합으로 분해하여 표현하게 됨<br/>
> ![GID_4.png](https://github.com/user-attachments/assets/a9c07b3a-a931-4099-b9e3-3d9a26fdfe34){: width="700px"}

<br/><br/><br/><br/>

**[Spectral volume을 예측하기 위한 LDM 설계]**

Spectral volume을 motion representation으로 사용하게 되었으므로, 이제 motion 예측 문제를 **multi-modal image-to-image translation task**로 공식화한다(입력: 이미지 → 출력: motion spectral volume). 

- **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Latent diffusion model (LDM)</span></mark>**을 사용하여 4$K$-channel 2D motion spectrum map으로 구성된 spectral volume을 생성한다.
- 여기서 K << T는 모델링된 frequency의 수 이고, 각 frequency에서 4개의 scalar가 필요한데, 각각  x- , y-차원의 **복소수 Fourier coefficient**를 나타낸다.
- 미래 time step 에서 픽셀의 motion trajectory $\mathcal F(\mathbf p) = \lbrace F_t (\mathbf p)∣t = 1, 2, ...T \rbrace$ 는 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>Fast Fourier Transform (FFT)</span></mark>**에 의해 그에 해당하는 spectral volume $\mathcal S(\mathbf p) = \lbrace S_{f_k}(\mathbf p) ∣ k = 0, 1, .. \frac{T}{2} − 1\rbrace$ 로 변환된다.
    
    $$
    \mathcal S(\mathbf p) = \text{FFT}(\mathcal F (\mathbf p))
    $$
  
<br/><br/><br/>  

**[Frequency space representation을 위한 추가적인 문제]**<br/>
![GID_5.png](https://github.com/user-attachments/assets/7e2d572d-3822-48a7-aee8-0ae461e479b1){: width="500px"}

이제 **K 출력 frequency 수를 어떻게 선택**해야하는지에 대한 문제가 남아있다. 저자는 이전 연구들을 통해 real-time animation에서 대부분의 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>자연스러운 진동 motion이 주로 low-frequency 성분으로 구성</span></mark>**되어 있다는 것을 관찰했다. 이를 검증하기 위해 random으로 샘플링한 1000개의 5초짜리 비디오 클립에서 추출한 motion의 평균 power spectrum을 계산했을 때, 위 플롯과 같이 high-frequency에 대해 기하급수적으로 감소하였다.

이는 대부분의 자연스러운 진동(natural oscillation) motion이 실제로 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>low-frequency term</span></mark>**으로 잘 표현될 수 있다는 것을 의미하고, 실제로 $K = 16$ Fourier coefficient를 사용했을 때 다양한 실제 비디오에서 원래의 motion을 사실적으로 재현하기에 충분하다는 것을 발견했다.
<br/><br/><br/><br/><br/>

## 2. Predicting motion with a diffusion model

Motion 예측을 위해 표준 Latent diffusion model (LDM)을 backbone으로 사용했다. 표준 LDM은 두 가지 모듈로 구성된다.

1. Variational autoencoder (VAE): Encoder $z = E(I)$를 통해 입력 이미지를 latent space로 압축, Decoder $I = D(z)$를 통해 latent feature로 부터 입력 이미지를 재구성
2. U-Net based Diffusion model: Gaussian noise에서 시작하여 denoising process를 반복적으로 적용하는 학습 방법 사용
<br/><br/>

![GID_6.png](https://github.com/user-attachments/assets/ca0e7756-2f92-4dc8-8151-8346cf1c9e47){: width="1300px"}

논문에서는 이 process를 RGB 이미지가 아닌 spectral volume에 적용한다. Spectral volume은 마찬가지로 encoding되고, 2D U-Net을 통해 noise $\epsilon_\theta(z^n; n, c)$ 를 반복적으로 제거하도록 학습된다. 이 때 **condition** $c$는 training video sequence의 첫 번째 프레임인 $I_0$이다. 그 다음 denoising 단계를 거친 latent features $z^0$을 decoder에 넣어 spectral volume을 복구한다.
<br/><br/><br/><br/>

#### Frequency adaptive normalization.

![GID_7.png](https://github.com/user-attachments/assets/c7eb0b33-f02b-403a-807c-a62fd214e728){: width="600px"}

이렇게 모델을 설계할 때 한 가지 고려해야할 문제는, motion texture가 frequency에 따라 특정 분포 성질을 가진다는 것이다. <span style='color: var(--txt-gray)'>~~(frequency와 amplitude는 기본적으로는 독립적인 특성인데, motion texture에 대해 분석했을 때 특정 연관성? 경향성?을 가진다는 뜻인 것 같다.)~~</span>

위 그림의 왼쪽 플롯과 같이 spectral volume의 amplitude는 0~100의 범위에 분포하고 있다. 이 때 학습을 위해 Normalization을 적용할 때, 이미지와 마찬가지로 [0, 1]의 범위로 normalization 하게 되면 위 그림의 오른쪽 플롯과 같이 높은 frequency의 거의 모든 coefficient가 0에 가까워지는 문제가 발생하게 된다. 
<br/><br/><br/>

이러한 문제를 해결하기 위해, 논문에서는 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>frequency adaptive normalization 방법</span></mark>**을 적용했다. 방법은 다음과 같다.

1. Training set에서 계산된 통계를 바탕으로 각 frequency에서 Fourier coefficient를 독립적으로 normalize한다.
즉, 각 개별 주파수 $f_j$에 대해, 모든 입력 샘플에 대한 Fourier coefficient 크기의 95번째 백분위수를 계산하고, 해당 값을 per-frequency scaling factor $s_{f_j}$로 사용한다.
2. 그 다음 scaling된 Fourier coefficient에 power transformation을 적용하여 극한값에서 벗어나도록 한다.
<br/><br/><br/>

위 방법을 사용하면 로그나 제곱근과 같은 nonlinear transformation보다 좋은 성능을 보인다고 한다. 요약하면, frequency $f_j$에서 spectral volume $\mathcal S(\mathbf p)$의 최종 coefficient value(학습하는데 사용)는 아래와 같이 계산된다. 위 그림의 오른쪽 플롯에서 볼 수 있듯, 해당 방법을 적용한 후 spectral volume coefficient는 더 고르게 분포한다.

$$
S'_{f_j}(\mathbf p) = \text{sign} (S_{f_j}) \sqrt{\bigg \lvert \frac{S_{f_j}(\mathbf p)}{s_{f_j}}\bigg \rvert}.
$$

<br/><br/><br/><br/>

#### Frequency-coordinated denoising.

$K$ frequency band의 spectral volume $\mathcal S$를 예측하기 위해 하나의 diffusion U-Net으로 4$K$ channel의 tensor를 출력하도록 하면 간단하지만, 너무 많은 수의 channel을 생성하도록 모델을 학습하면 지나치게 매끄럽고 부정확한 결과물을 생성한다는 이전의 연구 결과들이 있었다. 그렇다고 독립적으로 frequency를 예측하게 되면 서로 상관 관계가 없는 결과물을 생성하여 비현실적인 motion으로 이어지는 문제가 있다고 한다.
<br/><br/>

![GID_6.png](https://github.com/user-attachments/assets/ca0e7756-2f92-4dc8-8151-8346cf1c9e47){: width="1300px"}

따라서 저자는 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>frequency-coordinated denoising strategy</span></mark>**를 제안한다. 방법은 다음과 같다. 

1. 입력 이미지 $I_0$가 주어지면, spectral volume $S_{f_j}$의 **하나의 4-channel frequency slice를 예측**하기 위한 LDM $\epsilon_\theta$ 학습을 진행한다. 이 때, time stamp와 함께 추가적인 **frequency embedding**을 LDM에 입력한다.
2. 그 다음 이 LDM $\epsilon_\theta$의 parameter를 freeze한다.
3. LDM $\epsilon_\theta$에 2D spatial layer와 attention layer를 도입하고($K$-frequency band에 걸쳐서), fine-tuning 한다.
    1. 즉 batch size $B$일 때, $\epsilon_\theta$의 2D spatial layer에서 채널 크기가 $C$인 $B \cdot K$개의 noisy latent feature에 대해 shape $\mathcal R^{(B \cdot K)×C×H×W}$을 가진 독립접인 샘플로 처리한다.
    2. 그 다음 attention layer에서 위의 결과를 frequency 축에 걸쳐 있는 연속된 feature로 해석한다. 이를 위해 attention layer에 넣기 전에 2D spatial layer의 latent feature를 $\mathcal R^{B×K×C×H×W}$로 reshape한다.
    (즉, frequency attention layer는 **모든 frequency slice를 조정하여 일관된 spectral volume을 생성하도록 fine-tune**된다.)
<br/><br/><br/><br/><br/><br/>
   
# Image-based rendering

---

이제 입력이미지 $I_0$에서 예측된 spectral volume $\mathcal S$를 가지고 시간 $t$에서 미래 프레임 $I_t$를 렌더링하는 방법에 대해 살펴보자. 크게는 아래와 같이 두 단계로 나눌 수 있다.

1. 각 픽셀 $\mathcal F(\mathbf p) = \text {FFT}^{−1}(\mathcal S(\mathbf p))$에 inverse temporal FFT을 적용하여 time domain motion texture를 얻는다.
2. 미래 프레임 $\hat I_t$를 생성하기 위해 deep timage-based rendering을 사용한다. 예측된 motion field $F_t$를 사용(splatting)하여 $I_0$에 대해 forward-warp를 수행한다. 이 때, **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>feature pyramid softmax splatting 방법</span></mark>**을 적용한다.
<br/><br/>

> Forward-warpping 과정에서 입력 $I_0$의 여러 source 픽셀이 warping될 때, 하나의 output target 픽셀 위치에 매핑될 수도 있다(입출력에 대해 1:1 매핑이 아님). 따라서 output 이미지에 매핑되지 않은 픽셀에 대해 홀(hole)이 생겨 정보가 손실되거나 artifact가 생성되는 문제가 있다. 이러한 문제를 막기 위해 feature pyramid softmax splatting 방법을 사용했다. <br/>
> Softmax 함수는 여러 입력값 중에서 가장 큰 값을 부각시키면서도 다른 값들도 일정 정도 반영되게 하는 일종의 정규화 함수이다. Softmax Splatting에서는 각 source pixel이 target pixel(중간 프레임의 픽셀)에 기여하는 정도를 softmax 함수를 통해 계산하여, 겹치는 값을 합칠 때 각 픽셀의 기여도를 고려하는 방법으로 매핑 위치가 겹치지 않도록 한다.<br/>
> (참고: [https://kycu-sb.tistory.com/241](https://kycu-sb.tistory.com/241))<br/>
> ![GID_8.png](https://github.com/user-attachments/assets/541c4d54-c6b1-4d4d-a2d0-87284b4d451b){: width="450px"}

<br/><br/>
그럼 구체적인 방법을 하나씩 살펴보자.<br/>
![GID_9.png](https://github.com/user-attachments/assets/acfc4e25-9ce1-49fe-bbe5-8d8d14df5aeb){: width="600px"}

1. 먼저, feature extractor를 사용하여 I_0를 인코딩, multi-scale feature map을 생성한다.
2. 예측한 2D motion field $F_t$를 1번에서 생성한 feature map의 각각의 scale $j$에 맞게 resize한다.
3. 예측된 flow magnitude를 depth의 proxy로 사용하여 목적지 위치에 매핑된 각 source 픽셀의 기여 weight를 결정한다. 특히, 예측된 motion texture의 평균 magnitude로 per-pixel weight $W(\mathbf p) = \frac{1}{T} \sum_t ∣∣F_t(\mathbf p)∣∣_2$ 을 계산한다. 즉, 큰 motion이 움직이는 foreground 객체에 해당하고 작거나 0인 motion이 background에 해당한다고 가정한다.<br/>
    <span style='color: var(--txt-gray)'>→ ~~이 때 학습 가능한 weight 대신 motion-derived weight을 사용한다. Single-view의 경우 학습 가능한 weight은 disocclusion ambiguities, 즉 분리 모호성을 해결하는데 효과적이지 않기 때문이라고 한다.~~</span>
4. Motion field $F_t$와 weight $W$가 주어지면, 각 scale의 feature map에 soft-max splatting을 적용하여 warped feature를 얻는다.
5. Warped feature를 image synthesis decoder의 대응하는 scale에 넣어 최종 렌더링된 이미지 $\hat I_t$를 생성한다.
<br/><br/><br/>

논문에서는 실제 비디오에서 random하게 샘플링된 시작 프레임과 target 프레임$(I_0, I_t)$을 사용하여 feature extractor, synthesis 네트워크를 공동으로 학습시킨다. $I_0$에서 $I_t$으로 생성한 flow field를 사용하여 $I_0$에서 인코딩된 feature를 warp하고, VGG perceptual loss를 사용하여 supervised 방식으로  실제 프레임 $I_t$와 생성한 $\hat I_t$에 대해 loss를 계산한다. 
<br/><br/><br/><br/><br/><br/>

# Experiments

---

## 1. Quantitative results

![GID_10.png](https://github.com/user-attachments/assets/b6840921-247b-4019-ac11-d4abd4671f70){: width="500px"}

위의 표는 baseline과의 양적 비교를 나타낸다. 결과를 살펴보면, 낮은 FVD와 DT-FVD distance는 논문의 방법으로 생성된 비디오가 더 현실적이고 시간적으로 더 일관성이 있음을 나타낸다. 
<br/><br/><br/>

![GID_11.png](https://github.com/user-attachments/assets/d22aa300-0589-4fae-a925-0ab616a6e0ec){: width="600px"}

위의 그림은 다양한 방법으로 생성된 비디오의 sliding window FID와 sliding window DT-FVD distance를 보여준다. global spectral volume representation 덕분에 논문에서 제안된 방법으로 생성된 비디오는 시간이 지남에 따라 degradation 되지 않는 것을 볼 수 있다.
<br/><br/><br/><br/>

## 2. Qualitative results

![GID_12.png](https://github.com/user-attachments/assets/04f37010-828e-4341-8ba1-939515b03f6b){: width="1100px"}

비디오의 질적 비교를 위해 생성된 비디오를 spatio-temporal X-t slice로 시각화했다. 이는 비디오에서 작은 motion을 시각화하는 표준적인 방법이다. 위의 그림에서 볼 수 있듯이, 생성된 비디오 역학은 다른 방법에 비해 실제 reference 비디오(두 번째 열)에서 관찰된 motion 패턴과 더 유사하다.
<br/><br/><br/><br/>

## 3. Ablation study

![GID_13.png](https://github.com/user-attachments/assets/218b8870-9ce8-43f3-ab9a-17ef338b9114){: width="600px"}

논문의 motion prediction, rendering module에 대한 검증을 위해 ablation study를 수행하여 다양한 변형에 대해 비교했다. 

1. 다양한 frequency band K = 4, 8, 16, 24를 사용하여 비교를 수행했다. 결과적으로, frequency band의 수를 늘리면 비디오 예측 품질이 향상되지만 16개 이상의 frequency에서는 개선이 미미하다는 것을 관찰했다. 
2. Ground truth spectral volume에서 adaptive frequency normalization을 제거하고 대신 입력 이미지의 width, height에 따라 scale을 진행했다. 
3. Frequency coordinated-denoising moduled을 제거하거나 간단한 버전으로 대체한 결과에 대해 조사했다.
4. 학습 가능한 weight에 따라 single-scale feature에 softmax splatting을 적용하는 baseline rendering 방법을 사용하여 결과를 비교했다.
<br/><br/><br/><br/>

## 4. Comparing to large video models

![GID_14.png](https://github.com/user-attachments/assets/f996cb3e-c44c-41fc-b1ed-81201c3097f9){: width="600px"}

마지막으로, user study를 수행하고, 비디오 volume을 직접 예측하는 최근의  large video diffusion 모델인 AnimateDiff, ModelScope 및 Gen-2의 애니메이션 결과와 비교했다. Testset에서 random하게 선택한 30개 비디오에서 user에게 "어떤 비디오가 더 사실적입니까?"라고 물었을 때, 다른 방법보다 논문의 방법으로 생성한 결과를 80.9%로 선호했다. 

또한 위의 그림에서 볼 수 있듯이 baseline에서 생성된 비디오는 입력 이미지 콘텐츠를 유지하지 못하거나 시간이 지남에 따라 점진적인 color drift 및 왜곡을 보인다.
<br/><br/><br/><br/>
