---
title: "[ë…¼ë¬¸ ë¦¬ë·°] Scalable Pre-training of Large Autoregressive Image Models (AIM)"
author: lunalee
date: 2024-04-03 21:43:39 +0000
categories: [AI, Paper Review]
tags: [Image, Autoregressive, Transformer]
pin: false
math: true
---

<br/><br/>
`Apple` `arXiv 2024`

- Paper: [https://arxiv.org/abs/2401.08541](https://arxiv.org/abs/2401.08541)
- Git: [https://github.com/apple/ml-aim](https://github.com/apple/ml-aim)
<br/><br/><br/><br/><br/>

# Introduction

---

Pre-training task agnostic modelì€ ìµœê·¼ NLPì˜ í‘œì¤€ì´ ë˜ì—ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ì€ ë³µì¡í•œ ì¶”ë¡  ì‘ì—…ì„ í•´ê²°í•˜ê³  ChatGPTì™€ ê°™ì´ AI assistantë¡œ ë„ë¦¬ ì‚¬ìš©ë˜ê³  ìˆë‹¤.  ì„±ê³µì˜ í•µì‹¬ìš”ì†Œë¡œëŠ” capacity(parameter ìˆ˜), pre-training dataì˜ ì¦ê°€ì— ë”°ë¼ í–¥ìƒë˜ëŠ” ëŠ¥ë ¥ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.

ì´ëŸ¬í•œ ëª¨ë¸ì˜ í™•ì¥ì€ ë‘ ê°€ì§€ ì¸¡ë©´ì—ì„œ ì¤‘ìš”í•˜ë‹¤.

1. ëª¨ë¸ì€ **ê³¼ê±°ë¥¼ ê³ ë ¤í•˜ì—¬ ë¬¸ì¥ì˜ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡**í•˜ëŠ” ê°„ë‹¨í•œ ëª©í‘œë¡œ í›ˆë ¨ë˜ì—ˆì§€ë§Œ **ê¸´ contextì— ê±¸ì³ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµ**í•  ìˆ˜ ìˆë‹¤.
2. Autoregressive objectiveì˜ scalability(í™•ì¥ì„±)ëŠ” íŠ¹ì • ì•„í‚¤í…ì²˜, íŠ¹íˆ Transformerì™€ í•¨ê»˜ ì‚¬ìš©ë  ë•Œ ì£¼ë¡œ ê´€ì°°ë˜ë©° ì‹œë„ˆì§€ë¥¼ ë‚¸ë‹¤.
<br/><br/>

ì´ëŸ¬í•œ ìš”ì†ŒëŠ” language modelingì—ë§Œ êµ­í•œë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤. ë˜í•œ ìµœê·¼ ViT(Vision Transfomer)ì˜ ì„±ê³µì€ Transformer architectureê°€ computer visionì—ì„œë„ ì„±ê³µì ìœ¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤. ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” LLMì˜ ê²°ê³¼ë¥¼ ì¼ë°˜í™”í•˜ê¸° ìœ„í•œ ì²« ë²ˆì§¸ ë‹¨ê³„ë¡œ autoregressive objectiveë¥¼ ì‚¬ìš©í•˜ì—¬ ViT ëª¨ë¸ì„ í•™ìŠµí•˜ì—¬ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆëŠ”ì§€ ì‚´í´ë³¸ë‹¤.
<br/><br/><br/>

![AIM_1.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/69a6f355-4b55-4223-a15a-bab6a804fa8f){: width="1100px"}

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” visual featureë¥¼ ìœ„í•œ large-scale pre-trainingì„ ìœ„í•´ autoregressive ì ‘ê·¼ ë°©ë²•ì„ ì‚¬ìš©í•œ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Autoregressive Image Models (AIM)</span></mark>**ì„ ì œì•ˆí–ˆë‹¤. Vision transformer, large-scale web data, LLM pre-trainingê³¼ ê°™ì€ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ì˜ iGPTì™€ ê°™ì€ ë°©ë²•ì„ ì¬ê²€í† í–ˆë‹¤. ë˜í•œ  autoregressive pre-trainingì„ visual featureì— ì ìš©í•˜ê¸° ìœ„í•œ ë‘ ê°€ì§€ architecture ìˆ˜ì •ì‚¬í•­ì„ ë„ì…í–ˆë‹¤. 

ì €ìëŠ” ì„ ë³„ë˜ì§€ ì•Šì€(uncurated) 2B ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ 600M - 7Bì˜ parameterë¥¼ ê°€ì§€ëŠ” ëª¨ë¸ì— ëŒ€í•´ ì—°êµ¬í–ˆë‹¤. AIMì€ ì´ëŸ¬í•œ ì´ë¯¸ì§€ ëŒ€ê·œëª¨ ëª¨ë¸ì— ëŒ€í•´ saturation ì—†ì´ ì§€ì†ì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ì´ëŒì–´ëƒˆë‹¤. ì „ë°˜ì ì¸ ê²°ê³¼ëŠ” large language modelì˜ scaling ì—°êµ¬ì™€ ì¼ì¹˜í•œë‹¤.
<br/><br/><br/><br/><br/><br/>

# Pre-training

---

[Data filtering networksğŸ“„]([https://arxiv.org/abs/2309.17425](https://arxiv.org/abs/2309.17425)) ë…¼ë¬¸ì—ì„œ ì†Œê°œí•œ DFN ë°ì´í„°ì…‹ì— ëŒ€í•´ pre-trainì„ ì§„í–‰í–ˆë‹¤. ë°ì´í„°ì…‹ì€ [Common Crawl]([https://commoncrawl.org/](https://commoncrawl.org/))ì—ì„œ í•„í„°ë§ëœ 12.8B  image-text pairë¡œ êµ¬ì„±ë˜ë©° ë¶€ì ì ˆí•œ ì½˜í…ì¸  ì œê±°, ì–¼êµ´ blur, ì¤‘ë³µ ì œê±° ë“±ì˜ pre-processë¥¼ ì§„í–‰í–ˆë‹¤. Data filtering networkì—ì„œ imageì™€ caption ê°„ì˜ alignment scoreë¥¼ ì¸¡ì •í•˜ì—¬ ìƒ˜í”Œ ìˆœìœ„ë¥¼ ë§¤ê¸´ í›„, 12.8B ë°ì´í„°ë‚´ì—ì„œ ìƒìœ„ 15% ìƒ˜í”Œì„ ì„ ì •í•˜ì—¬ **DFN-2B(subset)** ë°ì´í„°ì…‹ì´ ì¶”ì¶œëœë‹¤. (Privacy ë° safety filter ì™¸ì— ì´ë¯¸ì§€ contentë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ curationì€ í¬í•¨ë˜ì§€ ì•Šì•˜ë‹¤.)
<br/><br/>

 Pre-train ì¤‘ì— LLMì—ì„œ ì‚¬ìš©í•˜ëŠ” ê¸°ì¡´ ë°©ë²•ì„ ë”°ë¼  p = 0.8ì˜ í™•ë¥ ë¡œ DFN-2Bì—ì„œ ì´ë¯¸ì§€ë¥¼ ìƒ˜í”Œë§í•˜ê³  p = 0.2ì˜ í™•ë¥ ë¡œ ImageNet-1kì—ì„œ ì´ë¯¸ì§€ë¥¼ ìƒ˜í”Œë§í–ˆë‹¤. ì´ëŸ¬í•œ ë°ì´í„°ì…‹ì„ DFN-2B+ë¼ê³  í•œë‹¤.
<br/><br/><br/><br/><br/><br/>

# Approach

---

## 1. Training Objective

ë…¼ë¬¸ì˜ training objectiveëŠ” **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>í‘œì¤€ autoregressive model</span></mark>**ì„ ë”°ë¥¸ë‹¤. ì´ë¯¸ì§€ $x$ê°€ ì£¼ì–´ì§€ë©´ $K$ê°œì˜ non-overlapping patch grid $x_k , k \in [1, K]$ë¡œ ë¶„í• ë˜ì–´ **token**ì„ í˜•ì„±í•œë‹¤. ì´ ë•Œ sequence ìˆœì„œëŠ” ëª¨ë“  ì´ë¯¸ì§€ì—ì„œ ê³ ì •ë˜ì–´ìˆë‹¤ê³  ê°€ì •í•˜ê³  ì¼ë°˜ì ìœ¼ë¡œ **raster ordering**(ìœ„â†’ì•„ë˜, ì™¼â†’ì˜¤)ì„ ì‚¬ìš©í•œë‹¤. ì´ ë•Œ ì´ë¯¸ì§€ í•˜ë‚˜ì˜ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
P(x) = \prod^K_{k=1} P(x_kâˆ£x_{<k})
$$


ì—¬ê¸°ì„œ $x_{<k}$ëŠ” $k-1$ê¹Œì§€ì˜ patch ì§‘í•©ì„ ë‚˜íƒ€ë‚´ë©° $k$ë²ˆì§¸ íŒ¨ì¹˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” contextì´ë‹¤(**í˜„ì¬ ìˆœì„œê¹Œì§€ì˜ patchë¥¼ ì‚¬ìš©í•´ ë‹¤ìŒ patchë¥¼ ì˜ˆì¸¡**í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤). ê·¸ ë‹¤ìŒ ì´ë¯¸ì§€ ì „ì²´ $\mathcal{X}$ì— ëŒ€í•œ training lossì€ **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>negative log-likelihood(NLL)</span></mark>**ë¡œ ì •ì˜ëœë‹¤.

$$
\sum_{x \in \mathcal{X}} \sum^K_{k=1} -\log P(x_kâˆ£x_{<k})
$$

<br/><br/>

### Prediction loss

ìœ„ì˜ training objectiveëŠ” ë¶„í¬ $P(x_k âˆ£ x_{<k})$ë¥¼ ì •ì˜í•¨ì— ë”°ë¼ ë‹¤ì–‘í•˜ê²Œ ë³€í˜•ëœë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ [Masked autoencoders are scalable vision learnersğŸ“„]([https://arxiv.org/abs/2111.06377](https://arxiv.org/abs/2111.06377)) ë…¼ë¬¸ê³¼ ìœ ì‚¬í•˜ê²Œ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>normalized pixel-level regression loss</span></mark>**ë¥¼ ì‚¬ìš©í•œë‹¤. 
<br/><br/>

ì´ LossëŠ” $P(x_k âˆ£ x_{<k})$ë¥¼ **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>ì¼ì •í•œ varianceë¥¼ ê°–ëŠ” Gaussian ë¶„í¬</span></mark>**ë¡œ ê°€ì •í•œë‹¤.<br/>
ì¦‰, $\hat{x}_k(\theta)$ê°€ $\theta$ë¡œ ë§¤ê°œë³€ìˆ˜í™”ëœ ë„¤íŠ¸ì›Œí¬ì˜ $k$ë²ˆì§¸ patch predictionì´ê³  $x_k$ê°€ í•´ë‹¹ ground-truth valueì¸ ê²½ìš°, ëª©í‘œëŠ” predictionê³¼ ground-truth ì‚¬ì´ì˜ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>sum $\ell_2$ squared distanceë¥¼ minimize</span></mark>**í•˜ëŠ” ê²ƒì´ë‹¤.

$$
\min_\theta \frac{1}{K} \sum^K_{k=1} \Vert \hat{x}_k(\theta) - x_k \Vert^2_2.
$$

<br/><br/><br/><br/><br/>

## 2. Architecture

![AIM_2.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/59567fba-a588-436f-826f-9b516125d6f5){: width="500px"}

Backboneìœ¼ë¡œ **ViT(Vision Transformer)**ë¥¼ ì‚¬ìš©í–ˆë‹¤. Model capacity scalingì„ ìœ„í•´ language modelingì˜ ì¼ë°˜ì ì¸ ë°©ë²•ì„ ë”°ë¥´ê³  depth í™•ì¥ë³´ë‹¤ëŠ” **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>width í™•ì¥ì„ ìš°ì„ ì‹œ</span></mark>**í•œë‹¤. AIMì˜ design parameterì— ëŒ€í•œ overviewëŠ” ì•„ë˜ í‘œì™€ ê°™ë‹¤.

![AIM_3.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/202096a0-756a-44af-bd34-51ab5e6b701c){: width="550px"}
<br/><br/><br/><br/>

Pre-training ì¤‘ ì´ì „ patchê°€ ì£¼ì–´ì§€ë©´, Self-attention layerì— **causal mask**ë¥¼ ì ìš©í•˜ì—¬ ë‹¤ìŒ patchì˜ í™•ë¥ ì„ ëª¨ë¸ë§í•œë‹¤. Patch $i$ì— ëŒ€í•œ embeddingì€ ì•„ë˜ì™€ ê°™ì´ ê³„ì‚°ëœë‹¤.

$$
y_i = \sum^K_{k=1} a_{ik}v_i
$$

ì—¬ê¸°ì„œ $a_{ik}$ëŠ” attention weightì´ê³  $v_k$ëŠ” value embeddingì´ë‹¤.  $k > i$ì— ëŒ€í•´ $a_{ik} = 0, \sum ^K_{k=1} a_{ik} = 1$ë¡œ ì„¤ì •í•˜ì—¬ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>ì´ì „ sequenceì˜ patchë§Œì„ ë³´ë„ë¡ í•˜ëŠ” casual mask</span></mark>**ë¥¼ ì ìš©í•˜ì˜€ë‹¤(ì´í›„ì˜ patchëŠ” ë³´ì§€ ì•ŠìŒ). ì¦‰, training ì¤‘ ì´ë¯¸ì§€ëŠ” **single forward pass**ë¡œ ì²˜ë¦¬ëœë‹¤. 
<br/><br/><br/><br/>

### Prefix Transformer.

![AIM_4.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/61a80bca-1df9-4f7f-8b12-8854ec85015b){: width="500px"}

Pre-training ì¤‘ self-attentionì—ëŠ” causal maskë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ, í‘œì¤€ ViT ëª¨ë¸ì˜ down-stream taskì—ì„œëŠ” bidirectional self-attentionì„ í•„ìš”ë¡œ í•œë‹¤. ì´ëŸ¬í•œ ë¶ˆì¼ì¹˜ëŠ” ì„±ëŠ¥ ì €í•˜ë¡œ ì´ì–´ì§€ê²Œ ë˜ë¯€ë¡œ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ [PrefixLMğŸ“„](https://arxiv.org/abs/1910.10683) ë…¼ë¬¸ê³¼ ê°™ì€ ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. 

**<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>ì´ˆê¸° ì¼ë¶€ë¶„ì˜ patchë¥¼ prefixë¡œ ê°„ì£¼</span></mark>**í•˜ê³ , ë‚˜ë¨¸ì§€ patchì—ì„œ ë³¼ ìˆ˜ ìˆë„ë¡(ë‚˜ë¨¸ì§€ patchë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>contextë¡œ í™œìš©</span></mark>**) casual maskë¥¼ ì œê±°í•œë‹¤(ë°©ë²•: prefix length size $S âˆˆ [1, K âˆ’ 1],\; k < S$ì— ëŒ€í•´ $a_{i,k} > 0$). ì´ ë°©ë²•ì„ í†µí•´ causal masking ì—†ì´ë„ ëª¨ë¸ì´ ë™ì‘í•  ìˆ˜ ìˆê³  downstream taskë¥¼ ìœ„í•œ ì¶”ê°€ì‘ì—… ì—†ì´ ì„±ëŠ¥ì„ ì˜¬ë¦´ ìˆ˜ ìˆë‹¤.
<br/><br/><br/><br/>

### MLP prediction heads.

Networkê°€ pre-training objectiveì— íŠ¹í™”ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì¼ë°˜ì ìœ¼ë¡œ pre-training ì¤‘ì— íŠ¹ì •í•œ prediction headë¥¼ ì¶”ê°€í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” transformer ìœ„ì— N blockì˜ MLPë¥¼ ì‚¬ìš©í•˜ì—¬ ê° patchë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í–ˆë‹¤. 
<br/><br/><br/><br/>

### Straightforward implementation.

- AIMì—ì„œëŠ” LayerScale, stochastic depth, QK-Norm, freezingê³¼ ê°™ì€ optimization ì•ˆì •ì„± ìœ ë„ ì‘ì—…ì´ í•„ìš”í•˜ì§€ ì•ŠìŒ
- Transformer ì•, MLP head ì•ì— sinusoidal positional embeddingì„ ì¶”ê°€í•¨
- Transformer, Headì— ì‚¬ìš©ë˜ëŠ” MLPëŠ” expansion ratio 4ë¥¼ ì‚¬ìš©í•¨
- ê¸°ì¡´ì˜ ViTì™€ ë‹¬ë¦¬ ì…ë ¥ì— classification tokenì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
<br/><br/><br/><br/>

### Downstream adaptation.

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” down-stream taskë¥¼ ìœ„í•´ model weightì„ fixí•˜ê³  classification headì— ëŒ€í•´ì„œë§Œ trainingì„ ì§„í–‰í–ˆë‹¤.
<br/><br/>

Pre-training ì¤‘ lossëŠ” ê° patchì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°ë˜ì—ˆê³  **Image-level toke**nì€ ì¡´ì¬í•˜ì§€ ì•Šì•˜ë‹¤. **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Image-level prediction</span></mark>**ì„ ìœ„í•œ globalí•œ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ ì¼ë°˜ì ìœ¼ë¡œ patch featureì—ì„œ global average poolingì„ ì‚¬ìš©í•œë‹¤. í•˜ì§€ë§Œ AIMì€ linear classifier ì•ì— **attention pooling operation**ë¥¼ ì‚¬ìš©í–ˆë‹¤.

patch features ì§‘í•© $P = \lbrace p_i âˆ£ 1 â‰¤ i â‰¤ K\rbrace$ê°€ ì£¼ì–´ì§ˆ ë•Œ, **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>multi-head attention pooling</span></mark>**ì„ ì‚¬ìš©í•œ global descriptor $\hat{p}$ ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.
<br/>

$$
\hat{p_h} = \sum^K_{i=1}\frac{\exp(q^T_h W^k_hp_i)}{\sum^K_{j=1}\exp(q^T_h W^k_hp_j)}W^v_h p_i
$$

<br/><br/>

ê° attention head  $h = \lbrace 1, ..., H \rbrace$ì— ëŒ€í•´ $W^h_k, W^h_v âˆˆ R^{d_h \times d}$ ëŠ” ê°ê° **key, value weight**ì„ ë‚˜íƒ€ë‚¸ë‹¤. $q_h$ ëŠ” learnableí•œ **query vector**ì´ë‹¤. 

ìœ„ ì‹ì˜ ê²°ê³¼ë¡œ linear classifier ì…ë ¥ì´ ë˜ëŠ” pooled feature $\hat{p} = [p_1 , ..., p_H ], \hat{p} \in R^d$ ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ attention poolingì„ ì‚¬ìš©í•˜ë©´ ì „ì²´ ì‘ì—…ì´ ì—„ê²©í•˜ê²Œ linearí•˜ê²Œ ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì´ë¥¼ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>"Attentive Probe"</span></mark>**ë¼ê³  ë¶€ë¥¸ë‹¤. ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  linear probingì˜ ì¥ì (ì˜ˆ: ì ì€ ì¶”ê°€ parsmeter ìˆ˜ ë° overfitting ìœ„í—˜ ê°ì†Œ)ì€ ì´ probeì—ì„œë„ ê·¸ëŒ€ë¡œ ìœ ì§€ëœë‹¤.
<br/><br/><br/><br/><br/><br/>

# Results

---

## 1. Impact of scaling

ì €ìëŠ” model parameter ë° training data ì¸¡ë©´ì—ì„œ scalingì˜ ì˜í–¥ì— ëŒ€í•´ ì¡°ì‚¬í–ˆë‹¤. ì‹¤í—˜ì€ IN-1k ë°ì´í„°ì…‹ì˜ validationì— ëŒ€í•´ ìˆ˜í–‰ë˜ì—ˆë‹¤.

![AIM_7.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/88aaab1f-521d-4054-ac1f-0c143660d21a){: width="1300px"}

ê° ëª¨ë¸ì— ëŒ€í•´ training iterationì— ë”°ë¥¸ pre-training loss ê°’ê³¼ validation setì˜ classification accuracyë¥¼ ì¸¡ì •í•˜ì˜€ë‹¤. ê²°ê³¼ëŠ” ìœ„ì˜ ê·¸ë˜í”„ì™€ ê°™ì´ ì „ì²´ training ë™ì•ˆ ê°œì„ ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ë˜í•œ ëª¨ë¸ì˜ capacityë¥¼ scalingí•¨ì— ë”°ë¼ down-stream ì‘ì—…ì˜ loss ê°’ê³¼ accuracyê°€ í–¥ìƒë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ìˆë‹¤.
<br/><br/><br/><br/>

![AIM_8.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/36461eb9-1519-47fe-bfc7-db5bd2d0cf51){: width="500px"}

ìœ„ì˜ ê·¸ë˜í”„ëŠ” 1M ì´ë¯¸ì§€ì˜ ì‘ì€ curated ë°ì´í„°ì…‹(ì˜ˆ: IN-1k) ë˜ëŠ” í° 2B ì´ë¯¸ì§€ ì„¸íŠ¸(ì˜ˆ: DFN-2B+)ë¥¼ pre-trainí•  ë•Œ validation lossì˜ ì§„í–‰ì„ ë‚˜íƒ€ë‚¸ë‹¤. IN-1kì— ëŒ€í•œ í•™ìŠµì€ ë¹ ë¥´ê²Œ validation lossê°€ ì¤„ì–´ë“¤ì§€ë§Œ overfittingì„ ë³´ì´ê³  ìˆë‹¤. ë°˜ë©´ uncurated DFN-2B ë°ì´í„°ì…‹ì€ validation lossê°€ ë¹¨ë¦¬ ì¤„ì–´ë“¤ì§€ëŠ” ì•Šì§€ë§Œ overfittingì´ ì¼ì–´ë‚˜ì§€ ì•Šì•˜ë‹¤.
<br/><br/>

ìœ„ì—ì„œ ì–¸ê¸‰í•œ ëŒ€ë¡œ ë™ì¼í•œ ë°ì´í„°ì…‹ì´ ê·œëª¨ê°€ ì‘ì€ IN-1k ë°ì´í„°ë¡œ augmentë˜ë©´ ê²°êµ­ IN-1kì— ëŒ€í•œ pre-trainì„ ì§„í–‰í•œ ê²ƒ ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

![AIM_9.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/1ee40632-9d33-4bc7-8e98-009778c1446b){: width="500px"}
<br/><br/><br/><br/><br/>

## 2. Architecture and Design

![AIM_10.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/c58a59eb-1035-43e4-a8ad-63d7b6eec52e){: width="1200px"}

- **Targets and objective (a):** target patchì— ëŒ€í•œ ë‹¤ì–‘í•œ representationì— ëŒ€í•´ ì¡°ì‚¬í–ˆë‹¤.
- **Autoregression pattern (b):** Autoregressive pre-trainingì„ ìœ„í•œ patch ìˆœì„œë¥¼ ì •í•˜ëŠ” ë°©ì‹ì— ëŒ€í•´ ì¡°ì‚¬í–ˆë‹¤. ([ì—°ê´€ 1.](#1-training-objective)) ì•„ë˜ì˜ Figure 7.ì—ì„œ ê°ê°ì˜ ë°©ë²•ì— ëŒ€í•œ patch prediction difficultyë¥¼ ì¡°ì‚¬í–ˆë‹¤.
- **Cropping scale (c):** cropping scaleì˜ í•˜í•œì„ ì¡°ì •í•˜ì—¬ ê° patchì˜ ì •ë³´ contentê°€ ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•´ ì¡°ì‚¬í–ˆë‹¤.
- **Causal *vs*. Prefix Attention (d):** í‘œì¤€ causal attentionë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ë°˜ëŒ€ë¡œ pre-trainì¤‘ì— prefix attentionë¥¼ í†µí•©í•˜ëŠ” ê²ƒì˜ ì˜í–¥ì„ ì¸¡ì •í–ˆë‹¤.
- **Head design (e):** pixel level predictionì„ ìœ„í•´ backboneì˜ topì— ìˆëŠ” ë‹¤ì–‘í•œ ìœ í˜•ì˜ headë¥¼ ê³ ë ¤í–ˆë‹¤.
- **Deeper *vs*. Wider architecture (f):** Depthê°€ Widthë³´ë‹¤ ë” ë¹ ë¥´ê²Œ í™•ì¥ë˜ëŠ” ViTì˜ ì›ë˜ ë””ìì¸ê³¼ ë‹¬ë¦¬ Llamaì™€ ìœ ì‚¬í•œ scaling ì „ëµì„ ì±„íƒí–ˆë‹¤. ìœ„ì˜ í‘œ 3fì—ì„œ wideí•œ ì•„í‚¤í…ì²˜ì˜ íš¨ìœ¨ì„±ì„ ê²€ì¦í–ˆë‹¤. ì‘ì€ ê·œëª¨ì˜ AIM-0.6B ëª¨ë¸ì˜ ê²½ìš°ì—ë„ wideí•œ ì•„í‚¤í…ì²˜ê°€ ì¢‹ì€ ì„±ëŠ¥ì„ ì œê³µí•  ë¿ë§Œ ì•„ë‹ˆë¼ í›ˆë ¨ ì•ˆì •ì„±ë„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.

![AIM_11.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/95ac1568-16a9-4100-a68f-3b13d53d882e){: width="500px"}
<br/><br/><br/><br/><br/>

## 3. Pre-training objective

![AIM_12.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/c01980da-dd52-4c12-afd0-f11da6c05ae5){: width="500px"}

**Autoregressive vs. Masking:** autoregressive objectiveì™€ masking objectiveë¡œ í•™ìŠµëœ ëª¨ë¸ì— ëŒ€í•´ ì¡°ì‚¬í–ˆë‹¤. ìœ„ì˜ í‘œëŠ” AIMì´ masking objectiveë³´ë‹¤ autoregressive objectiveì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.
<br/><br/><br/><br/><br/>

## 4. Comparison with other methods

15ê°€ì§€ benchmarkì— ëŒ€í•´ SOTA ë°©ë²•ë“¤ê³¼ ë¹„êµí–ˆë‹¤. ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

![AIM_13.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/fad28072-62ce-4452-8dd3-b651d0306fc5){: width="1000px"}
<br/><br/><br/><br/><br/>
