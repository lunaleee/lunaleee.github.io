---
title: "BLEU, CIDEr, SPICE 텍스트 생성 성능 평가 지표 총정리, 장단점 비교(feat. Rouge)"
author: lunalee
date: 2024-09-05 21:34:17 +0900
categories: [AI, Concept Note]
tags: [NLP, Basic]
pin: false
math: true
---
<br/>

오늘은 Image Captioning을 평가하는 대표적인 평가지표 3가지 BLEU, CIDEr, SPICE에 대해 알아보자. BLEU, CIDEr, SPICE을 위해 알고 넘어가야하는 개념부터, 각 지표에 대한 내용까지 차례대로 정리해보자. 각 방법에 대해 정리하기 전 TF-IDF의 개념이나 N-gram의 개념에 대한 이해가 필요하다면 해당 블로그의 관련 포스팅을 참고해보자!!
<br/>

- [NLP 기초: TF-IDF 개념, 계산 방법, 코드 구현(직접 구현부터 라이브러리를 사용한 간단 구현까지)](https://lunaleee.github.io/posts/tf-idf/)
- [NLP 기초: N-gram 개념 정리 (feat. Statistical Language Model, SLM)](https://lunaleee.github.io/posts/n-gram/)
<br/><br/><br/><br/><br/>

# BLEU (Bilingual Evaluation Understudy Score)

---

BLUE는 모델로부터 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>생성된 문장(Generated sentence)과 실제 정답(Reference sentence)이 얼마나 유사한지를 비교</span></mark>**하여 생성된 문장의 성능을 측정하는 방법이다. 기계 번역, Image captioning등의 성능 평가 지표로 사용된다. 이 때, 측정 기준은 **n-gram**에 기반한다.
<br/><br/><br/>

## 1. N-gram Precision

Generated sentence와 Reference sentence가 존재한다고 하자. 이 때 BLEU Score는 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>Gererated sentence의 단어가 Reference sentence에 얼마나 포함되는가</span></mark>**를 나타내는 수치라고 볼 수 있다(이 때, Reference sentence는 여러개가 될 수도 있으며, 이런 경우 Gererated sentence의 단어가 Reference sentence 집합에 얼마나 포함되는가로 볼 수있다).

계산 방법은 **“카운트”, 즉 빈도수**에 기반한다. 즉, Generated Sentence에 존재하는 단어가 Reference Setence에 존재하는지 하나씩 카운트하는 방식이다.
<br/>

$$
\text{Unigram Precision} = \frac{\text{G의 단어 중 R에 존재하는 단어 수}} {\text{G의 총 단어 수}}
$$

<br/><br/>

N-gram에 기반하는 이유는, 정확히는 “단어” 대신 “N-gram”으로 계산하기 때문이다. N-gram을 사용하므로서 어느정도 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>순서</span></mark>**를 고려해 줄 수 있다. 즉, 결론적으로는  **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>Gererated sentence의 n-gram이 Reference sentence에 얼마나 포함되는가</span></mark>**를 나타내는 수치라고 볼 수 있다. 
<br/>

$$
\text{N-gram Precision} = \frac{\text{G의 n-gram 중 R에 존재하는 n-gram 수}} {\text{G의 총 n-gram 수}}
$$

<br/><br/><br/><br/>

## 2. Modified N-gram Precision: Clipping

하지만 위에 언급한 방법은 문제점이 존재한다. 만약, Generated Sentence에 하나의 단어가 중복되어 여러번 등장하는 경우이다. 아래의 예시를 확인해보자.

![NS_1.png](https://github.com/user-attachments/assets/88c3bd57-ae22-47e1-a053-30f51e0f9a6e){: width="600px"}

이렇게 똑같은 단어가 여러번 등장하는 문제를 방지하기 위해 중복을 제거하기 위한 **“Clipping”**을 수행한다. 즉, **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>Generated Sentence에서 특정 단어가 등장하는 횟수를 카운트 할 때, Reference Sentence에 존재하는 해당 단어의 횟수를 초과해서 카운트할 수 없도록</span></mark>** 하는 것이다. 말이 헷갈린다면 보정된 예시를 참고해보자.

![NS_2.png](https://github.com/user-attachments/assets/c8da8ce7-5304-4d66-be4c-5f83c73a160e){: width="600px"}
<br/><br/><br/>

수식으로 표현하면 Modified Precision은 다음과 같이 정리할 수 있다.(G는 Generated Sentence를 의미한다.)
<br/>

$$
p_n = \frac{\sum_{n\text{-}gram \in G} Count_{clip}(n\text{-}gram)} {\sum_{n\text{-}gram \in G} Count_{clip}(n\text{-}gram)}
$$

<br/><br/>

마지막으로,  BLEU 식은 각각의 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>N-gram에 대해 계산한 $p_1, p_2, …, p_n$을 모두 조합하여 계산</span></mark>**한다. 예를 들어, BLEU@4를 구한다고 하면 N은 4가 되고, $p_1, p_2, p_3, p_4$를 조합하여 수식이 계산된다. 수식으로 표현하면 다음과 같다. 아래 수식에서 $w_n$은 가중치로, 각각의 Modified Precision에 서로 다른 가중치를 줄 수 있다.
<br/>

$$
BLEU = \exp(\sum^N_{n=1}w_n \log p_n)
$$

<br/><br/><br/><br/>

## 3. Brevity Penalty

마지막으로, 짧은 문장에 대한 패널티를 추가한다. 만약, 생성된 문장의 길이가 Reference Sentence에 비해 길다면 그만큼 Reference Sentence에 속하지 않을 확률이 높아지므로 점수가 낮게 측정되겠지만, 생성된 문장이 짧을 경우 높은 점수를 받을 수 있다. 이런 경우를 위한 패널티가 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>Brevity Penalty</span></mark>**이다.

Generated Sentence가 Reference Sentence보다 길다면, BP를 1로 설정하고, 짧다면 아래와 같은 값을 곱해주어 패널티를 부여한다. ($c$는 Generated Sentence의 길이, $r$은 Reference Sentence의 길이를 의미한다.)
<br/>

$$
 BP = \begin{cases}
   1 &\text{if } c>r\\
   e^{(1-r/c)} &\text{if } c \le r
\end{cases}
$$

<br/><br/>
최종적인 BLEU Score는 다음과 같다.
<br/>

$$
BLEU = BP \times \exp(\sum^N_{n=1}w_n \log p_n)
$$

<br/><br/><br/><br/>

**장점**:

- **특정 언어에 제한적이지 않음:** 언어의 종류와 상관 없이 사용할 수 있다.
- **단순하고 효율적**: 계산 방법이 단순하고 빠르다.
- **짧은 문장 평가에 적합**: 짧은 문장에서 중복을 줄이고 효율적인 평가가 가능함.
<br/><br/>

**단점**:

- **단어의 의미적 유사도 반영 불가:** 단어의 의미적인 유사도는 고려하지 않는다.
- **문장의 의미적 일치 반영 부족**: 단순히 n-그램 일치를 기준으로 하기 때문에 문장의 의미나 구조적 유사성을 잘 반영하지 못함.
- **Recall을 고려하지 않음**: Precision만 고려하는 방법으로, Recall을 고려하지 않아 중요한 정보가 누락되었을 때 이를 반영하지 않음.
- **긴 문장에서는 부정확**: 긴 문장의 경우 정밀도에 의한 페널티가 크게 작용하여 부정확한 평가가 될 수 있음.
<br/><br/><br/><br/>

## + Rouge(Recall-Oriented Understudy for Gisting Evaluation)

BLEU가 Generated Sentence의 단어가 Reference Sentence에 얼마나 포함되는가, 즉 Precision과 같은 개념이라고 한다면, Rouge는 반대로 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Reference Sentence의 단어가 Generated Sentence에 얼마나 포함되는가</span></mark>**, 즉 **Recall**과 같은 개념이다. 기본적인 계산 방법은 BLEU와 동일하나, G와 R을 반대로 계산하면 Rouge를 구할 수 있다.
<br/><br/><br/><br/><br/><br/><br/>

# CIDEr (Consensus-based Image Description Evaluation)

---

배경: CIDEr는 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Image Captioning</span></mark>**을 평가하기 위한 평가 지표이다. 기존에 BLEU, Rouge와 같은 방법이 인간의 판단과 일치하지 않는다는 문제에서 시작되었다. 인간의 평가는 정확성, 단어의 중요성등 여러 측면을 고려한다는 점에서 설계되었으며, '다수의 사람들이 생각하는 일반적인 설명'에 부합하기 위한 방법을 고려했다고 한다.
<br/><br/>

CIDEr는 기본적으로 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>TF-IDF(Term Frequency-Inverse Document Frequency) 가중치 방식</span></mark>**을 사용한다. BLEU와 마찬가지로 Generated Sentence와 Reference Sentence가 얼마나 유사한지를 기반으로 한다. 하지만 여기에 다음과 같은 요소를 고려한 평가를 진행한다.

1. **N-gram 기반 유사성 측정**: 문장을 단어 단위가 아닌 n-gram으로 분석하여, 단순히 단어만 일치하는 것이 아니라 **문장 내에서의 구문적 구조와 의미적 연관성**까지 반영.
2. **TF-IDF 가중치 적용**: TF는 특정 n-gram이 Reference Sentence에서 얼마나 자주 등장하는지를 나타내며, IDF는 데이터셋 전체에서 그 n-gram이 얼마나 희귀한지를 측정한다. 이를 통해 흔한 n-gram은 낮은 가중치를 받게 되어, **흔하지 않으면서 중요한 정보에 더 높은 가중치**를 부여하게 된다.
3. **코사인 유사도(cosine similarity) 계산**: Generated Sentence과 Reference Sentence 사이의 코사인 유사도를 계산하여 둘이 얼마나 많이 일치하는지(**precision**)와 Reference Sentence 에서 얼마나 많은 부분을 포괄하는지(**recall**)를 모두 반영한다.
<br/><br/><br/>

그럼 이제 계산 방법을 살펴보자.
<br/><br/><br/><br/><br/>

## 1. TF-IDF 가중치 계산(N-gram)

- 먼저, 생성된 문장과 참조 문장을 **n-gram**(n개의 연속적인 단어 집합) $w_n$으로 변환한다. CIDEr는 주로 **1-gram부터 4-gram**까지 사용한다.
ex) "The cat is sitting on the mat” →  $w_2$:  `["The cat", "cat is", "is sitting", "sitting on", "on the", "the mat"]`)
- 각 n-gram 원소에 대해 **TF-IDF 가중치**를 계산한다.
    
    ![NS_3.png](https://github.com/user-attachments/assets/e0557a1c-2806-4af7-a669-c0f549b7c26f){: width="700px"}
    
    <span style='color: var(--txt-gray)'>+ 여기서 전체이미지 개수란, 데이터셋에 존재하는 전체 이미지의 수를 의미한다고한다. 그렇다면 개별적인 이미지에 대해서 평가해야하는 경우에는 어떻게 계산해야하는지 의문이 든다. 이점은 CIDEr 계산 방법을 알아본 뒤, 마지막 부분에 설명을 추가하도록 하겠다. (바로 넘어가기 링크: [+ 데이터셋 기준](#-데이터셋-기준))</span>
  
<br/><br/><br/><br/>

## 2. 코사인 유사도(Cosine Similarity) 계산

다음으로는 n-gram TF-IDF 벡터를 기반으로 코사인 유사도를 계산한다. 

![NS_4.png](https://github.com/user-attachments/assets/1b293d77-84c9-4c47-b8f0-6a9956024dc7){: width="550px"}

코사인 유사도는 precision과 recall을 모두 반영한다.
<br/><br/><br/><br/><br/>

## 3. N-gram 별 CIDEr 점수 합산

각각의 n-gram에 대해 계산한 **코사인 유사도를 결합**하여 최종 CIDEr 점수를 계산한다. 각각의 n-gram 유사도를 가중 평균하여 계산한다.

![NS_5.png](https://github.com/user-attachments/assets/4625cbdd-47b0-477d-969d-7db5903cb0b0){: width="450px"}

예를 들어, 논문에서는 N=4를 사용하여 1-gram, 2-gram, 3-gram, 4-gram을 가중 평균한다.
<br/><br/><br/><br/><br/>

## + 데이터셋 기준

앞서 언급했듯, IDF값을 계산하는데 있어 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>고정된 데이터셋의 이미지 개수</span></mark>** 즉, “데이터셋에 존재하는 전체 이미지 수”를 어떻게 설정할지에 대한 문제는 중요한 요소이다.  이 문제는 CIDEr가 데이터셋 전체에서의 n-gram 빈도를 고려하기 때문이다.  따라서, $I$를 설정하는 방법에 따라 CIDEr의 계산이 달라질 수 있다.
<br/><br/>

1. **사전 정의된 데이터셋의 사용**: **특정 벤치마크**에서 제공하는 **표준 데이터셋**의 이미지를 사용하여 CIDEr 점수를 계산할 경우는 기본적으로 해당 데이터셋의 이미지 개수를 사용한다.
2. **데이터셋 크기를 고정하여 계산: 평가하고자 하는 모델을 학습할 때 사용했던 데이터셋을 기준**으로 설정한다. 즉, 학습할 때와 마찬가지로, 평가할 때도 특정 데이터셋에 대해 훈련된 모델을 사용하여 평가를 수행하므로, 그 데이터셋의 전체 이미지 개수를 사용한다.
3. **개인화된 설정:** 사용자가 **개인적으로 정의한 새로운 데이터셋**이나 **독립적인 이미지에 대해 평가**를 수행하려는 경우, 사전에 사용된 데이터셋(예: 학습 데이터)의 전체 이미지 개수를 사용하거나, 혹은 평가 시에 사용할 표준 데이터셋을 새롭게 설정해 $∣I∣$ 값을 고정할 수 있다.
4. **CIDEr-D 사용:** 실제로 **MS COCO 평가 서버**에서 사용하는 **CIDEr-D**는 이러한 상황을 고려하여 CIDEr의 기본 계산 방식에 몇 가지 수정 사항을 추가했다. CIDEr-D는 **개별적으로 점수가 최적화되지 않도록** 일부 가중치나 계산 방식을 수정하여 **보다 일반화된 평가**가 가능하도록 만든다. 이를 통해 개별 이미지 평가에서도 일관된 평가를 할 수 있도록 한다.

<br/><br/><br/><br/><br/>

**장점**:

- **특정 언어에 제한적이지 않음**: 언어의 종류와 상관 없이 사용할 수 있다.
- **Precision, Recall 모두 반영**: 코사인 유사도를 사용하여 문장이 얼마나 참조 문장과 유사한지를 Precision과 Recall 관점에서 평가할 수 있다.
- **다수의 인간 평가와의 높은 일치**: n-gram에 TF-IDF 가중치를 부여하여 일반적이지 않은 정보에 더 높은 가중치를 줌으로써, 인간 평가와 유사한 결과를 도출하는 효과가 있다.
<br/><br/>

**단점**:

- **단어의 의미적 유사도 반영 불가**: 단어의 의미적인 유사도는 고려하지 않는다.
- **문장 구조보다는 단순한 단어 매칭 중점**: 의미적 일치보다는 단어 사용 빈도에 중점을 두므로 문장 구조나 깊은 의미를 반영하지는 못함.
- **계산 복잡성**: TF-IDF를 기반으로 하여 계산이 BLEU에 비해 복잡하고 시간이 오래 걸릴 수 있음.
<br/><br/><br/><br/><br/><br/><br/>

# SPICE (Semantic Propositional Image Caption Evaluation)

---

배경: SPICE는 마찬가지로 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Image Captioning</span></mark>**을 평가하기 위한 평가 지표이다. 기존 지표들(BLEU, ROUGE, CIDEr, METEOR)은 주로 **n-gram 중복**을 기반으로 캡션의 품질을 평가하는데, 이는 문장의 의미를 정확하게 반영하지 못하는 한계가 있다. 예를 들어, 두 문장이 같은 의미를 전달하더라도 단어가 다르면 낮은 점수를 받거나, 전혀 다른 의미의 문장이지만 단어가 겹치면 높은 점수를 받는 문제가 발생했다. SPICE는 캡션의 **의미적 내용**을 평가하여 인간의 평가와 더 일치하는 결과를 제공하려는 목적을 가지고 개발되었다.
<br/><br/>

이를 극복하기 위해 SPICE는 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>의미론적 제안(semantic propositional content)을 중점적으로 평가</span></mark>**하는 방법을 도입했다. SPICE는 이미지와 캡션의 **객체(object), 속성(attribute), 관계(relation)**를 나타내는 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>장면 그래프(scene graph)</span></mark>**를 이용해, 의미적인 내용을 비교하여 캡션의 품질을 측정한다.
<br/><br/><br/>

그럼 SPICE Score를 구하는 방법을 차례대로 살펴보자.
<br/><br/><br/><br/>

## 1. 캡션을 장면 그래프(Scene graph)로 변환

- 먼저, 참조 캡션(reference caption)과 생성된 캡션(candidate caption)을 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>장면 그래프(scene graph)</span></mark>**로 변환한다.
- 장면 그래프는 캡션에 포함된 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>객체(object), 속성(attribute), 관계(relation)</span></mark>**를 추출하여 표현한다. 
예를 들어, “A young girl standing on top of a tennis court”라는 문장은 ‘girl(객체)’, ‘young(속성)’, ‘tennis court(객체)’, ‘standing(관계)’로 분해된다.
    
    ![NS_6.png](https://github.com/user-attachments/assets/01758f02-f55e-48cc-a815-cdf1fa62b6a1){: width="600px"}
    
- 객체(object) 집합은 $O(c)$, 객체들간의 관계(relation)을 나타내는 집합은 $E(c)$, 객체에 대한 속성(attribute)를 나타내는 집합은 $K(c)$로 표현된다($c$는 캡션을 의미한다.).
<br/><br/><br/>

#### ※ 장면 그래프(Scene graph)로 변환하는 과정에 대한 추가적인 설명

캡션 문장이 주어지면, 그래프가 만들어지는 과정은 다음과 같은 순서로 진행된다.

1. **학습된 Parser**를 이용하여 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>의존 구문 트리(Dependency Parse Tree)</span></mark>**로 변환<br/>
  학습된 **의존 구문 분석기(Dependency Parser)**를 사용하여 캡션 문장을 의존 구문 트리로 만든다. 이 의존 구문 분석기는 사전에 대규모 corpus에 의해 학습된 것을 가져다가 사용하며, 대표적인 의존 구문 분석기로는 **Stanford Dependency Parser**나 **spaCy**, **CoreNLP**가 있다.<br/>
  이러한 분석기는 대규모 텍스트 데이터로 훈련되어 다양한 문장 구조를 인식하고 구문 분석을 수행할 수 있다. **주어진 문장에서 단어 간의 의존 관계를 예측**하며, 각 단어는 해당 문장에서 문법적으로 어떻게 연결되는지를 나타내는 트리로 변환된다.<br/>
  ![NS_7.png](https://github.com/user-attachments/assets/7e648375-e8b4-4a8d-9f97-f37cf8c8223c){: width="600px"}
  _이미지 출처: [https://velog.io/@tobigs1516text/CS224N-Lecture5-Dependency-Parsing-crcm0227](https://velog.io/@tobigs1516text/CS224N-Lecture5-Dependency-Parsing-crcm0227)_
  <br/><br/>
2. **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>Rule-based</span></mark>**로 장면 그래프로 변환<br/>
  의존 구문 트리에서 장면 그래프(Scene Graph)로 변환할 때 사용하는 규칙들은 사전에 정의된 일련의 **언어적 규칙**에 기반한다. 예를 들어, '형용사(amod)'는 명사를 수식하므로, 의존 관계에서 형용사가 수식하는 명사와 연결하여 **속성**으로 변환되고, '주어(nsubj)'와 '목적어(dobj)'와 같은 관계들은 객체 간의 **관계**로 변환된다. 이렇게 의존 구문 트리에서 추출된 정보는 객체, 속성, 관계의 세 가지 요소로 정리되며, 이들이 서로 어떻게 연결되어 있는지를 나타내는 **장면 그래프**를 구성하게 된다.
<br/><br/><br/><br/><br/>

## 2. Reference 캡션과 Candidate 캡션의 장면 그래프 비교

- 각 캡션의 장면 그래프는 의미적 제안(semantic propositions)을 나타내는 **논리적 튜플(logical tuple)**로 변환된다. 튜플은 $T(\cdot)$ 와 같이 표현되며, 객체, 속성, 관계를 각각 나타낸다($c$는 캡션을 의미한다).
    <br/>

    $$
    T(G(c)) \triangleq O(c) \cup  E(c)  \cup  K(c)
    $$
    
    <br/>
- 예를 들어, 위의 그림 예시에 대해 다음과 같은 튜플들이 생성된다.
    <br/>

    $$
    \{ \text{ (girl), (court), (girl, young), (girl, standing) (court, tennis), (girl, on-top-of, court) } \}
    $$
    
<br/><br/><br/><br/>

## 3. F1 Score 계산

- Candidate 캡션과 Reference 캡션 모두 장면 그래프를 구성한 뒤, 서로 일치하는 **튜플**의 수를 계산하여 정밀도(Precision, $P$)와 재현율(Recall, $R$)을 구한다.
- Precision은 Candidate 캡션의 튜플 중 Reference 캡션과 일치하는 튜플의 비율을 의미하며, Recall은 Reference 캡션의 튜플 중 Candidate 캡션과 일치하는 튜플의 비율을 구하는 것이다.
- 마지막으로, SPICE Score는 **F1-Score**로 표현된다. F1-Score는 Precision과 Recall의 조화 평균을 의미한다.

![NS_8.png](https://github.com/user-attachments/assets/03ce6649-865a-4c18-b201-0e13be6a32f8){: width="500px"}

<br/><br/><br/>

**장점**:

- **의미적 일치에 중점**: 문장 구조와 의미적 유사성을 평가하여, 단순히 단어 일치뿐만 아니라 문장의 깊은 의미와 개념적 일치도 평가 가능.
- **의미적 구성 요소 평가**: 그래프 구조를 사용해 캡션에서 의미적 관계를 추출하고, 참조 문장과의 의미적 유사성을 평가.
- **복잡한 문장 평가**: 단순한 단어 매칭이 아닌 의미적 일치도를 평가하기 때문에 복잡한 문장에서도 효과적.
<br/><br/>

**단점**:

- **계산 비용이 큼**: 문장 의미를 그래프로 변환하고 이를 평가하는 과정에서 계산 비용이 크고, BLEU나 CIDEr에 비해 느림.
- **특정 언어에 제한적**: 특정 언어에 대해 학습된 모델을 사용하므로 학습되지 않은 언어에 대해서는 부정확함.
- **문장의 유창성(fluency), 자연스러움 평가 부족**: 문장의 **의미적 정확성**을 평가하는 데 중점을 두고 있기 때문에, 문장의 유창성(fluency)이나 **자연스러움**을 평가하는 데 한계가 있음. 즉문법적으로 부자연스러운 문장이라도 의미가 잘 전달되면 높은 점수를 줄 수 있음.
- **구문 문석기 성능에 의존**: 구문 분석기(Dependency Parser)의 정확도에 크게 의존함. 구문 분석이 정확하지 않으면 잘못된 **장면 그래프**가 생성되며, 이로 인해 SPICE 점수도 낮아짐.
<br/><br/><br/><br/><br/><br/>

---

#### Reference

[1] 딥러닝을 이용한 자연어 처리 입문: [https://wikidocs.net/31695](https://wikidocs.net/31695)<br/>
[2] CIDEr: Consensus-based Image Description Evaluation: [https://arxiv.org/abs/1411.5726](https://arxiv.org/abs/1411.5726)<br/>
[3] SPICE: Semantic Propositional Image Caption Evaluation: [https://arxiv.org/abs/1607.08822](https://arxiv.org/abs/1607.08822)<br/>
