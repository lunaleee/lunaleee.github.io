---
title: "[ë…¼ë¬¸ ë¦¬ë·°] GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models"
author: lunalee
date: 2024-03-27 22:40:20 +0900
categories: [AI, Paper Review]
tags: [Image, Generation, Diffusion, Text-conditional]
pin: false
math: true
---

<br/><br/>
`Open AI` `PMLR 2022`

- Paper: [https://arxiv.org/abs/2112.10741](https://arxiv.org/abs/2112.10741)
- Git: [https://github.com/openai/glide-text2im](https://github.com/openai/glide-text2im)
<br/><br/><br/><br/><br/>

# Introduction

---

ì‚¬ì§„ì´ë‚˜ ê·¸ë¦¼ê°™ì€ ì´ë¯¸ì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì‰½ê²Œ ì„¤ëª…í•  ìˆ˜ ìˆì§€ë§Œ, ì´ë¯¸ì§€ë¥¼ ë§Œë“œëŠ” ë°ëŠ” ì „ë¬¸ì ì¸ ê¸°ìˆ ê³¼ ë…¸ë™ì´ í•„ìš”í•˜ë‹¤. ë§Œì•½ ìì—°ì–´ë¥¼ í†µí•´ ì‚¬ì‹¤ì ì¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤ë©´, ì‰½ê³  í’ë¶€í•˜ê²Œ ì‹œê°ì  ì»¨í…ì¸ ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ìµœê·¼ text-conditional image modelì€ promptë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ë¯¸ìƒ ìœ ì‚¬í•œ  ì´ë¯¸ì§€ë¥¼ í•©ì„±í•  ìˆ˜ ìˆì§€ë§Œ text promptì˜ ëª¨ë“  ì¸¡ë©´ì„ í¬í•¨í•˜ëŠ” ì‚¬ì‹¤ì ì¸ ì´ë¯¸ì§€ ìƒì„±ì€ ì–´ë µë‹¤.

ë°˜ë©´ unconditional image modelì€ ì‚¬ì‹¤ì ì¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìœ¼ë©° ì‹¤ì œ ì´ë¯¸ì§€ì™€ êµ¬ë³„í•˜ê¸° í˜ë“¤ ì •ë„ë¡œ fidelityê°€ ë†’ë‹¤. ì´ëŸ¬í•œ ì—°êµ¬ë¡œ DIffusion modelì€ ìœ ë§í•œ Generative modelë¡œ ë“±ì¥í•˜ì—¬ SOTA í’ˆì§ˆì„ ë‹¬ì„±í–ˆë‹¤.
<br/><br/>

Class-conditional settingì—ì„œ photorealismì„ ìœ„í•´ ì´ì „ì˜ ì—°êµ¬ì—ì„  diffusion modelì— classifierì˜ labelì„ ì£¼ì–´ conditionì„ ì£¼ëŠ” **classifier guidance**ë¥¼ í†µí•´ diffusion modelì„ ê°•í™”í–ˆë‹¤. ê·¸ë¦¬ê³  ì´í›„ trained classifierë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ ë„ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•œ **classifier-free guidance**ê°€ ë“±ì¥í•˜ì˜€ë‹¤. 
<br/><br/>

![GLIDE_1.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/386f398a-d2e2-4f4f-808e-9cdeec774f8f){: width="800px"}
<br/><br/>

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ê¸°ì¡´ì˜ ì—°êµ¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ **text-conditional image synthesis ë¬¸ì œì— guided diffusionì„ ì ìš©**í•œë‹¤.  Text Encoderë¥¼ ì‚¬ìš©í•˜ì—¬ natural language descriptionì„ ì¡°ê±´ìœ¼ë¡œ í•˜ëŠ” diffusion modelì„ í•™ìŠµì‹œí‚¨ í›„, CLIP guidance/classifier-free guidance ë‘ text prompt ê°€ì´ë“œ ë°©ì‹ì— ëŒ€í•´ ë¹„êµí–ˆë‹¤. ì €ìëŠ” classifier-free guidance ë°©ì‹ìœ¼ë¡œ ìƒì„±ëœ ëª¨ë¸ì˜ ìƒ˜í”Œì´ ë” ì‚¬ì‹¤ì ì´ë©° ê´‘ë²”ìœ„í•œ ì§€ì‹ì„ ë°˜ì˜í•œë‹¤ëŠ” ì‚¬ì‹¤ì„ ë°œê²¬í–ˆë‹¤. 
<br/><br/>

ì œì•ˆí•œ ëª¨ë¸ì€ ë‹¤ì–‘í•œ text promptë¥¼ zero-shotìœ¼ë¡œ ë Œë”ë§ í•  ìˆ˜ ìˆì§€ë§Œ ë³µì¡í•œ promptì— ëŒ€í•´ì„œëŠ” ì‚¬ì‹¤ì ì¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ”ë° ì–´ë ¤ì›€ì´ ìˆë‹¤. ë”°ë¼ì„œ ì €ìëŠ” zero-shot generationì— ë”í•˜ì—¬, ë³µì¡í•œ promptì™€ ì¼ì¹˜í•  ë•Œê¹Œì§€ ì§ì ‘ ëª¨ë¸ì˜ ìƒ˜í”Œì„ iteratively ê°œì„ í•˜ëŠ” editing ê¸°ëŠ¥ì„ ì¶”ê°€í–ˆë‹¤.  ì´ëŸ¬í•œ ë°©ë²•ì„ í†µí•´ promptë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ ì´ë¯¸ì§€ë¥¼ ì‚¬ì‹¤ì ìœ¼ë¡œ í¸ì§‘í•  ìˆ˜ ìˆê³ , ì¸ê°„ì´ ì „ë¡€ ì—†ëŠ” ì†ë„ì™€ í¸ë¦¬í•¨ìœ¼ë¡œ ë§ì¶¤í˜• ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆë‹¤.

ì €ìëŠ” ë³¸ ë…¼ë¬¸ì˜ ë°©ë²•ì„ Guided Language to Image Diffusion for Generation and Editing, ì¤„ì—¬ì„œ GLIDE ë¼ê³  ì •ì˜í–ˆë‹¤.
<br/><br/><br/><br/><br/><br/>

# Background

---

## 1. Diffusion models

ë¨¼ì €, ë…¼ë¬¸ì—ì„œëŠ” diffusionì˜ ê¸°ë³¸ processì— ëŒ€í•œ ì„¤ëª…í•˜ê³  ìˆë‹¤. í•´ë‹¹ ë¶€ë¶„ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ë¸”ë¡œê·¸ ë‚´ì˜ ê²Œì‹œë¬¼ [[Generative model ê¸°ì´ˆ 3. Diffusion ì •ë¦¬](https://lunaleee.github.io/posts/Diffusion/)]ë¥¼ ì°¸ì¡°í•´ë³´ì.
<br/><br/>

Diffusionì€  í¬ê²Œ ë‘ ë‹¨ê³„ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤. Forward processì—ì„œ $x_0 âˆ¼ q(x_0)$ì˜ ìƒ˜í”Œì— ëŒ€í•´ Gaussian noiseë¥¼ ì ì§„ì ìœ¼ë¡œ ì¶”ê°€í•˜ì—¬ latent variable $x_1, ..., x_T$ì˜ Markov chainì„ ìƒì„±í•˜ê³ , Reverse processì—ì„œ noiseë¥¼ ì ì§„ì ìœ¼ë¡œ ì œê±°í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•´ ì‹¤ì œ posteriorë¥¼ ê·¼ì‚¬í•˜ê¸° ìœ„í•œ ëª¨ë¸ $p_Î¸(x_{tâˆ’1}âˆ£x_t)$ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ ë•Œ, Gaussian noise $Îµ$ì„ $x_0$ì— ì ìš©í•˜ì—¬ ìƒ˜í”Œ $x_t âˆ¼ q(x_tâˆ£x_0)$ë¥¼ ìƒì„±í•œ ë‹¤ìŒ mean-squared error lossì„ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€ëœ noiseì„ ì˜ˆì¸¡í•˜ë„ë¡ ëª¨ë¸ $Îµ_Î¸$ë¥¼ í•™ìŠµí•œë‹¤.
<br/>

![GLIDE_2.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/fb013c5b-1d7a-41bd-bef1-6ff67d0242a0){: width="1300px"}
<br/><br/><br/><br/><br/>

## 2. Guided Diffusion

[Diffusion models beat gans on image synthesisğŸ“„](https://arxiv.org/abs/2105.05233) ë…¼ë¬¸ì—ì„œëŠ” class-conditional diffusion modelì´ classifier guidanceë¥¼ í†µí•´ í–¥ìƒë˜ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤.

í‰ê·  $Î¼_Î¸(x_tâˆ£y)$ê³¼ ë¶„ì‚° $Î£_Î¸(x_tâˆ£y)$ì„ ê°–ëŠ” class-conditional diffusion modelì€ classifierì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê°€ì§€ê³  ì¡°ê±´ì„ ë¶€ì—¬ë°›ëŠ”ë‹¤. ì •í™•íˆëŠ” classifierì—ì„œ ì˜ˆì¸¡ëœ target class $y$ì˜ log-probability $\log p_\phi(yâˆ£x_t)$ì˜ gradientê°€ ì „ë‹¬ë˜ë©´ì„œ ì˜í–¥(êµë€)ì„ ë°›ëŠ”ë‹¤. ì¡°ê±´ì„ ë¶€ì—¬í•˜ê²Œ ëœë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ìƒì„±ëœ ìƒˆë¡œìš´ perturbed mean(êµë€ í‰ê· ) $\hat{Î¼}_Î¸(x_tâˆ£y)$ì€ ì•„ë˜ì™€ ê°™ì´ ì •ì˜ëœë‹¤. 

$$
\hat{\mu}_\theta = \mu_\theta(x_tâˆ£y) + s \; \cdot \; Î£_\theta(x_tâˆ£y) \; \nabla_{x_t} \log p_\phi (yâˆ£x_t)
$$

<br/>

Coefficient sëŠ” guidance scaleë¡œ, së¥¼ ë†’ì´ë©´ diversityê°€ í¬ìƒë˜ì§€ë§Œ sample qualityê°€ í–¥ìƒëœë‹¤.
<br/><br/><br/><br/><br/>

## 3. Classifier-free guidance

[Classifier-free diffusion guidanceğŸ“„](https://arxiv.org/abs/2207.12598) ë…¼ë¬¸ì—ì„œëŠ” ë³„ë„ì˜ classifier modelì„ í•™ìŠµí•  í•„ìš”ê°€ ì—†ì´ diffusion modelì— guideë¥¼ ì£¼ëŠ” classifier-free guidanceë¥¼ ì œì•ˆí–ˆë‹¤. 

Classifier-free guidanceë¥¼ ìœ„í•´ class-conditional diffusion model $Îµ_Î¸(x_tâˆ£y)$ì˜ label $y$ëŠ” í•™ìŠµ ì¤‘ì— ê³ ì •ëœ í™•ë¥ ë¡œ null label $âˆ…$ë¡œ ëŒ€ì²´ëœë‹¤. ì´ë ‡ê²Œ **label(condition)ì„ ë°›ì€ ê²½ìš°** $Îµ_Î¸(x_tâˆ£y)$, l**abel(condition)ì„ ë°›ì§€ ì•Šì€ ê²½ìš°** $Îµ_Î¸(x_tâˆ£âˆ…)$ ë‘ ë°©ë²•ì— ëŒ€í•´ í•™ìŠµì„ ì§„í–‰í•˜ê²Œ ë˜ê³ , sampling ì¤‘ ëª¨ë¸ì˜ ì¶œë ¥ì€ ì•„ë˜ì˜ ì‹ê³¼ ê°™ì´ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>ë‘ ë°©ë²•ì— ëŒ€í•´ extrapolate</span></mark>** ëœë‹¤. ($Îµ_Î¸(x_tâˆ£y)$ ë°©í–¥ìœ¼ë¡œ)
<br/><br/>

$$
\hat{\epsilon}_\theta(x_tâˆ£y) = \epsilon_\theta(x_tâˆ£âˆ…) + s \; \cdot \;(\epsilon_\theta(x_tâˆ£y) - \epsilon_\theta(x_tâˆ£âˆ…))
$$

<br/>

ìœ„ ë°©ë²•ì€ implicit classifierë¥¼ ê°€ì •í•˜ëŠ” ê²ƒì—ì„œ ì‹œì‘ë˜ì—ˆë‹¤. ì•„ë˜ (1)ê³¼ ê°™ì€ classifierê°€ ìˆë‹¤ê³  í•  ë•Œ, classifierì˜ gradientëŠ” true scores $\epsilon^âˆ—$ì˜ ê´€ì ì—ì„œ (2)ì™€ ê°™ì´ ì •ì˜ëœë‹¤.  

$$
\quad p^i(yâˆ£x_t) \propto \frac{p(x_tâˆ£y)}{p(x_t)}    \quad\qquad(1)
$$

$$
\quad \nabla_{x_t} \log p^i(x_tâˆ£y) \propto \nabla_{x_t} \log p(x_tâˆ£y) - \nabla_{x_t} \log p(x_t) \quad\qquad (2)
$$

$$
\propto \epsilon^*(x_tâˆ£y) - \epsilon^*(x_t) \qquad
$$

<br/>

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì¼ë°˜ text promptë¡œ classifier-free guidanceë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ í•™ìŠµ ì¤‘ ì¼ë¶€ë¶„ì˜ ê²½ìš°ì— text captionì„ ë¹ˆ ì‹œí€€ìŠ¤($âˆ…$)ë¡œ ëŒ€ì²´í•œë‹¤. ìˆ˜ì •ëœ prediction $\hat{\epsilon}$ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\hat{\epsilon}_\theta(x_tâˆ£c) = \epsilon_\theta(x_tâˆ£âˆ…) + s \; \cdot \;(\epsilon_\theta(x_tâˆ£c) - \epsilon_\theta(x_tâˆ£âˆ…))
$$

<br/>

Classifier-free guidanceëŠ” ë‘ê°€ì§€ ì¥ì ì´ ìˆë‹¤. 

1. ë³„ë„ì˜ classification modelì˜ knowledgeì— ì˜ì¡´í•˜ì§€ ì•Šê³  ëª¨ë¸ì´ ìì²´ì ì¸ knowledgeë¥¼ í™œìš©í•  ìˆ˜ ìˆë‹¤.
2.  Classifierë¡œ ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ìš´ ì •ë³´(ì˜ˆ: Text)ì— ëŒ€í•œ Conditionì„ ì§€ì •í•  ë•Œ guidanceë¥¼ ë‹¨ìˆœí™”í•œë‹¤.
<br/><br/><br/><br/><br/>

## 4. CLIP Guidance

CLIP ëª¨ë¸ì€ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê°„ì˜ joint representationì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ, Image Encoder $f(x)$ì™€ Caption Encoder $g(c)$ë¼ëŠ” ë‘ ê°œì˜ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. í•™ìŠµ ì¤‘ $(x, c)$ pairê°€ ì£¼ì–´ì§€ë©´ ì˜¬ë°”ë¥¸ pairì— ëŒ€í•´ ë‚´ì  $f(x) Â· g(c)$ì´ ë†’ì•„ì§€ë„ë¡ contrastive cross-entropy lossë¥¼ ìµœì†Œí™”í•œë‹¤. CLIPì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ë¸”ë¡œê·¸ì˜ ê²Œì‹œë¬¼ [[ë…¼ë¬¸ ë¦¬ë·°: CLIP](https://lunaleee.github.io/posts/CLIP/)]ì„ ì°¸ì¡°í•´ë³´ì.
<br/><br/>

CLIPì€ ì´ë¯¸ì§€ê°€ ìº¡ì…˜ê³¼ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ì§€ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ì œê³µí•˜ë¯€ë¡œ GANê³¼ ê°™ì€ ì—¬ëŸ¬ ì—°êµ¬ì—ì„œ ì´ë¥¼ í™œìš©í•˜ì˜€ë‹¤. ë™ì¼í•œ ê°œë…ì„ diffusion modelì— ì ìš©í•˜ê¸° ìœ„í•´ classifier guidanceì—ì„œ classifierë¥¼ CLIPìœ¼ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆë‹¤. [2. Guided Diffusion](#2-guided-diffusion)ì—ì„œ ì–¸ê¸‰í•œëŒ€ë¡œ, ì´ë¯¸ì§€ì— ëŒ€í•œ ë‚´ì (ì´ë¯¸ì§€ $\cdot$ ìº¡ì…˜) gradientë¥¼ reverse-processì— ì „ë‹¬í•˜ì—¬ reverse-process meanì— ì˜í–¥(êµë€)ì„ ì¤€ë‹¤.

$$
\hat\mu_\theta(x_tâˆ£c) = \mu_\theta(x_tâˆ£c) + s\; \cdot \; Î£_\theta(x_tâˆ£c) \nabla_{x_t}(f(x_t)\;\cdot\;g(c))
$$

<br/>

Classifier guidanceì™€ ìœ ì‚¬í•˜ê²Œ reverse processì—ì„œ ì˜¬ë°”ë¥¸ gradientë¥¼ ì–»ìœ¼ë ¤ë©´ noiseê°€ ìˆëŠ” ì´ë¯¸ì§€ $x_t$ì— ëŒ€í•´ CLIPì„ í•™ìŠµí•´ì•¼í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì‹¤í—˜ ì „ë°˜ì— ê±¸ì³ noiseë¥¼ ì¸ì‹í•˜ë„ë¡ í•™ìŠµëœ CLIP modelì„ ì‚¬ìš©í•˜ë©°, ì´ë¥¼ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>noised CLIP model</span></mark>**ì´ë¼ê³  í•œë‹¤.
<br/><br/><br/>

ì´ì „ì˜ ì—°êµ¬ì—ì„œëŠ” noiseê°€ ìˆëŠ” ì´ë¯¸ì§€ì— ëŒ€í•´ í•™ìŠµë˜ì§€ ì•Šì€ CLIP modelë„ diffusion modelì„ guideí•˜ëŠ”ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì§€ë§Œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” noised CLIP guidanceê°€ data augmentationì´ë‚˜ perceptual loss ê°™ì€ ì¶”ê°€ ë°©ë²• ì—†ì´ ìœ ë¦¬í•˜ê²Œ í•™ìŠµëœë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•œë‹¤. 

ì €ìëŠ” noiseê°€ ìˆëŠ” ì´ë¯¸ì§€ì— ëŒ€í•´ í•™ìŠµë˜ì§€ ì•Šì€ CLIPì„ ì‚¬ìš©í•˜ì—¬ guideí•˜ëŠ” ê²ƒì€ sampling ì¤‘ì˜ noised intermediate imageê°€ ëª¨ë¸ì— ëŒ€í•´ out-of-distributionì´ê¸° ë•Œë¬¸ì— sample qualityì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ê°€ì„¤ì„ ì„¸ì› ë‹¤.
<br/><br/><br/><br/><br/><br/>

# Training

---

64 Ã— 64 resolution ì´ë¯¸ì§€ì— ëŒ€í•´ 3.5B ë§¤ê°œë³€ìˆ˜ text-conditional diffusion modelì„ í•™ìŠµí•˜ê³ , resolutionì„ 256 Ã— 256 ìœ¼ë¡œ ë†’ì´ê¸° ìœ„í•´ ì¶”ê°€ì ì¸ 1.5B ë§¤ê°œë³€ìˆ˜ text-conditional upsampling diffusion modelì„ í•™ìŠµí–ˆë‹¤. CLIP guidanceë¥¼ ìœ„í•´ noised 64 Ã— 64 ViT-L CLIP modelì„ í•™ìŠµí–ˆë‹¤.

![GLIDE_3.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/0e6467d2-f3dc-4a15-ab66-20f072b33188){: width="700px"}
<br/><br/><br/><br/>

## 1. Text-Conditional Diffusion Models

ì €ìëŠ” [Diffusion models beat gans on image synthesisğŸ“„](https://arxiv.org/abs/2105.05233)ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ADM modelì„ ì‚¬ìš©í•˜ì§€ë§Œ text conditioningì„ ì¶”ê°€í•˜ì˜€ë‹¤. 
<br/><br/>

ê° <mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>noise image $x_t$ì™€ í•´ë‹¹ text caption $c$ì— ëŒ€í•´ ëª¨ë¸ì€ $p(x_{tâˆ’1}âˆ£x_t,c)$ë¥¼ ì˜ˆì¸¡</span></mark>í•œë‹¤. Textë¥¼ conditionìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ê¸° ìœ„í•´ $K$ token sequenceë¡œ encodingí•˜ê³ , ì´ tokenì„ Transformer modelì˜ ì…ë ¥ìœ¼ë¡œ ë„£ëŠ”ë‹¤. Transformerì˜ outputì€ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.

1. ADM ëª¨ë¸ì˜ class embedding ëŒ€ì‹  final token embeddinì´ ì‚¬ìš©ëœë‹¤.
2. Token embeddingì˜ ë§ˆì§€ë§‰ layer(sequence of $K$ feature vectors)ëŠ” ADM modelì˜ ê°ê°ì˜ attention layerì˜ ì°¨ì›ì— ë§ê²Œ ë³„ë„ë¡œ project ëœë‹¤. ê·¸ ë‹¤ìŒ ê° layerì˜ attention contextì— concat ëœë‹¤.
<br/><br/><br/>

ì €ìëŠ” DALL-E[[ë…¼ë¬¸ë¦¬ë·°: DALL-E](https://lunaleee.github.io/posts/DALL-E/)]ì™€ ë™ì¼í•œ ë°ì´í„°ì…‹ì—ì„œ í•™ìŠµí–ˆìœ¼ë©°, ADMì˜ ImageNet 64 Ã— 64 ëª¨ë¸ê³¼ ë™ì¼í•œ model êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ model widthë¥¼ 512 channelë¡œ í™•ì¥í•˜ì—¬ ì•½ 2.3B ë§¤ê°œë³€ìˆ˜ê°€ ìƒì„±ëœë‹¤(visual partì—ë§Œ). Text Encoding Transformerì˜ ê²½ìš° 1.2B ë§¤ê°œë³€ìˆ˜ê°€ ìƒì„±ëœë‹¤. ë˜í•œ 64 Ã— 64ì—ì„œ 256 Ã— 256ë¡œ reolustionì„ ì˜¬ë¦¬ê¸° ìœ„í•œ 1.5 ë§¤ê°œë³€ìˆ˜ Upsampling diffusion modelì„ í•™ìŠµí–ˆë‹¤.
<br/><br/><br/><br/><br/>

## 2. Fine-tuning for classifier-free guidance

Initial training í›„ unconditional image generationì„ ìœ„í•´ ëª¨ë¸ì„ fine-tuning í–ˆë‹¤. Fine-tuning ê³¼ì •ì€ text token sequenceì˜ 20%ê°€ empty sequenceë¡œ ëŒ€ì²´ëœë‹¤ëŠ” ì ë§Œ ì œì™¸í•˜ê³ ëŠ” pre-trainingê³¼ ë™ì¼í•˜ë‹¤. í•´ë‹¹ ë°©ë²•ì„ í†µí•´ text-conditional outputì„ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥ì„ ìœ ì§€í•˜ë˜, unconditional í•˜ê²Œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ë„ ìˆë‹¤.
<br/><br/><br/><br/><br/>

## 3. Image Inpainting

ê¸°ì¡´ì˜ Diffusion modelì„ ì‚¬ìš©í•˜ëŠ” ì—°êµ¬ì—ì„œëŠ” Inpaintingì„ ìœ„í•´ ë”°ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ì§€ ì•Šì•˜ë‹¤. Diffusion modelì—ì„œ inpaintingì„ ìœ„í•´ ê° sampling ë‹¨ê³„ ì´í›„ì— ì´ë¯¸ì§€ì˜ ì•Œë ¤ì§„ ë¶€ë¶„ì„ $q(x_tâˆ£x_0)$(noise)ë¡œ ëŒ€ì²´í•œë‹¤. ì´ì™¸ì˜ ë¶€ë¶„ì€ ë‹¤ë¥¸ ì‘ì—…ê³¼ ìœ ì‚¬í•˜ê²Œ ì§„í–‰ëœë‹¤. ì´ ë°©ë²•ì€ ëª¨ë¸ì´ sampling process ë™ì•ˆ ì „ì²´ contextë¥¼ ë³¼ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—(noised ë¶€ë¶„ë§Œ ë³´ê²Œ ë˜ë¯€ë¡œ) ì‹¤í—˜ ì´ˆê¸° ë‹¨ê³„ì—ì„œ ê°€ì¥ìë¦¬ì— artifactsê°€ ë°œìƒí•œë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. 
<br/><br/>

ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ Inpaintingì„ ìœ„í•œ fine-tuningì„ ì§„í–‰í•œë‹¤. Fine-tining ì¤‘ì— training exampleì˜ random ì˜ì—­ì´ ì§€ì›Œì§€ê³  ë‚˜ë¨¸ì§€ ë¶€ë¶„ì„ mask channel(additional conditioning)ìœ¼ë¡œ ëª¨ë¸ì— ê³µê¸‰í•œë‹¤. ì¦‰, RGB 3 channelì— 4ê°œì˜ ì¶”ê°€ ì…ë ¥ channelì„ ê°–ë„ë¡ ëª¨ë¸ êµ¬ì¡°ë¥¼ ìˆ˜ì •í•œë‹¤. fine-tuning ì „ì—ëŠ” ìƒˆë¡œìš´ channelì— í•´ë‹¹í•˜ëŠ” weightì„ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•œë‹¤.

Upsampling modelì˜ ê²½ìš°, ì›ë˜ëŠ” full  low-resolution imageë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ì§€ë§Œ, inpainingì˜ ê²½ìš°ì— unmasked regionì— ëŒ€í•´ì„œëŠ” high-resolution imageë¥¼ ë„£ëŠ”ë‹¤.
<br/><br/><br/><br/><br/>

## 4. Noised CLIP models

Classifier guidance techniqueì„ ë” ì˜ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•´ noised images $x_t$ë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ëŠ” Image Encoder $f(x_t, t)$ë¥¼ ì‚¬ìš©í•˜ì—¬ noised CLIP modelì„ í•™ìŠµí•œë‹¤. Base modelê³¼ ë™ì¼í•œ noise scheduleì„ ì‚¬ìš©í•˜ì—¬ 64 Ã— 64 resolutionìœ¼ë¡œ í•™ìŠµí–ˆë‹¤.
<br/><br/><br/><br/><br/><br/>

# Experiments

---

### 1. Qualitative Results

ì•„ë˜ ê·¸ë¦¼ì€ CLIP guidanceì™€ classifier-free guidanceë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œ ê²°ê³¼ì— ëŒ€í•´ ë¹„êµí•˜ê³  ìˆë‹¤. ì‹œê°ì ìœ¼ë¡œ ë¹„êµí•  ë•Œ CLIP guidanceë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ë³´ë‹¤ classifier-free guidanceë¡œ ìƒì„±í•œ ì´ë¯¸ì§€ê°€ realistic í•´ë³´ì´ëŠ” ê²½ìš°ê°€ ë§ë‹¤.

ë˜í•œ MS-COCO ë°ì´í„°ì…‹ì—ì„œ ì´ì „ì˜ SOTA text-conditional image generation modelê³¼ ë¹„êµí•˜ì˜€ì„ ë•Œ GLIDEê°€  cherry-picking ì—†ì´ë„ ë” realisticí•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

![GLIDE_4.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/0c5594b1-4d02-4830-9282-ece09dbc8c7f){: width="500px"}
<br/><br/><br/><br/>

ì•„ë˜ì˜ ê·¸ë¦¼ì€ classifier-free guidance GLIDEê°€ ë‹¤ì–‘í•œ promptì— ëŒ€í•´ generalizing í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤.

![GLIDE_5.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/2bf507c8-2021-428c-8f86-5c16b1dc9fc8){: width="900px"}
<br/><br/><br/><br/>

ì €ìëŠ” inpatinting ì‘ì—…ì—ì„œ GLIDEê°€ Text promptë¥¼ í™œìš©í•˜ì—¬ ê°ì²´, ê·¸ë¦¼ì ë° ë°˜ì‚¬ë¥¼ ì‚½ì…í•˜ì—¬ ê¸°ì¡´ ì´ë¯¸ì§€ë¥¼ í˜„ì‹¤ì ìœ¼ë¡œ ìˆ˜ì •í•  ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆë‹¤. 

![GLIDE_6.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/31ef2522-040f-44ce-aef4-ee1c34db4762){: width="600px"}
<br/><br/><br/><br/><br/>

### 2. Quantitative Results

ë¨¼ì € **quality-fidelity trade-off**ì˜ Pareto frontierë¥¼ ì‚´í´ë´„ìœ¼ë¡œì¨ CLIP guidanceì™€ classifier-free guidanceì˜ ì°¨ì´ë¥¼ í‰ê°€í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ 64Ã—64 resolutionì—ì„œ zero-shot MS-COCO generationì— ëŒ€í•œ ë‘ ê°€ì§€ ë°©ì‹ì„ ëª¨ë‘ í‰ê°€í•œ ê²°ê³¼ì´ë‹¤. Precision/Recallê³¼ IS/FID, CLIP score/FIDì— ëŒ€í•´ ì¡°ì‚¬í–ˆë‹¤. 

![GLIDE_7.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/8331aa5c-3929-47a1-b179-e76a118fe3bf){: width="1200px"}
<br/><br/>

ì•ì˜ ë‘ curveì˜ ê²½ìš°ì—ëŠ” classifier-free guidanceê°€ ê±°ì˜ Pareto optimalì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆëŠ”ë°, ë§ˆì§€ë§‰ curveëŠ” **ë°˜ëŒ€ì˜ ê²½í–¥**ì„ ë³¼ ìˆ˜ ìˆë‹¤. íŠ¹íˆ CLIP guidanceëŠ” classifier-free guidanceë³´ë‹¤ CLIP scoreë¥¼ boost í•  ìˆ˜ ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.

ì €ìëŠ” ì´ì— ëŒ€í•´ CLIP guidanceê°€ prompt ì¼ì¹˜ì™€ ê´€ë ¨í•˜ì—¬ classifier-free guidanceë³´ë‹¤ ì‹¤ì œë¡œ ë›°ì–´ë‚œ ê²ƒì´ ì•„ë‹ˆë¼ evaluation **CLIP modelì— ëŒ€í•œ adversarial exampleì„ ì°¾ëŠ”ë‹¤ëŠ” ê°€ì„¤**ì„ ì„¸ì› ë‹¤. ì´ ê°€ì„¤ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ **human evaluator**ë¥¼ ê³ ìš©í•˜ì—¬ ìƒì„±ëœ ì´ë¯¸ì§€ì˜ í’ˆì§ˆì— ëŒ€í•´ í‰ê°€í–ˆë‹¤. Human evaluatorëŠ” 1) ì£¼ì–´ì§„ captionê³¼ ë” ì˜ ì¼ì¹˜í•˜ê±°ë‚˜ 2) ë” ì‚¬ì‹¤ì ìœ¼ë¡œ ë³´ì´ëŠ” ìƒ˜í”Œì„ ì„ íƒí•´ì•¼ í•œë‹¤.
<br/><br/><br/>

human evaluation protocolì„ ì‚¬ìš©í•˜ì—¬ ë‘ ì ‘ê·¼ ë°©ì‹ì— ëŒ€í•œ guidance scaleì„ ê²€í† í–ˆë‹¤. ê²°ê³¼ëŠ” ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ë‹¤. ì €ìëŠ” ì¸ê°„ì´ CLIP scoreì— ë™ì˜í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤.

![GLIDE_8.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/ca9a8018-7d30-417c-8194-327602b4e528){: width="450px"}
<br/><br/><br/>

ë˜í•œ ë‘ ê°€ì§€ ë°©ë²•ì„ ì´ì „ ë‹¨ê³„ì˜ best scaleê³¼ ë¹„êµí–ˆë‹¤.

![GLIDE_9.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/4562b3d1-9e4d-4955-9c36-3c9e12090396){: width="500px"}
<br/><br/><br/>

GLIDEë¥¼ ë‹¤ë¥¸ text-conditional generative image modelê³¼ ë¹„êµí–ˆë‹¤. ê²°ê³¼ëŠ” ì•„ë˜ í‘œì™€ ê°™ë‹¤.

![GLIDE_10.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/6f11dc85-53c2-4505-9d21-dffbe3208675){: width="500px"}
<br/><br/><br/>

ë§ˆì§€ë§‰ìœ¼ë¡œ human evaluation protocolì„ ì‚¬ìš©í•˜ì—¬ GLIDEì™€ DALL-Eë¥¼ ë¹„êµí–ˆë‹¤. GLIDEëŠ” DALL-Eì™€ ê±°ì˜ ë™ì¼í•œ training computingì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµë˜ì—ˆì§€ë§Œ í›¨ì”¬ ë” ì‘ì€ ëª¨ë¸(3.5B vs. 12B)ì„ ì‚¬ìš©í–ˆë‹¤. 

![GLIDE_11.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/0a953a0f-20d6-45c9-bf83-1f5e59d7174d){: width="500px"}
<br/><br/><br/><br/><br/>
