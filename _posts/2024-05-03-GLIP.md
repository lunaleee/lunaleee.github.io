---
title: "[논문 리뷰] (GLIP) Grounded Language-Image Pre-training"
author: lunalee
date: 2024-05-03 19:23:12 +0900
categories: [AI, Paper Review]
tags: [Multi-modal, Detection, Knowledge Distillation]
pin: false
math: true
---

<br/><br/>
`Miscrosoft Research` `CVPR 2022`

- Paper: [https://arxiv.org/abs/2112.03857](https://arxiv.org/abs/2112.03857)
- Git: [https://github.com/microsoft/GLIP](https://github.com/microsoft/GLIP)
<br/><br/><br/>

#### 📖 핵심 훑어보기 !!

- Image-Text pair, 즉 **Multi-modal pre-training 개념**을 Object detection task로 확장
- 확장을 위해 Object detection을 **Phrase grounding과 통합**함. Detection의 bbox를 $c$ classes로 분류하는 대신 region-word aligment scores를 계산함!
- CLIP 처럼 마지막 layer에서 융합하여 text-image alignment score를 계산하는 late fusion 방식이 아닌, 마지막 여러개 layer에서 융합하는 **Deep-fusion 구조** 사용
- 기존의 **self-training 방법**(knowledge distillation)을 확장하여 큰 규모의 데이터셋에 대해 학습함
<br/><br/><br/><br/>

# Introduction

---

CLIP은 대량의 image-text pair에서 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>image-level representation</span></mark>**을 효과적으로 학습하였다. Image-text pair를 통해 미리 정해 둔 category가 아닌 풍부한 visual concept에 대해 학습하는 **zero-shot setting**으로 downstream task로 쉽게 적용될 수 있다. 하지만  object detection, segmentation등의 task에서는 image-level representation이 아닌 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>object-level visual representation</span></mark>**이 필요한 문제가 있다. 
<br/><br/>

본 논문에서는 **문장 내 문구**와 **image 내 object(또는 region)** 사이의 correspondence을 식별하는 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>phrase grounding</span></mark>**이 object-level 학습을 위한 효과적인 작업이라는 것을 증명한다. 또한 language-aware 및 semantic-rich visual representation을 제공하는 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>GLIP(Grounded Language-Image Pre-training)</span></mark>**을 제안한다.

> **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>Phrase Grounding</span></mark>** (Visual Grounding): 이미지와 해당 이미지에 대한 caption이 주어졌을 때, caption의 명사구가 언급한 실체를 이미지의 region에서 표시하는 것을 목표로 하는 것. 즉, 이미지의 어느 부분을 묘사하였는지 bounding box로 표시하는 것을 의미
>

![GLIP_1.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/7adcb46e-6075-4a50-b744-1a7f273a7b94){: width="600px"}
<br/><br/><br/>

주요한 contribution은 아래와 같다.
<br/><br/>

**1) Unifying detection and grounding by reformulating object detection as phrase grounding.**

![GLIP_2.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/f3a4df9c-78c0-485a-a760-330cbaa52870){: width="1000px"}

Object detection을 phrase grounding으로 reformulation하기 위해 detection 모델의 입력으로 이미지 뿐 아니라 **prompt 입력**을 추가한다. Prompt는 이미지 내에서 **모든 후보 category**를 의미한다. 예를 들면 COCO object detection을 위한 prompt는 위의 그림(왼쪽)과 같이 object class를 “.”으로 구분해서 나열한 문자열이다. 
<br/><br/>

모든 object detection 모델에서 box classifier의 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>object classification logit을 word-region alignment score로 대체하여줌으로서 grounding 모델로 변환</span></mark>**할 수 있다. 즉, 위 그림의 오른쪽과 같이 **region (or box) visual feature와 token (or phrase) language feature를 dot product**하여 word-region alignment score를 구한다. 

마지막 dot product layer에서만 vision과 language를 융합하는 CLIP과 달리 GLIP은 위의 그림과 같이 **deep cross-modality fusion**을 적용한다. Detection 및 grounding의 통합을 통해 두 가지 유형의 데이터를 모두 사용하여 pre-train 할 수 있으며 두 task 모두에 이점을 얻을 수 있다.
<br/><br/><br/><br/>

**2) Scaling up visual concepts with massive image-text data.**

좋은 grounding 모델(teacher)이 주어지면 NLP parser에 의해 명사구를 감지해서 grounding box를 자동으로 생성하고, 대규모 image-text-paired 데이터를 생성하여 GLIP pre-training 데이터로 사용할 수 있다. 논문에서는 27M개의 grounding data를 사용하여 (student) GLIP-Large를 pre-train 했다.

![GLIP_3.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/da85b8a5-d702-4fc5-bc06-1ce5773119c3){: width="600px"}<br/>
위의 그림은 teacher 모델에 의해 생성된 generated box에 대한 예제이다. 위와 같이 semantic-rich한 데이터를 사용하여 student 모델에서도 semantic-rich한 모델을 학습할 수 있다. Grounding data를 확장하는 간단한 전략이 경험적으로 효과적이며 특히 downstream task의 성능을 개선한다는 것을 증명한다.
<br/><br/><br/><br/>

**3) Transfer learning with GLIP: one model for all.**

Semantic-rich한 pre-training은 **domain transfer에 유리**하여 추가적인 human annotation 없이도 다양한 task로 transfer할 수 있다. GLIP-L 모델은  COCO, LVIS 데이터셋에서 많은 supervised baseline을 능가했다. 

Task-specific annotation을 사용할 수 있는 경우 전체 모델을 tuning하는 대신 **task-specific prompt embedding만 tuning**하여 사용할 수 있다. 이러한 방법을 통해 하나의 GLIP 모델은 모든 downstream task에서 동시에 잘 수행되어 fine-tuning 및 배포 비용을 줄일 수 있다.
<br/><br/><br/><br/><br/><br/>

# Grounded Language Image Pre-training

---

## 1. Unified Formulation

해당 section에서는 기존의 Object detection을 grounding problem으로 전환하고 통일하는 방법에 대한 내용을 담고 있다. 
<br/><br/>

#### Background: object detection.

먼저, Object detection에 대해 간단히 짚어보자. 일반적인 detection 모델은 아래와 같이 Visual Encoder $\text{Enc}_I$, Box Classifier $C$, Box Regressor $R$로 구성되어 있다. 

![GLIP_4.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/add07d5b-a1ab-4af6-8da4-069638f73f3a){: width="900px"}
<br/>

여기서 Box Classifier는 일반적으로 간단한 linear layer로 구성되고, classification loss 는 다음과 같다.

![GLIP_5.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/14ef48a0-e71f-4432-826f-1875c4027104){: width="700px"}

(2)번 수식에서 $T \in \lbrace0, 1\rbrace^{N \times c}$ 는 region과 class간의 **target matching**이고, $loss(S;T)$는 일반적으로 two-stage detector의 경우 cross-entropy loss이고 one-stage detector의 경우 focal loss 이다.
<br/><br/>

> $T \in \lbrace0,1\rbrace^{N \times c}$ 는 region과 class간의 **target matching**이다. $T$는 고전적인 many-to-1 matching 또는 bipartite Hungarian match 방법을 사용하여 계산되는 것을 말한다. 기본적인 Object Detection의 방법은 아래와 같은 순서로 진행된다.
>
> 1. **Region Proposal**: RPN을 사용하여 이미지에서 잠재적인 객체가 존재할 수 있는 여러 영역을 제안
> 2. **Feature Extraction**: 각 제안된 영역에서 특징을 추출
> 3. **Classification and Scoring**: 각 영역의 특징을 기반으로 classification head를 통해 해당 영역이 특정 클래스(예: 사람, 자동차, 동물 등)에 속할 확률을 계산
> 4. **Matching**: Hungarian 등의 알고리즘을 사용하여 각 영역을 최적의 클래스에 매칭. 이때 매칭 비용은 영역과 클래스 간의 유사도 점수에 기반함
>
> * Matching 과정은 training과 inference 모두에서 중요한 역할을 한다. Training 시에는 모델이 정확하게 학습되도록 도와주며, inference 시에는 최적의 예측 결과를 도출하는 데 사용된다.
>

<br/><br/>

#### Object detection as phrase grounding.

이제 detection을 reformulate하여 grounding task로 만들어보자. 각각 region/box를 $c$ classes로 분류하는 대신 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>각각의 region을 text prompt의 $c$ phrase에 grounding/aligning</span></mark>** 한다.
<br/><br/>

Detection을 위한 text prompt는 가장 쉽게는 ground 되는 **후보 문구를 나열**하는 방식이다.
<br/>

$$
\text{Prompt =  “Detect: person, bicycle, car, ... , toothbrush”}
$$

Prompt는 다양한 방법으로 디자인 할 수 있지만, BERT 모델을 사용하여 language encoder $\text{Enc}_L$을 초기화하는 경우는 사람에게 친숙한 문장형 prompt 보다 위와 같은 단어 나열형이 더 좋은 성능을 보였다(뒤에서 다시 언급).
<br/><br/>

Grounding 모델은 image region과 단어 사이의 alignment score $S_{\text{ground}}$를 계산한다.

![GLIP_6.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/7da518eb-34e4-4e70-a289-8f85a557e75f){: width="600px"}<br/>
P는 language encoder의 contextual word/token feature이며 (2)의 weight matrix W와 유사한 역할을 한다.

Grounding 모델은 image encoder $\mathrm{Enc_{\mathnormal{I}}}$와 language encoder $\mathrm{Enc_{\mathnormal{L}}}$로 구성되어 있다. Loss는 (1)과 (2)의 식을 minimize 하는 것과 유사하지만, (2)에서 classification logit $S_{\text{cls}}$를 (3)에 나와있는 대로 region-word aligment scores $S_{\text{ground}}$으로 대체하여 사용한다.
<br/><br/><br/><br/><br/>

위와 같은 방식을 사용하면 어떤 Detection 모델도 grounding 모델로 변환할 수 있다.

Grounding formulation은 [MDETR📄](https://arxiv.org/pdf/2104.12763)에서 영향을 얻었다. Grounding 모델은 zero-shot detection 모델과 유사하지만 GLIP이 detection과 grounding을 통합하고, language-aware deep fusion을 사용한다는 점과 image-text data를 사용한 확장이 가능한 점에서 다르다.
<br/><br/><br/><br/><br/>

## 2. Language-Aware Deep Fusion

![GLIP_7.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/b4f18dbf-7073-4ab1-8657-36c0a6ffb7c2){: width="500px"}<br/>
Image와 text를 인코딩한 후 마지막에만 융합하여 alignment score를 계산하는 late-fusion 모델과 달리, phrase grounding 모델을 위해서는 deep fusion 모델이 필요하다. 논문에서는 위의 그림과 같이 마지막 몇 개의 인코딩 layer에서 **Image와 text 정보를 융합하는 deep fusion**을 제안했다. 
<br/><br/>

Image encoder로는 **DyHead**를 사용했고, Text encoder로는 **BERT**를 사용했다. 위의 그림과 같이 Visual backbone에서 나온 visual feature $O^0$, Language backbone에서 나온 token backbone $P^0$에서 시작해 Fusion, 즉 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Cross modality multi-head attention</span></mark>** 과정을 거친 뒤 다음 layer에 전달한다. 위 그림의 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>deep-fused encoder</span></mark>**를 수식으로 표현하면 아래와 같다.

![GLIP_8.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/b176b9ca-d8d7-4e6a-9dc1-838fe971620b){: width="800px"}
<br/><br/>

더 자세히 (4)의 X-MHA는 아래와 같이 구성되어 있다. <br/>
![GLIP_9.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/d2603fbb-045d-450a-a1e9-c207927181b1){: width="650px"}
<br/><br/>

이와 같은 deep-fused encoder는 2가지 장점이 있다.

1. phrase grounding 성능을 향상시킨다.
2. 하나의 모델이 여러가지 downstream detection task에 잘 동작하도록 한다: 학습된 visual-feature가 language를 인식하게 하므로 모델의 prediction은 text prompt에 기반함.
<br/><br/><br/><br/><br/>

## 3. Pre-training with Scalable Semantic-Rich Data

이전까지 의미적으로 풍부하고 방대한 detection 데이터를 수집하기 위한 노력이 있었지만 human annotation은 비용이 많이 들고 제한적인 문제가 있다. 기존에 이를 보완하기 위한 self-training 방법이 있었다.

[**Self-training 방법**📄](https://arxiv.org/abs/2006.06882)은 knowledge distillation 방법의 응용으로, teacher(pre-trained detector)를 사용하여 raw 이미지에서 bbox를 예측하고 pseudo detection label을 생성하여 student 모델을 학습하는 방법이다. 해당 방법은 기존 데이터셋을 기반하여 미리 정의된 label에 대해서만 예측할 수 있으므로 concept pool의 관점에서 여전히 제한적이다.
<br/><br/><br/>

GLIP에서는 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>grounding data를 이용하여 self-training 방식을 확장</span></mark>**할 수 있음을 증명했다. 기존에 detection에서 사용한 가장 큰 category가 2000개 이하인데 비해 caption에는 아주 넓은 범위의 concept를 포함하고 있다.

방법은 기존의 self-training 방식을 확장하였고, 아래와 같은 과정을 거쳐 이루어진다.

1. Gold (human-annotated) detection + grounding data를 사용하여 teacher GLIP을 학습한다.
2. 학습이 끝난 뒤, teacher 모델을 사용하여 web-collected image-text data에 대해 box를 예측한다. 이 때, text data에서 NLP parser에 의해 감지된 명사구에 대해 예측한다.
3. 최종적으로 student model은 1번의 Gold data + 생성된 pseudo grounding data에 대해 학습된다.
<br/><br/><br/><br/><br/><br/>

# Transfer to Established Benchmarks

---

논문에서는 아래의 3가지 benchmark에 대해 domain transfer 성능을 보여주고 있다. 

1) MS-COCO object detection (COCO)<br/>
2) LVIS<br/>
3) Flickr30 for phrase grounding
<br/><br/>

저자는 논문에 제한된 핵심 기술에 대해 증명하기 위해 모델을 다양하게 변형했다. 증명을 위한 model variant는 아래 표와 같다.<br/>
![GLIP_10.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/a2bce8c6-8cfd-4ced-a1d3-663cbf926f13){: width="550px"}
<br/><br/><br/>

### 1. Zero-Shot and Supervised Transfer on COCO

모델의 common category로의 transfer ability를 평가하기 위해 MS-COCO에 대해 실험했다. 1) zero-shot domain transfer, and 2) supervised transfer 두 가지 설정에 대해 평가했다. 

결과는 아래의 표와 같다. 전반적으로 GLIP 모델은 강력한 zero-shot 및 supervised 성능을 달성했다. 다양한 model variant에 대한 결과를 통해 모델에서 제안된 방법이 성능 향상에 필수적이라는 것을 알 수 있다.<br/>
![GLIP_11.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/8cc948c2-6d9d-4ece-ae84-4514a1459097){: width="1400px"}
<br/><br/><br/><br/>

### 2. Zero-Shot Transfer on LVIS

다음으로는 zero-shot setting에서 LVIS의 다양하고 희귀한 물체를 인식하는 모델의 능력을 평가했다. LVIS의 annotated data에 대해 학습된 세 가지 supervised 모델과 비교했다. 결과는 아래 표와 같다. GLIP은 supervised 모델과 유사하거나 훨씬 뛰어난 결과를 보여주고 있다.<br/>
![GLIP_12.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/971d6633-b899-46ae-8ea5-e228717f73e1){: width="1400px"}
<br/><br/><br/><br/>

### 3. Phrase Grounding on Flickr30K Entities

Natural language에서 Flickr30K으로 ground 하는 모델의 능력을 평가한다. Flickr30K는 gold grounding data에 포함되어 있으므로 pre-training 후 모델을 직접 평가했다. <br/>
![GLIP_13.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/74ce75cf-cdef-4c22-90a8-dd7be4a8d865){: width="1200px"}
<br/><br/><br/>

### 4. Analysis
![GLIP_14.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/5799a435-ee88-4066-9440-b4401750c506){: width="600px"}

마지막으로 다양한 데이터 소스에 대해 GLIP-T를 pre-train하여 ablation study를 수행했다.

1. GLIP의 접근 방식은 모델을 bootstrap 하기 위해 detection 데이터셋을 사용한다고 가정했다. 그렇다면 grounding data가 detection data와 결합되면 성능이 정말 개선되는가? → 위 표의 행 1-6을 통해 일관된 개선을 확인할 수 있다.
2. Grounding 데이터가 common category와 rare category 모두에서 효과적이라는 것을 보여주었다. Detection 데이터와 grounding 데이터의 확대와 관련하여 실증적 비교를 제시한다. → 4개의 공개 detection 데이터셋(표의 8행)에대해 학습된 GLIP을 제시. 여러개의 데이터셋을 합친 큰 규모이지만 image-text pair를 사용했을 때보다는 부족한 것을 볼 수 있다.
<br/><br/><br/><br/><br/><br/>

# Object Detection in the Wild

---

다양한 real-world task에 대한 GLIP의 transferability를 평가하기 위해 “Object Detection in the Wild” (ODinW) setting을 만들었다. [Roboflow](https://public.roboflow.com/object-detection)에서 각각 다른 localization skill을 필요로 하는 13개의 public 데이터셋을 선정했다.(e.g. EgoHands-사람 손 위치 찾기, Pothole-도로의 구멍 감지)
<br/><br/>

GLIP은 다양한 task로 transfer가 용이하다.<br/>
1. baseline보다 적은 양의 task-specific data로 동일한 성능에 도달
2. 전체 grounding model을 변경하지 않고 text prompt를 변경하는 것 만으로도 새로운 task를 수행함
<br/><br/><br/><br/>

### 1. Data Efficiency

![GLIP_15.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/632f9738-2c9a-46fd-a6b9-f431e475409a){: width="500px"}

Objects365 데이터셋에서 pre-train된 SoTA detector DyHead-T와 데이터 효율성에 대해 비교했다. 결과는 위 그림과 같다. grounding reformulation, deep fusion, grounding data, model scale-up 까지 모두 데이터 효율성 향상에 기여하는 것을 알 수 있다. 
<br/><br/><br/>

![GLIP_16.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/60731746-e824-40b6-98bd-840bc5f26822){: width="550px"}

위 그래프에서 5개의 서로 다른 데이터셋에 대한 GLIP variant의 zero-shot 성능을 추가로 조사했다. Grounding 데이터를 사용한 모델이 grounding 데이터를 사용하지 않은 모델보다 Pothole, EgoHands와 같은 새로운 concept을 테스트하는 특정 작업에서 상당한 개선을 가져온다는 것을 보여주고 있다.
<br/><br/><br/><br/>

### 2. One Model for All Tasks

최근 연구에서 pre-train 모델을 새로운 domain에 적용하면서 최소한의 parameter만 변경하는 방법에 대한 연구가 증가하고 있다. 이러한 관점에서 deployment efficiency metric을 기준으로 모델을 평가했다.

![GLIP_17.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/17b57112-cb0c-4b91-a8e3-6574e1a012ac){: width="600px"}

먼저 GLIP이 language-aware localization을 수행하므로, 즉 GLIP의 출력이 언어 입력에 크게 좌우되므로 GLIP이 task transfer을 수행하는 효율적인 방법을 제안했다. 사용자가 text prompt에 설명을 추가하여 속성을 추가하는 것이다.

예를들어 위의 그림을 살펴보자. 그림 6의 왼쪽에서 모델은 새로운 개체 "stingray"의 모든 발생 위치를 파악하는 데 실패했다. 그러나 prompt에 속성(예: "flat and round")을 추가함으로써 모델은 가오리의 모든 발생 위치를 파악했다. 간단한 prompt 변경을 통해 AP50을 4.6에서 9.7로 개선했다. 이는 GPT-3의 prompt 설계 기술과 유사하며 annotated data나 모델 재학습이 필요하지 않는 장점이 있다.
<br/><br/><br/>

![GLIP_18.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/fea97299-7e0e-406b-be38-c985253b888a){: width="500px"}

저자는 쉬운 deployment를 위해 최소한의 parameter를 조정하는 설정을 추가로 고려했다. 고전적인 detection 모델의 경우 "linear probing"(box regression, classification head만 학습)의 효율성을 조사한다. GLIP의 경우에는 box head와 region과 prompt embedding 사이의 projection layer만 fine-tuning하는 방식으로 "linear probing"을 수행했다.

3가지 설정(linear probing, prompt tuning, full-model tuning)에 대해 모델 성능을 평가한 결과는 위 그림과 같다. DyHead-T의 경우 linear probing과 전체 모델 tuning 사이의 격차가 크지만 GLIP의 경우 prompt tuning이 전체 tuning 결과와 거의 일치하는 것을 볼 수 있다.

<br/><br/><br/><br/>
