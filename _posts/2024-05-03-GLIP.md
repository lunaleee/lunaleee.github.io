---
title: "[ë…¼ë¬¸ ë¦¬ë·°] (GLIP) Grounded Language-Image Pre-training"
author: lunalee
date: 2024-05-03 19:23:12 +0900
categories: [AI, Paper Review]
tags: [Multi-modal, Detection, Knowledge Distillation]
pin: false
math: true
---

<br/><br/>
`Miscrosoft Research` `CVPR 2022`

- Paper: [https://arxiv.org/abs/2112.03857](https://arxiv.org/abs/2112.03857)
- Git: [https://github.com/microsoft/GLIP](https://github.com/microsoft/GLIP)
<br/><br/><br/>

#### ğŸ“– í•µì‹¬ í›‘ì–´ë³´ê¸° !!

- Image-Text pair, ì¦‰ **Multi-modal pre-training ê°œë…**ì„ Object detection taskë¡œ í™•ì¥
- í™•ì¥ì„ ìœ„í•´ Object detectionì„ **Phrase groundingê³¼ í†µí•©**í•¨. Detectionì˜ bboxë¥¼ $c$ classesë¡œ ë¶„ë¥˜í•˜ëŠ” ëŒ€ì‹  region-word aligment scoresë¥¼ ê³„ì‚°í•¨!
- CLIP ì²˜ëŸ¼ ë§ˆì§€ë§‰ layerì—ì„œ ìœµí•©í•˜ì—¬ text-image alignment scoreë¥¼ ê³„ì‚°í•˜ëŠ” late fusion ë°©ì‹ì´ ì•„ë‹Œ, ë§ˆì§€ë§‰ ì—¬ëŸ¬ê°œ layerì—ì„œ ìœµí•©í•˜ëŠ” **Deep-fusion êµ¬ì¡°** ì‚¬ìš©
- ê¸°ì¡´ì˜ **self-training ë°©ë²•**(knowledge distillation)ì„ í™•ì¥í•˜ì—¬ í° ê·œëª¨ì˜ ë°ì´í„°ì…‹ì— ëŒ€í•´ í•™ìŠµí•¨
<br/><br/><br/><br/>

# Introduction

---

CLIPì€ ëŒ€ëŸ‰ì˜ image-text pairì—ì„œ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>image-level representation</span></mark>**ì„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•˜ì˜€ë‹¤. Image-text pairë¥¼ í†µí•´ ë¯¸ë¦¬ ì •í•´ ë‘” categoryê°€ ì•„ë‹Œ í’ë¶€í•œ visual conceptì— ëŒ€í•´ í•™ìŠµí•˜ëŠ” **zero-shot setting**ìœ¼ë¡œ downstream taskë¡œ ì‰½ê²Œ ì ìš©ë  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ  object detection, segmentationë“±ì˜ taskì—ì„œëŠ” image-level representationì´ ì•„ë‹Œ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>object-level visual representation</span></mark>**ì´ í•„ìš”í•œ ë¬¸ì œê°€ ìˆë‹¤. 
<br/><br/>

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **ë¬¸ì¥ ë‚´ ë¬¸êµ¬**ì™€ **image ë‚´ object(ë˜ëŠ” region)** ì‚¬ì´ì˜ correspondenceì„ ì‹ë³„í•˜ëŠ” **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>phrase grounding</span></mark>**ì´ object-level í•™ìŠµì„ ìœ„í•œ íš¨ê³¼ì ì¸ ì‘ì—…ì´ë¼ëŠ” ê²ƒì„ ì¦ëª…í•œë‹¤. ë˜í•œ language-aware ë° semantic-rich visual representationì„ ì œê³µí•˜ëŠ” **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>GLIP(Grounded Language-Image Pre-training)</span></mark>**ì„ ì œì•ˆí•œë‹¤.

> **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>Phrase Grounding</span></mark>** (Visual Grounding): ì´ë¯¸ì§€ì™€ í•´ë‹¹ ì´ë¯¸ì§€ì— ëŒ€í•œ captionì´ ì£¼ì–´ì¡Œì„ ë•Œ, captionì˜ ëª…ì‚¬êµ¬ê°€ ì–¸ê¸‰í•œ ì‹¤ì²´ë¥¼ ì´ë¯¸ì§€ì˜ regionì—ì„œ í‘œì‹œí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ëŠ” ê²ƒ. ì¦‰, ì´ë¯¸ì§€ì˜ ì–´ëŠ ë¶€ë¶„ì„ ë¬˜ì‚¬í•˜ì˜€ëŠ”ì§€ bounding boxë¡œ í‘œì‹œí•˜ëŠ” ê²ƒì„ ì˜ë¯¸
>

![GLIP_1.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/7adcb46e-6075-4a50-b744-1a7f273a7b94){: width="600px"}
<br/><br/><br/>

ì£¼ìš”í•œ contributionì€ ì•„ë˜ì™€ ê°™ë‹¤.
<br/><br/>

**1) Unifying detection and grounding by reformulating object detection as phrase grounding.**

![GLIP_2.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/f3a4df9c-78c0-485a-a760-330cbaa52870){: width="1000px"}

Object detectionì„ phrase groundingìœ¼ë¡œ reformulationí•˜ê¸° ìœ„í•´ detection ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì´ë¯¸ì§€ ë¿ ì•„ë‹ˆë¼ **prompt ì…ë ¥**ì„ ì¶”ê°€í•œë‹¤. PromptëŠ” ì´ë¯¸ì§€ ë‚´ì—ì„œ **ëª¨ë“  í›„ë³´ category**ë¥¼ ì˜ë¯¸í•œë‹¤. ì˜ˆë¥¼ ë“¤ë©´ COCO object detectionì„ ìœ„í•œ promptëŠ” ìœ„ì˜ ê·¸ë¦¼(ì™¼ìª½)ê³¼ ê°™ì´ object classë¥¼ â€œ.â€ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ë‚˜ì—´í•œ ë¬¸ìì—´ì´ë‹¤. 
<br/><br/>

ëª¨ë“  object detection ëª¨ë¸ì—ì„œ box classifierì˜ **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>object classification logitì„ word-region alignment scoreë¡œ ëŒ€ì²´í•˜ì—¬ì¤Œìœ¼ë¡œì„œ grounding ëª¨ë¸ë¡œ ë³€í™˜</span></mark>**í•  ìˆ˜ ìˆë‹¤. ì¦‰, ìœ„ ê·¸ë¦¼ì˜ ì˜¤ë¥¸ìª½ê³¼ ê°™ì´ **region (or box) visual featureì™€ token (or phrase) language featureë¥¼ dot product**í•˜ì—¬ word-region alignment scoreë¥¼ êµ¬í•œë‹¤. 

ë§ˆì§€ë§‰ dot product layerì—ì„œë§Œ visionê³¼ languageë¥¼ ìœµí•©í•˜ëŠ” CLIPê³¼ ë‹¬ë¦¬ GLIPì€ ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ **deep cross-modality fusion**ì„ ì ìš©í•œë‹¤. Detection ë° groundingì˜ í†µí•©ì„ í†µí•´ ë‘ ê°€ì§€ ìœ í˜•ì˜ ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ pre-train í•  ìˆ˜ ìˆìœ¼ë©° ë‘ task ëª¨ë‘ì— ì´ì ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.
<br/><br/><br/><br/>

**2) Scaling up visual concepts with massive image-text data.**

ì¢‹ì€ grounding ëª¨ë¸(teacher)ì´ ì£¼ì–´ì§€ë©´ NLP parserì— ì˜í•´ ëª…ì‚¬êµ¬ë¥¼ ê°ì§€í•´ì„œ grounding boxë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ê³ , ëŒ€ê·œëª¨ image-text-paired ë°ì´í„°ë¥¼ ìƒì„±í•˜ì—¬ GLIP pre-training ë°ì´í„°ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” 27Mê°œì˜ grounding dataë¥¼ ì‚¬ìš©í•˜ì—¬ (student) GLIP-Largeë¥¼ pre-train í–ˆë‹¤.

![GLIP_3.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/da85b8a5-d702-4fc5-bc06-1ce5773119c3){: width="600px"}<br/>
ìœ„ì˜ ê·¸ë¦¼ì€ teacher ëª¨ë¸ì— ì˜í•´ ìƒì„±ëœ generated boxì— ëŒ€í•œ ì˜ˆì œì´ë‹¤. ìœ„ì™€ ê°™ì´ semantic-richí•œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ student ëª¨ë¸ì—ì„œë„ semantic-richí•œ ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. Grounding dataë¥¼ í™•ì¥í•˜ëŠ” ê°„ë‹¨í•œ ì „ëµì´ ê²½í—˜ì ìœ¼ë¡œ íš¨ê³¼ì ì´ë©° íŠ¹íˆ downstream taskì˜ ì„±ëŠ¥ì„ ê°œì„ í•œë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•œë‹¤.
<br/><br/><br/><br/>

**3) Transfer learning with GLIP: one model for all.**

Semantic-richí•œ pre-trainingì€ **domain transferì— ìœ ë¦¬**í•˜ì—¬ ì¶”ê°€ì ì¸ human annotation ì—†ì´ë„ ë‹¤ì–‘í•œ taskë¡œ transferí•  ìˆ˜ ìˆë‹¤. GLIP-L ëª¨ë¸ì€  COCO, LVIS ë°ì´í„°ì…‹ì—ì„œ ë§ì€ supervised baselineì„ ëŠ¥ê°€í–ˆë‹¤. 

Task-specific annotationì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê²½ìš° ì „ì²´ ëª¨ë¸ì„ tuningí•˜ëŠ” ëŒ€ì‹  **task-specific prompt embeddingë§Œ tuning**í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ë°©ë²•ì„ í†µí•´ í•˜ë‚˜ì˜ GLIP ëª¨ë¸ì€ ëª¨ë“  downstream taskì—ì„œ ë™ì‹œì— ì˜ ìˆ˜í–‰ë˜ì–´ fine-tuning ë° ë°°í¬ ë¹„ìš©ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤.
<br/><br/><br/><br/><br/><br/>

# Grounded Language Image Pre-training

---

## 1. Unified Formulation

í•´ë‹¹ sectionì—ì„œëŠ” ê¸°ì¡´ì˜ Object detectionì„ grounding problemìœ¼ë¡œ ì „í™˜í•˜ê³  í†µì¼í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ë‚´ìš©ì„ ë‹´ê³  ìˆë‹¤. 
<br/><br/>

#### Background: object detection.

ë¨¼ì €, Object detectionì— ëŒ€í•´ ê°„ë‹¨íˆ ì§šì–´ë³´ì. ì¼ë°˜ì ì¸ detection ëª¨ë¸ì€ ì•„ë˜ì™€ ê°™ì´ Visual Encoder $\text{Enc}_I$, Box Classifier $C$, Box Regressor $R$ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. 

![GLIP_4.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/add07d5b-a1ab-4af6-8da4-069638f73f3a){: width="900px"}
<br/>

ì—¬ê¸°ì„œ Box ClassifierëŠ” ì¼ë°˜ì ìœ¼ë¡œ ê°„ë‹¨í•œ linear layerë¡œ êµ¬ì„±ë˜ê³ , classification loss ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![GLIP_5.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/14ef48a0-e71f-4432-826f-1875c4027104){: width="700px"}

(2)ë²ˆ ìˆ˜ì‹ì—ì„œ $T \in \lbrace0, 1\rbrace^{N \times c}$ ëŠ” regionê³¼ classê°„ì˜ **target matching**ì´ê³ , $loss(S;T)$ëŠ” ì¼ë°˜ì ìœ¼ë¡œ two-stage detectorì˜ ê²½ìš° cross-entropy lossì´ê³  one-stage detectorì˜ ê²½ìš° focal loss ì´ë‹¤.
<br/><br/>

> $T \in \lbrace0,1\rbrace^{N \times c}$ ëŠ” regionê³¼ classê°„ì˜ **target matching**ì´ë‹¤. $T$ëŠ” ê³ ì „ì ì¸ many-to-1 matching ë˜ëŠ” bipartite Hungarian match ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ë˜ëŠ” ê²ƒì„ ë§í•œë‹¤. ê¸°ë³¸ì ì¸ Object Detectionì˜ ë°©ë²•ì€ ì•„ë˜ì™€ ê°™ì€ ìˆœì„œë¡œ ì§„í–‰ëœë‹¤.
>
> 1. **Region Proposal**: RPNì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ ì ì¬ì ì¸ ê°ì²´ê°€ ì¡´ì¬í•  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ ì˜ì—­ì„ ì œì•ˆ
> 2. **Feature Extraction**: ê° ì œì•ˆëœ ì˜ì—­ì—ì„œ íŠ¹ì§•ì„ ì¶”ì¶œ
> 3. **Classification and Scoring**: ê° ì˜ì—­ì˜ íŠ¹ì§•ì„ ê¸°ë°˜ìœ¼ë¡œ classification headë¥¼ í†µí•´ í•´ë‹¹ ì˜ì—­ì´ íŠ¹ì • í´ë˜ìŠ¤(ì˜ˆ: ì‚¬ëŒ, ìë™ì°¨, ë™ë¬¼ ë“±)ì— ì†í•  í™•ë¥ ì„ ê³„ì‚°
> 4. **Matching**: Hungarian ë“±ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê° ì˜ì—­ì„ ìµœì ì˜ í´ë˜ìŠ¤ì— ë§¤ì¹­. ì´ë•Œ ë§¤ì¹­ ë¹„ìš©ì€ ì˜ì—­ê³¼ í´ë˜ìŠ¤ ê°„ì˜ ìœ ì‚¬ë„ ì ìˆ˜ì— ê¸°ë°˜í•¨
>
> * Matching ê³¼ì •ì€ trainingê³¼ inference ëª¨ë‘ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤. Training ì‹œì—ëŠ” ëª¨ë¸ì´ ì •í™•í•˜ê²Œ í•™ìŠµë˜ë„ë¡ ë„ì™€ì£¼ë©°, inference ì‹œì—ëŠ” ìµœì ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤.
>

<br/><br/>

#### Object detection as phrase grounding.

ì´ì œ detectionì„ reformulateí•˜ì—¬ grounding taskë¡œ ë§Œë“¤ì–´ë³´ì. ê°ê° region/boxë¥¼ $c$ classesë¡œ ë¶„ë¥˜í•˜ëŠ” ëŒ€ì‹  **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>ê°ê°ì˜ regionì„ text promptì˜ $c$ phraseì— grounding/aligning</span></mark>** í•œë‹¤.
<br/><br/>

Detectionì„ ìœ„í•œ text promptëŠ” ê°€ì¥ ì‰½ê²ŒëŠ” ground ë˜ëŠ” **í›„ë³´ ë¬¸êµ¬ë¥¼ ë‚˜ì—´**í•˜ëŠ” ë°©ì‹ì´ë‹¤.
<br/>

$$
\text{Prompt =  â€œDetect: person, bicycle, car, ... , toothbrushâ€}
$$

PromptëŠ” ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ë””ìì¸ í•  ìˆ˜ ìˆì§€ë§Œ, BERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ language encoder $\text{Enc}_L$ì„ ì´ˆê¸°í™”í•˜ëŠ” ê²½ìš°ëŠ” ì‚¬ëŒì—ê²Œ ì¹œìˆ™í•œ ë¬¸ì¥í˜• prompt ë³´ë‹¤ ìœ„ì™€ ê°™ì€ ë‹¨ì–´ ë‚˜ì—´í˜•ì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤(ë’¤ì—ì„œ ë‹¤ì‹œ ì–¸ê¸‰).
<br/><br/>

Grounding ëª¨ë¸ì€ image regionê³¼ ë‹¨ì–´ ì‚¬ì´ì˜ alignment score $S_{\text{ground}}$ë¥¼ ê³„ì‚°í•œë‹¤.

![GLIP_6.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/7da518eb-34e4-4e70-a289-8f85a557e75f){: width="600px"}<br/>
PëŠ” language encoderì˜ contextual word/token featureì´ë©° (2)ì˜ weight matrix Wì™€ ìœ ì‚¬í•œ ì—­í• ì„ í•œë‹¤.

Grounding ëª¨ë¸ì€ image encoder $\mathrm{Enc_{\mathnormal{I}}}$ì™€ language encoder $\mathrm{Enc_{\mathnormal{L}}}$ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. LossëŠ” (1)ê³¼ (2)ì˜ ì‹ì„ minimize í•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, (2)ì—ì„œ classification logit $S_{\text{cls}}$ë¥¼ (3)ì— ë‚˜ì™€ìˆëŠ” ëŒ€ë¡œ region-word aligment scores $S_{\text{ground}}$ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ ì‚¬ìš©í•œë‹¤.
<br/><br/><br/><br/><br/>

ìœ„ì™€ ê°™ì€ ë°©ì‹ì„ ì‚¬ìš©í•˜ë©´ ì–´ë–¤ Detection ëª¨ë¸ë„ grounding ëª¨ë¸ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.

Grounding formulationì€ [MDETRğŸ“„](https://arxiv.org/pdf/2104.12763)ì—ì„œ ì˜í–¥ì„ ì–»ì—ˆë‹¤. Grounding ëª¨ë¸ì€ zero-shot detection ëª¨ë¸ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ GLIPì´ detectionê³¼ groundingì„ í†µí•©í•˜ê³ , language-aware deep fusionì„ ì‚¬ìš©í•œë‹¤ëŠ” ì ê³¼ image-text dataë¥¼ ì‚¬ìš©í•œ í™•ì¥ì´ ê°€ëŠ¥í•œ ì ì—ì„œ ë‹¤ë¥´ë‹¤.
<br/><br/><br/><br/><br/>

## 2. Language-Aware Deep Fusion

![GLIP_7.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/b4f18dbf-7073-4ab1-8657-36c0a6ffb7c2){: width="500px"}<br/>
Imageì™€ textë¥¼ ì¸ì½”ë”©í•œ í›„ ë§ˆì§€ë§‰ì—ë§Œ ìœµí•©í•˜ì—¬ alignment scoreë¥¼ ê³„ì‚°í•˜ëŠ” late-fusion ëª¨ë¸ê³¼ ë‹¬ë¦¬, phrase grounding ëª¨ë¸ì„ ìœ„í•´ì„œëŠ” deep fusion ëª¨ë¸ì´ í•„ìš”í•˜ë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë§ˆì§€ë§‰ ëª‡ ê°œì˜ ì¸ì½”ë”© layerì—ì„œ **Imageì™€ text ì •ë³´ë¥¼ ìœµí•©í•˜ëŠ” deep fusion**ì„ ì œì•ˆí–ˆë‹¤. 
<br/><br/>

Image encoderë¡œëŠ” **DyHead**ë¥¼ ì‚¬ìš©í–ˆê³ , Text encoderë¡œëŠ” **BERT**ë¥¼ ì‚¬ìš©í–ˆë‹¤. ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ Visual backboneì—ì„œ ë‚˜ì˜¨ visual feature $O^0$, Language backboneì—ì„œ ë‚˜ì˜¨ token backbone $P^0$ì—ì„œ ì‹œì‘í•´ Fusion, ì¦‰ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Cross modality multi-head attention</span></mark>** ê³¼ì •ì„ ê±°ì¹œ ë’¤ ë‹¤ìŒ layerì— ì „ë‹¬í•œë‹¤. ìœ„ ê·¸ë¦¼ì˜ **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>deep-fused encoder</span></mark>**ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

![GLIP_8.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/b176b9ca-d8d7-4e6a-9dc1-838fe971620b){: width="800px"}
<br/><br/>

ë” ìì„¸íˆ (4)ì˜ X-MHAëŠ” ì•„ë˜ì™€ ê°™ì´ êµ¬ì„±ë˜ì–´ ìˆë‹¤. <br/>
![GLIP_9.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/d2603fbb-045d-450a-a1e9-c207927181b1){: width="650px"}
<br/><br/>

ì´ì™€ ê°™ì€ deep-fused encoderëŠ” 2ê°€ì§€ ì¥ì ì´ ìˆë‹¤.

1. phrase grounding ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤.
2. í•˜ë‚˜ì˜ ëª¨ë¸ì´ ì—¬ëŸ¬ê°€ì§€ downstream detection taskì— ì˜ ë™ì‘í•˜ë„ë¡ í•œë‹¤: í•™ìŠµëœ visual-featureê°€ languageë¥¼ ì¸ì‹í•˜ê²Œ í•˜ë¯€ë¡œ ëª¨ë¸ì˜ predictionì€ text promptì— ê¸°ë°˜í•¨.
<br/><br/><br/><br/><br/>

## 3. Pre-training with Scalable Semantic-Rich Data

ì´ì „ê¹Œì§€ ì˜ë¯¸ì ìœ¼ë¡œ í’ë¶€í•˜ê³  ë°©ëŒ€í•œ detection ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê¸° ìœ„í•œ ë…¸ë ¥ì´ ìˆì—ˆì§€ë§Œ human annotationì€ ë¹„ìš©ì´ ë§ì´ ë“¤ê³  ì œí•œì ì¸ ë¬¸ì œê°€ ìˆë‹¤. ê¸°ì¡´ì— ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•œ self-training ë°©ë²•ì´ ìˆì—ˆë‹¤.

[**Self-training ë°©ë²•**ğŸ“„](https://arxiv.org/abs/2006.06882)ì€ knowledge distillation ë°©ë²•ì˜ ì‘ìš©ìœ¼ë¡œ, teacher(pre-trained detector)ë¥¼ ì‚¬ìš©í•˜ì—¬ raw ì´ë¯¸ì§€ì—ì„œ bboxë¥¼ ì˜ˆì¸¡í•˜ê³  pseudo detection labelì„ ìƒì„±í•˜ì—¬ student ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ë‹¤. í•´ë‹¹ ë°©ë²•ì€ ê¸°ì¡´ ë°ì´í„°ì…‹ì„ ê¸°ë°˜í•˜ì—¬ ë¯¸ë¦¬ ì •ì˜ëœ labelì— ëŒ€í•´ì„œë§Œ ì˜ˆì¸¡í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ concept poolì˜ ê´€ì ì—ì„œ ì—¬ì „íˆ ì œí•œì ì´ë‹¤.
<br/><br/><br/>

GLIPì—ì„œëŠ” **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>grounding dataë¥¼ ì´ìš©í•˜ì—¬ self-training ë°©ì‹ì„ í™•ì¥</span></mark>**í•  ìˆ˜ ìˆìŒì„ ì¦ëª…í–ˆë‹¤. ê¸°ì¡´ì— detectionì—ì„œ ì‚¬ìš©í•œ ê°€ì¥ í° categoryê°€ 2000ê°œ ì´í•˜ì¸ë° ë¹„í•´ captionì—ëŠ” ì•„ì£¼ ë„“ì€ ë²”ìœ„ì˜ conceptë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤.

ë°©ë²•ì€ ê¸°ì¡´ì˜ self-training ë°©ì‹ì„ í™•ì¥í•˜ì˜€ê³ , ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ì„ ê±°ì³ ì´ë£¨ì–´ì§„ë‹¤.

1. Gold (human-annotated) detection + grounding dataë¥¼ ì‚¬ìš©í•˜ì—¬ teacher GLIPì„ í•™ìŠµí•œë‹¤.
2. í•™ìŠµì´ ëë‚œ ë’¤, teacher ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ web-collected image-text dataì— ëŒ€í•´ boxë¥¼ ì˜ˆì¸¡í•œë‹¤. ì´ ë•Œ, text dataì—ì„œ NLP parserì— ì˜í•´ ê°ì§€ëœ ëª…ì‚¬êµ¬ì— ëŒ€í•´ ì˜ˆì¸¡í•œë‹¤.
3. ìµœì¢…ì ìœ¼ë¡œ student modelì€ 1ë²ˆì˜ Gold data + ìƒì„±ëœ pseudo grounding dataì— ëŒ€í•´ í•™ìŠµëœë‹¤.
<br/><br/><br/><br/><br/><br/>

# Transfer to Established Benchmarks

---

ë…¼ë¬¸ì—ì„œëŠ” ì•„ë˜ì˜ 3ê°€ì§€ benchmarkì— ëŒ€í•´ domain transfer ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤. 

1) MS-COCO object detection (COCO)<br/>
2) LVIS<br/>
3) Flickr30 for phrase grounding
<br/><br/>

ì €ìëŠ” ë…¼ë¬¸ì— ì œí•œëœ í•µì‹¬ ê¸°ìˆ ì— ëŒ€í•´ ì¦ëª…í•˜ê¸° ìœ„í•´ ëª¨ë¸ì„ ë‹¤ì–‘í•˜ê²Œ ë³€í˜•í–ˆë‹¤. ì¦ëª…ì„ ìœ„í•œ model variantëŠ” ì•„ë˜ í‘œì™€ ê°™ë‹¤.<br/>
![GLIP_10.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/a2bce8c6-8cfd-4ced-a1d3-663cbf926f13){: width="550px"}
<br/><br/><br/>

### 1. Zero-Shot and Supervised Transfer on COCO

ëª¨ë¸ì˜ common categoryë¡œì˜ transfer abilityë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ MS-COCOì— ëŒ€í•´ ì‹¤í—˜í–ˆë‹¤. 1) zero-shot domain transfer, and 2) supervised transfer ë‘ ê°€ì§€ ì„¤ì •ì— ëŒ€í•´ í‰ê°€í–ˆë‹¤. 

ê²°ê³¼ëŠ” ì•„ë˜ì˜ í‘œì™€ ê°™ë‹¤. ì „ë°˜ì ìœ¼ë¡œ GLIP ëª¨ë¸ì€ ê°•ë ¥í•œ zero-shot ë° supervised ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆë‹¤. ë‹¤ì–‘í•œ model variantì— ëŒ€í•œ ê²°ê³¼ë¥¼ í†µí•´ ëª¨ë¸ì—ì„œ ì œì•ˆëœ ë°©ë²•ì´ ì„±ëŠ¥ í–¥ìƒì— í•„ìˆ˜ì ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.<br/>
![GLIP_11.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/8cc948c2-6d9d-4ece-ae84-4514a1459097){: width="1400px"}
<br/><br/><br/><br/>

### 2. Zero-Shot Transfer on LVIS

ë‹¤ìŒìœ¼ë¡œëŠ” zero-shot settingì—ì„œ LVISì˜ ë‹¤ì–‘í•˜ê³  í¬ê·€í•œ ë¬¼ì²´ë¥¼ ì¸ì‹í•˜ëŠ” ëª¨ë¸ì˜ ëŠ¥ë ¥ì„ í‰ê°€í–ˆë‹¤. LVISì˜ annotated dataì— ëŒ€í•´ í•™ìŠµëœ ì„¸ ê°€ì§€ supervised ëª¨ë¸ê³¼ ë¹„êµí–ˆë‹¤. ê²°ê³¼ëŠ” ì•„ë˜ í‘œì™€ ê°™ë‹¤. GLIPì€ supervised ëª¨ë¸ê³¼ ìœ ì‚¬í•˜ê±°ë‚˜ í›¨ì”¬ ë›°ì–´ë‚œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ê³  ìˆë‹¤.<br/>
![GLIP_12.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/971d6633-b899-46ae-8ea5-e228717f73e1){: width="1400px"}
<br/><br/><br/><br/>

### 3. Phrase Grounding on Flickr30K Entities

Natural languageì—ì„œ Flickr30Kìœ¼ë¡œ ground í•˜ëŠ” ëª¨ë¸ì˜ ëŠ¥ë ¥ì„ í‰ê°€í•œë‹¤. Flickr30KëŠ” gold grounding dataì— í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ pre-training í›„ ëª¨ë¸ì„ ì§ì ‘ í‰ê°€í–ˆë‹¤. <br/>
![GLIP_13.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/74ce75cf-cdef-4c22-90a8-dd7be4a8d865){: width="1200px"}
<br/><br/><br/>

### 4. Analysis
![GLIP_14.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/5799a435-ee88-4066-9440-b4401750c506){: width="600px"}

ë§ˆì§€ë§‰ìœ¼ë¡œ ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì— ëŒ€í•´ GLIP-Të¥¼ pre-trainí•˜ì—¬ ablation studyë¥¼ ìˆ˜í–‰í–ˆë‹¤.

1. GLIPì˜ ì ‘ê·¼ ë°©ì‹ì€ ëª¨ë¸ì„ bootstrap í•˜ê¸° ìœ„í•´ detection ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •í–ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ grounding dataê°€ detection dataì™€ ê²°í•©ë˜ë©´ ì„±ëŠ¥ì´ ì •ë§ ê°œì„ ë˜ëŠ”ê°€? â†’ ìœ„ í‘œì˜ í–‰ 1-6ì„ í†µí•´ ì¼ê´€ëœ ê°œì„ ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
2. Grounding ë°ì´í„°ê°€ common categoryì™€ rare category ëª¨ë‘ì—ì„œ íš¨ê³¼ì ì´ë¼ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ì—ˆë‹¤. Detection ë°ì´í„°ì™€ grounding ë°ì´í„°ì˜ í™•ëŒ€ì™€ ê´€ë ¨í•˜ì—¬ ì‹¤ì¦ì  ë¹„êµë¥¼ ì œì‹œí•œë‹¤. â†’ 4ê°œì˜ ê³µê°œ detection ë°ì´í„°ì…‹(í‘œì˜ 8í–‰)ì—ëŒ€í•´ í•™ìŠµëœ GLIPì„ ì œì‹œ. ì—¬ëŸ¬ê°œì˜ ë°ì´í„°ì…‹ì„ í•©ì¹œ í° ê·œëª¨ì´ì§€ë§Œ image-text pairë¥¼ ì‚¬ìš©í–ˆì„ ë•Œë³´ë‹¤ëŠ” ë¶€ì¡±í•œ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.
<br/><br/><br/><br/><br/><br/>

# Object Detection in the Wild

---

ë‹¤ì–‘í•œ real-world taskì— ëŒ€í•œ GLIPì˜ transferabilityë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ â€œObject Detection in the Wildâ€ (ODinW) settingì„ ë§Œë“¤ì—ˆë‹¤. [Roboflow](https://public.roboflow.com/object-detection)ì—ì„œ ê°ê° ë‹¤ë¥¸ localization skillì„ í•„ìš”ë¡œ í•˜ëŠ” 13ê°œì˜ public ë°ì´í„°ì…‹ì„ ì„ ì •í–ˆë‹¤.(e.g. EgoHands-ì‚¬ëŒ ì† ìœ„ì¹˜ ì°¾ê¸°, Pothole-ë„ë¡œì˜ êµ¬ë© ê°ì§€)
<br/><br/>

GLIPì€ ë‹¤ì–‘í•œ taskë¡œ transferê°€ ìš©ì´í•˜ë‹¤.<br/>
1. baselineë³´ë‹¤ ì ì€ ì–‘ì˜ task-specific dataë¡œ ë™ì¼í•œ ì„±ëŠ¥ì— ë„ë‹¬
2. ì „ì²´ grounding modelì„ ë³€ê²½í•˜ì§€ ì•Šê³  text promptë¥¼ ë³€ê²½í•˜ëŠ” ê²ƒ ë§Œìœ¼ë¡œë„ ìƒˆë¡œìš´ taskë¥¼ ìˆ˜í–‰í•¨
<br/><br/><br/><br/>

### 1. Data Efficiency

![GLIP_15.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/632f9738-2c9a-46fd-a6b9-f431e475409a){: width="500px"}

Objects365 ë°ì´í„°ì…‹ì—ì„œ pre-trainëœ SoTA detector DyHead-Tì™€ ë°ì´í„° íš¨ìœ¨ì„±ì— ëŒ€í•´ ë¹„êµí–ˆë‹¤. ê²°ê³¼ëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ë‹¤. grounding reformulation, deep fusion, grounding data, model scale-up ê¹Œì§€ ëª¨ë‘ ë°ì´í„° íš¨ìœ¨ì„± í–¥ìƒì— ê¸°ì—¬í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. 
<br/><br/><br/>

![GLIP_16.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/60731746-e824-40b6-98bd-840bc5f26822){: width="550px"}

ìœ„ ê·¸ë˜í”„ì—ì„œ 5ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ GLIP variantì˜ zero-shot ì„±ëŠ¥ì„ ì¶”ê°€ë¡œ ì¡°ì‚¬í–ˆë‹¤. Grounding ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ì´ grounding ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì€ ëª¨ë¸ë³´ë‹¤ Pothole, EgoHandsì™€ ê°™ì€ ìƒˆë¡œìš´ conceptì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” íŠ¹ì • ì‘ì—…ì—ì„œ ìƒë‹¹í•œ ê°œì„ ì„ ê°€ì ¸ì˜¨ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤.
<br/><br/><br/><br/>

### 2. One Model for All Tasks

ìµœê·¼ ì—°êµ¬ì—ì„œ pre-train ëª¨ë¸ì„ ìƒˆë¡œìš´ domainì— ì ìš©í•˜ë©´ì„œ ìµœì†Œí•œì˜ parameterë§Œ ë³€ê²½í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì—°êµ¬ê°€ ì¦ê°€í•˜ê³  ìˆë‹¤. ì´ëŸ¬í•œ ê´€ì ì—ì„œ deployment efficiency metricì„ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ì„ í‰ê°€í–ˆë‹¤.

![GLIP_17.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/17b57112-cb0c-4b91-a8e3-6574e1a012ac){: width="600px"}

ë¨¼ì € GLIPì´ language-aware localizationì„ ìˆ˜í–‰í•˜ë¯€ë¡œ, ì¦‰ GLIPì˜ ì¶œë ¥ì´ ì–¸ì–´ ì…ë ¥ì— í¬ê²Œ ì¢Œìš°ë˜ë¯€ë¡œ GLIPì´ task transferì„ ìˆ˜í–‰í•˜ëŠ” íš¨ìœ¨ì ì¸ ë°©ë²•ì„ ì œì•ˆí–ˆë‹¤. ì‚¬ìš©ìê°€ text promptì— ì„¤ëª…ì„ ì¶”ê°€í•˜ì—¬ ì†ì„±ì„ ì¶”ê°€í•˜ëŠ” ê²ƒì´ë‹¤.

ì˜ˆë¥¼ë“¤ì–´ ìœ„ì˜ ê·¸ë¦¼ì„ ì‚´í´ë³´ì. ê·¸ë¦¼ 6ì˜ ì™¼ìª½ì—ì„œ ëª¨ë¸ì€ ìƒˆë¡œìš´ ê°œì²´ "stingray"ì˜ ëª¨ë“  ë°œìƒ ìœ„ì¹˜ë¥¼ íŒŒì•…í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆë‹¤. ê·¸ëŸ¬ë‚˜ promptì— ì†ì„±(ì˜ˆ: "flat and round")ì„ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ëª¨ë¸ì€ ê°€ì˜¤ë¦¬ì˜ ëª¨ë“  ë°œìƒ ìœ„ì¹˜ë¥¼ íŒŒì•…í–ˆë‹¤. ê°„ë‹¨í•œ prompt ë³€ê²½ì„ í†µí•´ AP50ì„ 4.6ì—ì„œ 9.7ë¡œ ê°œì„ í–ˆë‹¤. ì´ëŠ” GPT-3ì˜ prompt ì„¤ê³„ ê¸°ìˆ ê³¼ ìœ ì‚¬í•˜ë©° annotated dataë‚˜ ëª¨ë¸ ì¬í•™ìŠµì´ í•„ìš”í•˜ì§€ ì•ŠëŠ” ì¥ì ì´ ìˆë‹¤.
<br/><br/><br/>

![GLIP_18.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/fea97299-7e0e-406b-be38-c985253b888a){: width="500px"}

ì €ìëŠ” ì‰¬ìš´ deploymentë¥¼ ìœ„í•´ ìµœì†Œí•œì˜ parameterë¥¼ ì¡°ì •í•˜ëŠ” ì„¤ì •ì„ ì¶”ê°€ë¡œ ê³ ë ¤í–ˆë‹¤. ê³ ì „ì ì¸ detection ëª¨ë¸ì˜ ê²½ìš° "linear probing"(box regression, classification headë§Œ í•™ìŠµ)ì˜ íš¨ìœ¨ì„±ì„ ì¡°ì‚¬í•œë‹¤. GLIPì˜ ê²½ìš°ì—ëŠ” box headì™€ regionê³¼ prompt embedding ì‚¬ì´ì˜ projection layerë§Œ fine-tuningí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ "linear probing"ì„ ìˆ˜í–‰í–ˆë‹¤.

3ê°€ì§€ ì„¤ì •(linear probing, prompt tuning, full-model tuning)ì— ëŒ€í•´ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•œ ê²°ê³¼ëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ë‹¤. DyHead-Tì˜ ê²½ìš° linear probingê³¼ ì „ì²´ ëª¨ë¸ tuning ì‚¬ì´ì˜ ê²©ì°¨ê°€ í¬ì§€ë§Œ GLIPì˜ ê²½ìš° prompt tuningì´ ì „ì²´ tuning ê²°ê³¼ì™€ ê±°ì˜ ì¼ì¹˜í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

<br/><br/><br/><br/>
