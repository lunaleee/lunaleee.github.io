---
title: "[ë…¼ë¬¸ ë¦¬ë·°] ControlNet, Adding Conditional Control to Text-to-Image Diffusion Models"
author: lunalee
date: 2024-06-13 20:12:49 +0900
categories: [AI, Paper Review]
tags: [Multi-modal, Diffusion, Generation]
pin: false
math: true
---

<br/><br/>
`ICCV 2023`

- Paper: [https://arxiv.org/abs/2302.05543](https://arxiv.org/abs/2302.05543)
- Git: [https://github.com/lllyasviel/ControlNet](https://github.com/lllyasviel/ControlNet)
<br/><br/><br/>

#### ğŸ“– í•µì‹¬ í›‘ì–´ë³´ê¸° !!

- Text-to-image ìƒì„±ì—ì„œ ë³µì¡í•œ ë ˆì´ì•„ì›ƒì´ë‚˜ í¬ì¦ˆ, ëª¨ì–‘ì„ ì œì–´í•˜ê¸° ìœ„í•´ Canny edges, key point, segmentation map ë“± **ë‹¤ì–‘í•œ ì…ë ¥**ì„ ì‚¬ìš©í•˜ì—¬ diffusion modelì„ ì œì–´í•˜ëŠ” ControlNet êµ¬ì¡° ì œì•ˆ.
- ëŒ€ê·œëª¨ **pre-trained diffusion ëª¨ë¸ì˜ parameterë¥¼ ê³ ì •**í•˜ê³ , **â€œtrainable copyâ€**ë¼ëŠ” ì¶”ê°€ì ì¸ blockì„ ì‚¬ìš©í•´ large ëª¨ë¸ì˜ qualityë‚˜ ëŠ¥ë ¥ì€ ë³´ì¡´í•˜ë©´ì„œë„ ëŒ€ê·œëª¨ pre-trained ëª¨ë¸ì„ ì¬ì‚¬ìš©í•˜ëŠ” íš¨ê³¼ë¥¼ ì–»ìŒ.
- ControlNetì˜ trainable copyì—ì„œ **zero convolution**ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì´ˆê¸°ì— random noiseë¡œë¶€í„° backboneì„ ë³´í˜¸í•œë‹¤.
<br/><br/><br/><br/>

# Introduction

---

Stable Diffusionê³¼ ê°™ì€ text-to-image diffusion ëª¨ë¸ì˜ ë“±ì¥ìœ¼ë¡œ text promptë¥¼ í†µí•œ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ìƒì„±ì´ ê°€ëŠ¥í•´ì¡Œë‹¤. ì´ëŸ¬í•œ ë°©ë²•ë“¤ì€ ì‚¬ì‹¤ì ì´ê³  ë†’ì€ í€„ë¦¬í‹°ì˜ ì´ë¯¸ì§€ë¥¼ ì œê³µí•˜ì§€ë§Œ, ê³µê°„ì ì¸ êµ¬ì„±ì— ëŒ€í•œ ì œì–´ê°€ ì œí•œì ì´ë‹¤. ë³µì¡í•œ ë ˆì´ì•„ì›ƒì´ë‚˜ í¬ì¦ˆ, ëª¨ì–‘ì— ëŒ€í•´ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ì›í•˜ëŠ” ì´ë¯¸ì§€ì™€ ì •í™•í•˜ê²Œ ì¼ì¹˜í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš´ ë¬¸ì œì´ë‹¤. 
<br/><br/>

ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì €ìëŠ” ëŒ€ê·œëª¨ì˜ pre-trainëœ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Stable diffusionì— ëŒ€í•œ ì¡°ê±´ ì œì–´ë¥¼ í•™ìŠµí•˜ëŠ” end-to-end êµ¬ì¡°</span></mark>**ì¸  **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>ControlNet</span></mark>**ì„ ì œì‹œí–ˆë‹¤. í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ëŒ€ê·œëª¨ pre-trained diffusion ëª¨ë¸ì„ backboneìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. Large diffusion ëª¨ë¸ì˜ parameterë¥¼ ê³ ì •í•˜ê³ , ControlNetì˜ encoding layerì—ì„œ trainable copyë¥¼ ë§Œë“œëŠ” ë°©ì‹ì„ ì‚¬ìš©í•´ large ëª¨ë¸ì˜ qualityë‚˜ ëŠ¥ë ¥ì€ ë³´ì¡´í•  ìˆ˜ ìˆë‹¤. ì´ ë•Œ zero convolution layerë¥¼ ë„ì…í•˜ì—¬ í•™ìŠµì´ ì•ˆì •ì ìœ¼ë¡œ ì§„í–‰ë  ìˆ˜ ìˆë„ë¡ í–ˆë‹¤.
<br/><br/>

![ControlNet_1.png](https://github.com/user-attachments/assets/7d75cd0c-5c28-4e51-b954-9d3434b9b2c5){: width="1000px"}

ë…¼ë¬¸ì—ì„œëŠ” ControlNetì„ ì‚¬ìš©í•˜ì—¬ Canny edges, Hough lines, human key point, segmentation map, depth ë“± ë‹¤ì–‘í•œ ì»¨ë””ì…”ë‹ ì…ë ¥ì„ ì‚¬ìš©í•˜ì—¬ stable diffusionì„ ì œì–´í–ˆë‹¤. ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë‹¤ì–‘í•œ ì…ë ¥ì— ëŒ€í•´ ì•ˆì •ì ìœ¼ë¡œ ê²°ê³¼ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. êµ¬ì²´ì ì¸ ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì.
<br/><br/><br/><br/><br/><br/>

# Method

---

## 1. ControlNet

![ControlNet_2.png](https://github.com/user-attachments/assets/37a28738-fa7d-4602-838a-1e8332d49767){: width="600px"}

ControlNetì€ ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ neural network blockì— conditionì„ ì£¼ì…í•œë‹¤. ì—¬ê¸°ì„œ network blockì´ë¼ëŠ” ë‹¨ì–´ëŠ” ResNet block, Trasformer block ë“±ê³¼ ê°™ì€ ì¼ë ¨ì˜ neural layerë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
<br/><br/><br/>

- $\mathcal F(\cdot \ ; \Theta)$ëŠ” parameter $\Theta$ë¥¼ ê°–ëŠ” í•™ìŠµëœ neural blockì´ê³  ì…ë ¥ feature map $x$ë¥¼ $y$ë¡œ ë³€í™˜í•œë‹¤($x \in â„^{h \times w \times d}$, $h$: height, $w$: width, $d$: depth).
    
    $$
    y = \mathcal F(x \ ; \Theta)
    $$
    
- Pre-trained neural blockì— ControlNetì„ ì¶”ê°€í•˜ê¸° ìœ„í•´ blockì˜ parameter  $\Theta$ë¥¼ ê³ ì •(freeze)í•˜ê³ , parameter $\Theta_c$ë¥¼ ê°–ëŠ” **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>trainable copy</span></mark>**ë¡œ blockì„ ë³µì œí•œë‹¤(ê·¸ë¦¼ (b)).
- trainable copyëŠ” external conditioning vector $c$ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤.
- trainable copyëŠ” zero convolution **layer $Z(\cdot \ ; \cdot)$ë¥¼ ì‚¬ìš©í•œë‹¤. zero convolutionì€ weightì™€ biasê°€ ëª¨ë‘ 0ìœ¼ë¡œ ì´ˆê¸°í™”ëœ $1 Ã— 1$ convolution layerì´ë‹¤.
- 2ê°œì˜ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>zero convolution</span></mark>**ì„ ì‚¬ìš©í•˜ëŠ”ë°, ê°ê° paremeter $\Theta_{z1}, \Theta_{z2}$ë¥¼ ê°€ì§ˆ ë•Œ ControlNetì€ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤. 
(ì—¬ê¸°ì„œ $y_c$ëŠ” ControlNetì˜ ì¶œë ¥ì´ë‹¤)
    
    $$
    y_c = \mathcal F(x \ ; \Theta) \ + \ \mathcal Z(\mathcal F(x + \mathcal Z(c \ ; \Theta_{z1}); \Theta_c); \Theta_{z2})
    $$
    
- ì²« ë²ˆì§¸ í•™ìŠµ ë‹¨ê³„ì—ì„œ zero convolutionì´ ëª¨ë‘ 0ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ë¯€ë¡œ ìœ„ ì‹ì˜ $Z(\cdot \ ; \cdot)$í•­ì€ ëª¨ë‘ 0ì´ ëœë‹¤. ë”°ë¼ì„œ <mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>$y_c=y.$</span></mark> ì¦‰, ControlNetì˜ ì¶œë ¥ì€ pre-trained ëª¨ë¸ì˜ ì¶œë ¥ê³¼ ê°™ì•„ì§€ê²Œ ëœë‹¤. ê²°ê³¼ì ìœ¼ë¡œ í•™ìŠµì´ ì‹œì‘ë  ë•Œ ìœ í•´í•œ noiseê°€ trainable copyì˜ hidden layerì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ì—†ë‹¤.
<br/><br/><br/><br/>

ì´ êµ¬ì¡°ê°€ Stable Diffusionê³¼ ê°™ì€ ëŒ€ê·œëª¨ ëª¨ë¸ì— ì ìš©ë˜ì—ˆì„ ë•Œ, ê³ ì •ëœ parameterë¡œ ì¸í•´ ìˆ˜ì‹­ì–µ ê°œì˜ ì´ë¯¸ì§€ë¡œ í•™ìŠµëœ ê¸°ì¡´ ëª¨ë¸ì„ ë³´ì¡´í•˜ëŠ” ë°˜ë©´, trainble copyëŠ” ì´ëŸ¬í•œ ëŒ€ê·œëª¨ pre-trained ëª¨ë¸ì„ ì¬ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì…ë ¥ ì¡°ê±´ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ê°•ë ¥í•œ backboneì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤.
<br/><br/>

íŠ¹íˆ zero convolutionì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì´ˆê¸°ì—ëŠ” trainable copyê°€ ê¸°ëŠ¥ì ìœ¼ë¡œ large, pretrained ëª¨ë¸ì˜ ì—­í• ì„ ìˆ˜í–‰í•˜ë¯€ë¡œ, ê°•ë ¥í•œ backbone ì—­í• ì„ í•  ìˆ˜ ìˆë‹¤. ì´ˆê¸° í•™ìŠµ ë‹¨ê³„ì—ì„œ gradientì˜ random noiseë¥¼ ì œê±°í•˜ì—¬ backboneì„ ë³´í˜¸í•œë‹¤. 
<br/><br/><br/><br/><br/>

## 2. ControlNet for Text-to-Image Diffusion

![ControlNet_3.png](https://github.com/user-attachments/assets/c6ea81e7-1859-42f1-a8e7-3486a60bfe11){: width="500px"}

Stable Diffusionì˜ U-net êµ¬ì¡°ì— ControlNetì„ ì¶”ê°€í•œ êµ¬ì¡°ëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ë‹¤. Encoder, middle block, skip-connected decoderë¡œ ì´ë£¨ì–´ì§„ U-netì˜ Encoder levelì— ControlNetì´ ì ìš©ëœë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ì´ 12ê°œì˜ encoding blockê³¼ 1ê°œì˜ middle blockì˜ trainable copyê°€ ìƒì„±ëœë‹¤. Outputì€ 12ê°œì˜ skip-connectionê³¼ 1ê°œì˜ middle blockì— ë”í•´ì§„ë‹¤(ê·¸ë¦¼(b) ì°¸ì¡°). 

Text Encoderë¡œëŠ” CLIPì„ ì‚¬ìš©í–ˆë‹¤. Stable Diffusionì€ ì¼ë°˜ì ì¸ U-net êµ¬ì¡°ì´ë¯€ë¡œ ë‹¤ë¥¸ ëª¨ë¸ì—ë„ ì‰½ê²Œ ì ìš©ì´ ê°€ëŠ¥í•˜ë‹¤. 
<br/><br/>

Stable Diffusionì€ VQ-GANê³¼ ìœ ì‚¬í•œ ì „ì²˜ë¦¬ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ 512Ã—512 pixel space ì´ë¯¸ì§€ë¥¼ 64Ã—64 latent ì´ë¯¸ì§€ë¡œ ë³€í™˜í•œë‹¤. ë”°ë¼ì„œ ControlNetì—ì„œë„ Stable Diffusionê³¼ ë§ì¶”ê¸° ìœ„í•´ 64Ã—64 feature space vectorë¡œ ë³€í™˜í•´ì•¼í•œë‹¤. ì´ë¥¼ ìœ„í•´ tiny network $\mathcal E(\cdot)$ë¥¼ ì‚¬ìš©í•˜ì—¬ image-space condition $c_{\text i}$ë¥¼ feature space conditioning vector $c_{\text f}$ë¡œ ì¸ì½”ë”©í–ˆë‹¤.

$$
c_{\text f} = \mathcal E(c_{\text i})
$$

<br/><br/><br/><br/>

## 3. Training

í•™ìŠµ ê³¼ì •ì€ ì¼ë°˜ì ì¸ Diffusion ëª¨ë¸ì˜ denoising processì™€ ë™ì¼í•˜ë‹¤. ì´ ë¶€ë¶„ì€ [Stable Diffusion ë…¼ë¬¸ ë¦¬ë·°](https://lunaleee.github.io/posts/stablediffusion/)ë¥¼ ì‚´í´ë³´ì.

í•™ìŠµ ê³¼ì •ì—ì„œ text prompt $c_t$ì˜ 50%ë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ë¬´ì‘ìœ„ë¡œ ë°”ê¿¨ë‹¤. ì´ ë°©ì‹ì€ ì…ë ¥ conditioning ì´ë¯¸ì§€(ì˜ˆ: edges, poses, depth ë“±)ì—ì„œ ì˜ë¯¸ë¥¼ ì§ì ‘ ì¸ì‹í•˜ëŠ” ControlNetì˜ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤. 
<br/><br/>

![ControlNet_4.png](https://github.com/user-attachments/assets/10e9af08-217c-41dc-9a2c-4e0ecc00ea45){: width="500px"}

ë…¼ë¬¸ì˜ ë°©ì‹ì€ ëª¨ë¸ì´ control conditionì„ ì ì§„ì ìœ¼ë¡œ í•™ìŠµí•˜ì§€ ì•Šê³  ìœ„ì˜ ê·¸ë¦¼ì˜ 6133 stepì—ì„œì™€ ê°™ì´ ê°‘ìê¸° conditioning ì´ë¯¸ì§€ë¥¼ ë”°ë¼ê°€ëŠ” í˜„ìƒì´ ë°œìƒí•œë‹¤. ì´ë¥¼ **â€œsudden convergence phenomenonâ€**ì´ë¼ê³  ì§€ì¹­í–ˆë‹¤.
<br/><br/><br/><br/><br/>

## 4. Inference

![ControlNet_5.png](https://github.com/user-attachments/assets/970f46ab-e02d-41e1-8ee5-fbe692c78ea8){: width="500px"}

ì…ë ¥ìœ¼ë¡œ ì—¬ëŸ¬ê°œì˜ conditioning ì´ë¯¸ì§€(e.g. Canny edge, pose)ë¥¼ ì ìš©í•˜ê¸° ìœ„í•´ ControlNetì˜ í•´ë‹¹ outputì„ ë°”ë¡œ Stable Diffusionì— ì¶”ê°€í•´ì¤„ ìˆ˜ ìˆë‹¤. ì¶”ê°€ì ì¸ weightì´ë‚˜ linear interpolationì—†ì´ ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì—¬ëŸ¬ê°œì˜ conditionì„ ì¶©ì¡±í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.
<br/><br/><br/><br/><br/><br/>

# Experiments

---

### 1.  Qualitative Results

![ControlNet_6.png](https://github.com/user-attachments/assets/fbc3558a-7028-418c-958b-752f30ebad22){: width="800px"}

ìœ„ì˜ ê·¸ë¦¼ì€ prompt ì—†ì´ ë‹¤ì–‘í•œ conditionì— ìƒì„±ëœ ê²°ê³¼ì´ë‹¤.
<br/><br/><br/><br/><br/>

### 2. Ablative Study

ë³¸ ë…¼ë¬¸ì˜ êµ¬ì¡°ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•´ ì•„ë˜ì˜ ë‘ê°€ì§€ ì¡°ê±´ìœ¼ë¡œ êµ¬ì¡°ë¥¼ ë³€ê²½í•˜ì—¬ ì‹¤í—˜ì„ ì§„í–‰í–ˆë‹¤.

1.  zero convolutionì„ gaussian weightë¡œ ì´ˆê¸°í™”ëœ standard convolution layerë¡œ ëŒ€ì²´
2. ê° blockì˜ trainable copyë¥¼ ControlNet-liteë¼ê³  í•˜ëŠ” ë‹¨ì¼ convolution layerë¡œ ëŒ€ì²´
<br/><br/><br/>

ë˜í•œ 4ê°€ì§€ prompt ì„¤ì •ì— ëŒ€í•´ ì‹¤í—˜í–ˆë‹¤.

1. prompt ì—†ìŒ
2. conditioning ì´ë¯¸ì§€ì˜ ê°ì²´ë¥¼ ì™„ì „íˆ í¬í•¨í•˜ì§€ ì•ŠëŠ” ë¶ˆì¶©ë¶„í•œ prompt (e.g. â€œa high-quality, detailed, and professional imageâ€)
3. conditioning ì´ë¯¸ì§€ì˜ ì˜ë¯¸ì™€ ìƒì¶©ë˜ëŠ” prompt
4. í•„ìš”í•œ content ì˜ë¯¸ë¥¼ ì„¤ëª…í•˜ëŠ” ì™„ë²½í•œ prompt (e.g. "a nice house")
<br/><br/>

ì•„ë˜ì˜ ê·¸ë¦¼ì€ 4ê°€ì§€ prompt ì„¤ì •ì— ëŒ€í•´ ëª¨ë‘ ì„±ê³µì ì¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤. 

![ControlNet_7.png](https://github.com/user-attachments/assets/48f9d99e-8966-4732-b020-9e103330d171){: width="900px"}
<br/><br/><br/><br/><br/>

### 3. Quantitative Evaluation

![ControlNet_8.png](https://github.com/user-attachments/assets/b4d9f829-5fe9-4a46-895f-59c006f595c7){: width="400px"}

20ê°œì˜ ì†ìœ¼ë¡œ ê·¸ë¦° ìŠ¤ì¼€ì¹˜ì— ëŒ€í•´ 5ê°œì˜ ë°©ë²•ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í–ˆë‹¤. 12ëª…ì˜ ì‚¬ìš©ìì—ê²Œ "í‘œì‹œëœ ì´ë¯¸ì§€ì˜ í’ˆì§ˆ"ê³¼ "ìŠ¤ì¼€ì¹˜ì˜ ì¶©ì‹¤ë„" ì¸¡ë©´ì—ì„œ ê°œë³„ì ìœ¼ë¡œ ìˆœìœ„ë¥¼ ë§¤ê¸°ë„ë¡ ìš”ì²­í–ˆë‹¤.  ì‚¬ìš©ìê°€ ê° ê²°ê³¼ë¥¼ 1~5ì  ì²™ë„ë¡œ í‰ê°€í•˜ëŠ” Average Human Ranking(AHR)ë¥¼ ì‚¬ìš©í–ˆë‹¤. ê²°ê³¼ëŠ” ìœ„ í‘œì™€ ê°™ë‹¤.
<br/><br/><br/>

![ControlNet_9.png](https://github.com/user-attachments/assets/86d00ddf-3cf5-465c-a7f2-6199459542c0){: width="500px"}

![ControlNet_10.png](https://github.com/user-attachments/assets/2c4c10b0-38f0-47b6-9e85-e895319b33c3){: width="500px"}

ADE20Kì˜ testsetì„ ì‚¬ìš©í•˜ì—¬ conditioning ì¶©ì‹¤ë„ë¥¼ í‰ê°€í–ˆë‹¤. SOTA segmentation methodì¸ OneFormerë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ì´ë¯¸ì§€ì— ëŒ€í•´ segmentationì„ ìˆ˜í–‰í•˜ê³  IoUë¥¼ ê³„ì‚°í–ˆë‹¤.

FIDë¥¼ ì‚¬ìš©í•˜ì—¬ distribution distanceë¥¼ ì¸¡ì •í–ˆë‹¤. ë˜í•œ text-image CLIP scoreì™€ CLIP aesthetic scoreë¥¼ ì¸¡ì •í–ˆë‹¤. ê²°ê³¼ëŠ” ìœ„ ë‘ ê°œ í‘œì™€ ê°™ë‹¤.
<br/><br/><br/><br/><br/>

### 4. Comparison to Previous Methods

![ControlNet_11.png](https://github.com/user-attachments/assets/2be8878c-faa5-459c-859e-20470753da01){: width="450px"}

ìœ„ì˜ ê·¸ë¦¼ì€ baselineê³¼ ë…¼ë¬¸ì˜ ë°©ë²•(Stable Diffusion + ControlNet)ì˜ ì‹œê°ì  ë¹„êµë¥¼ ë³´ì—¬ì¤€ë‹¤. ControlNetì€ ë‹¤ì–‘í•œ conditioning ì´ë¯¸ì§€ë¥¼ ê²¬ê³ í•˜ê²Œ ì²˜ë¦¬í•˜ê³  ì„ ëª…í•˜ê³  ê¹¨ë—í•œ ê²°ê³¼ë¥¼ ìƒì„±í•œë‹¤.
<br/><br/><br/><br/>
