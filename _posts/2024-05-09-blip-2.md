---
title: "[논문 리뷰] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"
author: lunalee
date: 2024-05-09 23:21:45 +0900
categories: [AI, Paper Review]
tags: [Multi-modal, Diffusion, Generation]
pin: false
math: true
---

<br/><br/>
`Salesforce Research` `ICML 2023`

- Paper: [https://arxiv.org/abs/2301.12597](https://arxiv.org/abs/2301.12597)
- Git: [https://github.com/salesforce/LAVIS/tree/main/projects/blip2](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)
<br/><br/><br/>

#### 📖 핵심 훑어보기 !!

- Vision 모델과 Language 모델 각각의 unimodal을 활용하여 multi-modal 작업(VLP)을 수행하기 위한 방법 제안
- Frozen image encoder와 frozen LLM을 활용하기 위해, 두 modality의 gap을 해소하기 위한 Q-Former 구조 제안
- Q-Former를 학습하기 위한 2 stage pre-train 방법 제안
    1. Frozen image encoder를 사용하는 vision-language **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>representation learning stage</span></mark>**
    2. Frozen LLM을 사용하는 vision-to-language **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>generative learning stage</span></mark>**
<br/><br/><br/><br/>

# Introduction

---

Vision-language pre-training (VLP) 연구에서 점점 더 큰 규모의 pre-train 모델이 개발되고 지속적으로 발전을 이루고 있지만, 대부분 대규모 모델과 데이터셋을 사용한 end-to-end 학습으로 인한 큰 계산 비용이 발생한다. 

논문에서는 pre-train된 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Vision 모델과 Language 모델을 bootstrapping하여 효율적인 VLP 방법</span></mark>**을 제안한다. 계산 비용을 줄이고 catastrophic forgetting(망각) 문제를 피하기 위해 각각의 unimodal pre-train 모델은 frozen된 채로 유지한다. 
<br/><br/><br/>

이와 같이 VLP 작업에 pre-train된 unimodal을 활용하기 위해서는 **cross-modal alignment**가 매우 중요하다. 그러나 LLM은 unimodal pre-training 중에 이미지를 보지 못했으므로 모델을 frozen하게 되면 alignment가 어려워진다. 따라서 modality gap을 충분히 연결해주는 작업이 필요하다. 

논문에서는 frozen unimodal 모델로 효과적인 vision-language alignment을 달성하기 위해 새로운 2단계 방법으로 pre-train된 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Querying Transformer(Q-Former)</span></mark>**를 제안했다. Q-Former는 frozen image encoder와 frozen LLM 사이의 **information bottleneck** 역할을 하며, LLM이 원하는 텍스트를 출력하는 데 가장 유용한 visual feature을 제공한다.

![BLIP-2_1.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/53e79c3e-6d58-4224-8c9d-7b5e90e9a2b8){: width="600px"}
<br/><br/><br/>

BLIP-2의 주요 장점은 다음과 같다.

- BLIP-2는 frozen pre-trained 이미지 모델과 언어 모델을 모두 효과적으로 활용한다. Representation learning stage, generative learning stage의 두 단계로 pre-train된 Q-Former를 사용하여 modality gap을 해소했다.
- FlanT5 LLM을 기반으로 하는 BLIP-2는 자연어 지침에 따라 zero-shot image-to-text generation이 가능하다. 이를 통해 다양한 시각적 추론 및 대화가 가능하다(아래 그림 예시).
- Frozen unimodal 모델과 경량 Q-Former를 사용하므로 BLIP-2는 기존 최첨단 모델보다 컴퓨팅 효율성이 더 높다.
    
    ![BLIP-2_2.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/ba214ecb-f60a-440a-ba44-22107d190130){: width="1300px"}
    
<br/><br/><br/><br/><br/><br/>

# Method

---

BLIP-2는 frozen pre-train unimodal 모델에서 bootstrap을 수행하는 vision-language pre-training 방법이다. Modality gap을 해소하기 위해 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>Querying Transformer(Q-Former)</span></mark>**을 제안한다. Q-Former는 두 단계를 거쳐 pre-train된다.

1. Frozen image encoder를 사용하는 vision-language **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>representation learning stage</span></mark>**
2. Frozen LLM을 사용하는 vision-to-language **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>generative learning stage</span></mark>**
<br/><br/><br/><br/>

## 1. Model Architecture

![BLIP-2_3.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/bd6cabf8-4d34-444d-a2b6-31bd88a1b80d){: width="700px"}

앞서 언급했듯 Q-Former는 frozen image encoder와 frozen LLM 사이의 gap을 해소하기 위한 trainable module이다.  위 그림이 Q-Former의 전반적인 구조를 나타내고 있다. 특징은 다음과 같다.
<br/><br/>

- Image encoder에서는 입력 이미지 resolution과 상관 없이 고정된 크기의 output feature를 추출한다.
- Q-Former는 크게 두 개의 transformer submodule로 구성되어 있으며, submodule에서 **self-attention layer는 공유**한다(동일한 self-attnetion layer 사용).
    1. Image transformer: visual feature 추출을 위해 frozen image encoder와 상호작용함
    2. Text transformer: text encoder와 text decoder 두 가지로 기능할 수 있음
<br/><br/>

- Image transformer의 입력으로는 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>learnable query embedding 집합</span></mark>**을 생성한다. query들은 image transformer에 들어간 뒤 **self-attention layer**를 통해 query 끼리 서로 상호작용하고, **cross-attnetion layer**를 통해 frozen image feature와 상호작용한다.
- 여기서, self-attention layer에서 query 끼리 서로 상호작용 뿐 아니라 **Text와도 추가로 상호작용**할 수 있다.
- Text와 상호작용 할 때, 학습중인 task에 따라 다양한 **self-attention mask를 적용**하여 query-text 상호작용을 제어할 수 있다.
- Q-Former는 BERTbase의 weight으로 초기화되는데, 여기서 cross-attention layer는 randomly 초기화 된다.
<br/><br/>

<details>
  <summary><b>Learnable Query Embdding에 대한 자세한 추가 설명(해석)</b></summary>
  <div markdown="1">
  - Learned Queries는 **이미지 특징(Visual Features)을 텍스트 특징(Language Features)으로 변환하는 데 필요한 중간 매개체** 역할을 한다. 구체적으로, Q-Former 내에서 각 쿼리 벡터는 ViT로부터 나온 이미지 특징 중에서 **텍스트 정보와 일치할 수 있는 정보**를 추출하는 데 사용된다(Visual Feature을 Language Features로 변환하는 데 필요한 중간 매개체 역할).
  - **N개의 학습 가능한 쿼리 벡터**로 구성되어 있으며, 이 벡터들은 트랜스포머의 쿼리 부분에서 입력으로 사용된다.
  - 이 쿼리 벡터들은 Q-Former의 pre-training 및 fine-tuning 과정에서 최적화되며, Q-Former가 이미지에서 **어떤 정보를 추출하고자 하는지**를 결정하게된다.
  - **Learned Queries** 벡터들은 Q-Former 내부에서 **self-attention**을 통해 서로 간의 관계를 학습한다. 즉, ViT에서 추출된 이미지 특징과는 상호작용하지 않고, 각 쿼리가 현재 상태에서 **다른 쿼리들과 어떻게 상호작용할지를 결정한다.**
  - **Cross-Attention Layer**에서는 Learned Queries가 각 패치에서 어떤 정보를 가져올지 결정한다. 구체적으로, 쿼리(Query)는 Q-Former의 Learned Queries에서, 키(Key)와 값(Value)은 ViT의 이미지 feature에서 생성된다. 이 과정에서, Q-Former의 각 쿼리 벡터는 ViT에서 추출된 패치 특징 중 **자신에게 필요한 정보**를 선택적으로 강조하고, 나머지 특징은 무시한다. 예를 들어, 특정 쿼리 벡터는 이미지 내 특정 객체(예: 사람, 동물)에 대한 정보를 강조하고, 다른 쿼리 벡터는 배경이나 주변 환경에 대한 정보를 가져올 수 있다.
  - **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>인코더 최적화된 Learned Queries</span></mark>**: **Q-Former의 Learned Queries**는 특정 이미지 인코더와의 조합에 최적화되어 있으므로, 다른 이미지 인코더를 사용할 경우 해당 인코더의 특징 추출 방식에 맞춰 **Q-Former를 다시 학습**해야한다.
  - Query 집합은 이미지 Transformer를 거쳐, 각 Query당 하나의 Output embedding으로 생성된다.(이 output 집합을 Z라고 표현했다)
  </div>
</details>
<br/><br/>

Input query의 output(Z라고 표현함)은 frozen image feature보다 훨씬 작은 크기를 가지며, 이러한 bottleneck 구조는 query가 text와 가장 관련성이 높은 시각적 정보를 추출하도록 하는 효과가 있다.
<br/><br/><br/><br/><br/><br/>

## 2. Bootstrap Vision-Language Representation Learning from a Frozen Image Encoder

앞서 언급한 Q-Former의 두 단계 학습 방법 중 1단계인 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>representation learning stage</span></mark>**에 대해 먼저 살펴보자. 
<br/><br/>

Representation learning stage에서는 Q-Former를 frozen image encoder에 연결하고 image-text pair를 사용하여 pre-train을 수행한다. 학습의 목적은 **query가 visual representation을 추출하는 방법을 학습**하는 것인데, 이 때 이 visual representaion은  **text와 연관지어지는 유익한 정보**여야한다.

BLIP 논문([BLIP 논문 리뷰 참조](https://lunaleee.github.io/posts/blip/))의 방법과 유사하게 세가지 objective를 jointly optimize한다. Query-text 간의 상호작용을 조절하기 위해 각각의 objective는 서로 다른 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>attention masking strategy</span></mark>**를 적용한다. (Cross attention layer에 적용됨)

![BLIP-2_4.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/95d57a08-df46-4942-b680-5712114ae880){: width="650px"}

각각 Mask에 대한 설명

- Bi-direntional Self-Attention Mask: 모든 Query와 Text token이 서로 attend 할 수 있다.
- Multi-modal Causal Self-Attention Mask: Query는 모든 query에 대해 서로 attend 할 수 있지만 text에는 attend 할 수 없다. 반면 Text 모든 query에 attend 할 수 있고, 이전 시점까지의 text에 attend 할 수 있다.
- Uni-modal Self-Attention Mask: Query와 text는 서로에게 attend 할 수 없다.
<br/><br/><br/><br/>

#### Image-Text Contrastive Learning (ITC)

ITC는 image representation과 text representation을 align 할 때 **mutual information이 최대가 되도록 학습**을 진행한다. 이를 위해 positive pair와 negative pair의 **image-text similarity를 비교**하는 **Contrastive learning** 방법으로 학습한다.
<br/><br/>

- 먼저 image transformer의 output query representation Z를 text transformer의 text representation t, 즉 [CLS] token의 output embedding과 align 한다.
- Z에는 여러 개의 output embedding(여러 개의 query, 각 query 당 하나씩 output)이 포함되어 있는데, 각각 **query output과 t 간의 pairwise similarity**를 구하고 이 중 가장 높은 것을 image-text similarity로 선택한다.
- Attention 과정에서 query와 text가 서로의 정보를 직접 보게 되면 정보를 미리 유출하는 것이 되므로 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>unimodal self-attention mask</span></mark>**를 사용한다.
- Frozen image encoder를 사용하므로 end-to-end 방법에 비해 GPU당 더 많은 샘플을 batch에 사용할 수 있다. 기존에 BLIP에서 사용한 momentum queue 방식 대신 in-batch negatives를 사용했다.<br/><br/>
  <details>
  <summary><b>BLIP의 momentum queue??</b></summary>
  <div markdown="1">
> BLIP에서는 positive pair에 대해서 weakly-correlated 인 경우, negative text임에도 image와 매칭되는 등 **noisy한 web 데이터를 처리**하기 위해 momentum 모델을 사용한다. Momentum 모델을 통해 pseudo-target을 만들고, target에 대해 **soft label**을 생성함으로서 기존 ont-hot label의 문제점을 보완한다.<br/><br/>
> BLIP-2에서는 GPU 당 더 많은 샘플을  batch에 사용할 수 있으므로 **in-batch negative**를 사용한다. In-batch negatives 란, random하게 batch의 요소들을 구성하고 batch 요소들 중 특정 query에 해당하는 text 이외에 나머지를 negative 관계로 보고 학습하는 것이다. Batch 내 샘플 수가 많으므로 noise에 덜 취약할 수 있다.
  </div>
  </details>
 
<br/><br/><br/>

#### Image-grounded Text Generation (ITG)

ITG는 input image가 condition으로 주어졌을 때, Q-Former가 text를 생성(generation)하도록 학습한다. 

Q-Former는 frozen image encoder와 text token간의 직접적인 상호작용을 허용하지 않으므로, text 생성에 필요한 정보는 query를 통해 추출된 다음 Self-attention layer를 통해 text token에 전달되어야 한다. 따라서 query는 text에 대한 모든 정보를 캡쳐하는 visual feature을 추출해야 한다. 

Query-text 상호작용을 제어하기 위해 [UniLM 논문📄](https://arxiv.org/abs/1905.03197)에서와 유사한 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>multimodal causal self-attention mask</span></mark>**를 사용한다. 위의 그림과 같이 query는 query끼리 서로 attend 할 수 있지만, text token에는 attend 할 수 없다. 반면에 text token은 모든 query에 attend 할 수 있고, 이전 text token에 attend 할 수 있다.

또한 decoding task를 알리기 위해서는 첫 번째 text token을 [CLS] token에서 [DEC] token으로 대체한다.
<br/><br/><br/><br/>

#### Image-Text Matching (ITM)

ITM은 image와 text간의 fine-grained alignment를 학습하는 것을 목표로 한다. BLIP과 마찬가지로 image-text pair가 positive인지, negative인지 모델이 예측하게 하는 **binary classification task**이다. 여기서는 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>bi-directional self-attention mask</span></mark>**를 사용하여 모든 query와 text가 서로 attend 할 수 있다. 따라서 output query embedding Z는 multimodal information을 캡처한다. 

각각의 output query embedding을 two-class linear classifier에 넣어 logit을 구한 뒤, 전체 query들의 **logit들을 average**하여 **output matching score**를 구한다. 또한 informative negative pair를 선정하기 위해 hard negative mining* 전략을 사용했다. 

> **Hard negative mining***<br/>
Hard negative한 데이터는 모델이 예측하기 어려운 sample(실제로는 negative→positive로 예측)을 의미한다. Hard negative mining 기법은 이러한 모델이 맞추기 어려운 데이터를 추출하여 모델이 학습하게 하여 모델이 False Positive 오류에 강인해지도록 학습하는 방법을 의미한다.
> 

<br/><br/><br/><br/><br/>

## 3. Bootstrap Vision-to-Language Generative Learning from a Frozen LLM

![BLIP-2_5.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/bdd002a9-c6dd-4d53-8b92-865bc7a5edd4){: width="1200px"}

다음으로 Q-Former의 두 단계 학습 방법 중 2단계인 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>generative learning stage</span></mark>**에 대해 살펴보자.
<br/><br/>

- Generative learning stage에서는Q-Former(frozen image encoder가 연결된)를 frozen LLM에 연결하여 LLM의 generative language capability를 수집한다.
- 위의 그림과 같이 **fully-connected(FC) layer**를 사용하여 output query embedding Z를 LLM의 text embedding과 동일한 차원으로 linearly project 한다.
- 그 뒤에 projection된 query embedding을 input text embedding 앞에 추가한다. 추가된 projected query embedding은 Q-Former에서 추출한 visual representation에 따라 LLM에 조건을 부여하는 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>soft visual prompt</span></mark>** 역할을 한다.
- Q-Former는 language-informative visual representation을 추출하도록 pre-train되었기 때문에 관련 없는 visual 정보를 제거하면서 LLM에 가장 유용한 정보를 제공하는 information bottleneck으로 효과적으로 기능한다.
<br/><br/><br/><br/>

저자는 두 가지 유형의 LLM에 대해 실험을 진행했다. 

1. Decoder-based LLMs: frozen LLM이 Q-Former의 visual representation에 따라 text를 생성하도록 하는 **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>language modeling loss</span></mark>**를 사용하여 pre-train 진행.
2. Encoder-decoder-based LLMs: **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>prefix language modeling loss</span></mark>**를 사용하여 pre-train 함. 여기서 text를 두 부분으로 나눈다. Prefix text는 visual representation과 concat 되어 **LLM의 encoder의 입력**으로 사용되고, suffix text는 LLM decoder의 **generation target**으로 사용된다.
<br/><br/><br/><br/><br/>

## 4. Model Pre-training

#### Pre-trainig data.

- Pre-training 데이터셋으로 BLIP과 동일한 데이터셋을 사용했다. 총 129M의 이미지(COCO, Visual Genome, CC3M, CC12M, SBU, LAION400M 일부)를 사용했다.
- 또한 BLIP의 web 이미지에 대한 합성 caption을 생성하기 위해 **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>CapFilt 방법</span></mark>**을 사용했다. CapFilt 방법은 [BLIP 논문 리뷰](https://lunaleee.github.io/posts/blip/)를 참조하자.
<br/><br/><br/>

#### Pre-trained image encoder and LLM.

- Frozen image encoder의 경우 ViT-L/14(from CLIP), ViT-g/14(from EVA-CLIP) 두 가지를 사용하며 마지막 layer를 제거하고 뒤에서 두 번째 layer의 output feature를 사용한다.
- Frozen language model의 경우, decoder-based LLM으로는 unsupervised-trained OPT model family를 사용하고, encoder-decoder-based LLM으로는 instruction-trained FlanT5 model family를 사용한다.
<br/><br/><br/><br/><br/><br/>

# Experiments

---

표 1은 다양한 zero-shot vision-language task에 대한 BLIP-2 성능을 나타내고 있다. 이전 SOTA 모델과 비교하여 BLIP-2는 parameter 수가 훨씬 적으면서 향상된 성능을 달성했다.

![BLIP-2_6.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/6877fa0f-545d-4995-b5e1-057685a457a7){: width="1000px"}
<br/><br/>

### 1. Instructed Zero-shot Image-to-Text Generation

BLIP-2를 사용하면 LLM이 text prompt 내용을 유지하면서 이미지를 효과적으로 이해할 수 있으므로, LLM에 대한 입력으로 visual prompt 뒤에 text prompt를 추가하여 image-to-text generation을 제어할 수 있다. 아래 그림은 다양한 zero-shot image-to-text capability에 대한 예시이다.

![BLIP-2_7.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/3830106c-132c-41e8-83a3-d38c8907f082){: width="700px"}
<br/><br/>

#### Zero-shot VQA.

먼저 zero-shot visual question answering task에 대한 정량평가를 수행했다. OPT 모델의 경우 "Question: {} Answer:” prompt를 사용했고, FlanT5 모델의 경우 “Question: {} Short answer:”  prompt를 사용했다. 생성 중 width 5의 beam search를 사용했고 length-penalty를 주어 짧은 답변을 생성하도록 했다.

결과는 아래 표와 같다. BLIP-2가 VQAv2 데이터셋을 사용했을 때, parameter 수가 54배 적음에도 불구하고 SOTA를 달성한 것을 볼 수 있다. 결과를 통해 중요한 결과, 즉 더 **강력한 image encoder나 LLM이 모두 더 나은 성능을 가져온다**는 것을 증명했다고 한다.

![BLIP-2_8.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/a3ad6e38-e6c7-46df-bf50-79d43c332626){: width="700px"}
<br/><br/><br/>

첫 번째 pre-training 단계 representation learning stage에서 Q-Former를 pre-train하여 text와 관련된 visual feature를 학습하므로 LLM이 vision-language alignment을 학습해야하는 부담이 줄어든다. 아래 그림은 generative learning에 대한 representation learning의 효과를 보여준다. Representation learning이 없으면 두 LLM 모두 zero-shot VQA에서 낮은 성능을 보인다.

![BLIP-2_9.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/08e74054-a809-44b6-b68c-7aff1b8cef03){: width="550px"}
<br/><br/><br/><br/>

### 2. Image Captioning

BLIP-2 모델을 fine-tuning하여 이미지의 visual content에 대한 text description을 생성하도록 요청하는 **image captioning task**를 수행했다. LLM을 frozen 상태로 유지하고 image encoder와 함께 Q-Former의 parameter를 업데이트했다. COCO에 대해 fine-tuning을 수행하고 COCO testset과 NoCaps validation set으로의 zero-shot transfer를 모두 평가했다. 결과는 아래 표와 같다. BLIP-2는 SOTA 성능을 달성하여 out-domain image에 대한 강력한 generalization 능력을 보여주고 있다.

![BLIP-2_10.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/ae21cffd-cc97-412a-b3cc-aef8ab22d8c8){: width="1100px"}
<br/><br/><br/><br/>

### 3. Visual Question Answering

Annotated VQA 데이터가 주어지는 경우, LLM을 frozen 상태로 유지하면서 Q-Former와 image encoder를 fine-tuning했다. LLM이 Q-Former의 출력과 질문을 입력으로 받고 답변을 생성하도록 요청받는 **open-ended answer generation loss**를 사용하여 fine-tuning을 진행했다. 

질문과 더 관련성이 높은 이미지 feature를 추출하기 위해 질문에 대해 Q-Former에 추가적인 condition을 걸었다. 구체적으로, question token은 Q-Former에 대한 입력으로 들어가고 self-attention 레이어를 통해 query들과 상호 작용한다. 이를 통해 Q-Former의 cross-attention layer가 더 많은 정보를 제공하는 image region에 집중하도록 유도한다. 결과는 아래 표와 같다.

![BLIP-2_11.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/d79be056-3d31-4b11-ab43-83a23b116906){: width="500px"}
<br/><br/><br/><br/>

### 4. Image-Text Retrieval

image-text retrieval을 위해(language generation이 필요하지 않음), LLM 없이 pre-train된 first-stage 모델을 바로 fine-tuning 했다. 특히 pre-train과 동일한 objective(ITC, ITM 및 ITG)를 사용하여 Q-Former와 image encoder를 COCO 데이터셋으로 pre-train 했다. 그 다음 COCO 및 Flickr30K 데이터셋에 대해 image-to-text retrieval 및 text-to-image retrieval에 대해 모델을 평가했다. 결과는 아래 표와 같다.

![BLIP-2_12.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/d4c91074-0213-4425-93f4-82d087a24453){: width="1100px"}
<br/><br/><br/>

ITC 및 ITM loss는 image-text similarity을 직접 학습하므로 image-text retrieval에 필수적이다. 표 6에서는 ITG(image-grounded text generation) loss가 image-text retrieval에도 유용하다는 것을 보여준다. ITG loss는 text와 가장 관련성이 높은 visual feature를 추출하도록 query를 학습하여 vision-language alignment을 향상시킨다.

![BLIP-2_13.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/7a854202-0c9e-49d0-a0aa-6ac712973054){: width="500px"}

<br/><br/><br/><br/>
