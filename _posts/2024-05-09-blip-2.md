---
title: "[ë…¼ë¬¸ ë¦¬ë·°] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"
author: lunalee
date: 2024-05-09 23:21:45 +0900
categories: [AI, Paper Review]
tags: [Multi-modal, Diffusion, Generation]
pin: false
math: true
---

<br/><br/>
`Salesforce Research` `ICML 2023`

- Paper: [https://arxiv.org/abs/2301.12597](https://arxiv.org/abs/2301.12597)
- Git: [https://github.com/salesforce/LAVIS/tree/main/projects/blip2](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)
<br/><br/><br/>

#### ğŸ“– í•µì‹¬ í›‘ì–´ë³´ê¸° !!

- Vision ëª¨ë¸ê³¼ Language ëª¨ë¸ ê°ê°ì˜ unimodalì„ í™œìš©í•˜ì—¬ multi-modal ì‘ì—…(VLP)ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë°©ë²• ì œì•ˆ
- Frozen image encoderì™€ frozen LLMì„ í™œìš©í•˜ê¸° ìœ„í•´, ë‘ modalityì˜ gapì„ í•´ì†Œí•˜ê¸° ìœ„í•œ Q-Former êµ¬ì¡° ì œì•ˆ
- Q-Formerë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•œ 2 stage pre-train ë°©ë²• ì œì•ˆ
    1. Frozen image encoderë¥¼ ì‚¬ìš©í•˜ëŠ” vision-language **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>representation learning stage</span></mark>**
    2. Frozen LLMì„ ì‚¬ìš©í•˜ëŠ” vision-to-language **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>generative learning stage</span></mark>**
<br/><br/><br/><br/>

# Introduction

---

Vision-language pre-training (VLP) ì—°êµ¬ì—ì„œ ì ì  ë” í° ê·œëª¨ì˜ pre-train ëª¨ë¸ì´ ê°œë°œë˜ê³  ì§€ì†ì ìœ¼ë¡œ ë°œì „ì„ ì´ë£¨ê³  ìˆì§€ë§Œ, ëŒ€ë¶€ë¶„ ëŒ€ê·œëª¨ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ end-to-end í•™ìŠµìœ¼ë¡œ ì¸í•œ í° ê³„ì‚° ë¹„ìš©ì´ ë°œìƒí•œë‹¤. 

ë…¼ë¬¸ì—ì„œëŠ” pre-trainëœ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Vision ëª¨ë¸ê³¼ Language ëª¨ë¸ì„ bootstrappingí•˜ì—¬ íš¨ìœ¨ì ì¸ VLP ë°©ë²•</span></mark>**ì„ ì œì•ˆí•œë‹¤. ê³„ì‚° ë¹„ìš©ì„ ì¤„ì´ê³  catastrophic forgetting(ë§ê°) ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ ê°ê°ì˜ unimodal pre-train ëª¨ë¸ì€ frozenëœ ì±„ë¡œ ìœ ì§€í•œë‹¤. 
<br/><br/><br/>

ì´ì™€ ê°™ì´ VLP ì‘ì—…ì— pre-trainëœ unimodalì„ í™œìš©í•˜ê¸° ìœ„í•´ì„œëŠ” **cross-modal alignment**ê°€ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤. ê·¸ëŸ¬ë‚˜ LLMì€ unimodal pre-training ì¤‘ì— ì´ë¯¸ì§€ë¥¼ ë³´ì§€ ëª»í–ˆìœ¼ë¯€ë¡œ ëª¨ë¸ì„ frození•˜ê²Œ ë˜ë©´ alignmentê°€ ì–´ë ¤ì›Œì§„ë‹¤. ë”°ë¼ì„œ modality gapì„ ì¶©ë¶„íˆ ì—°ê²°í•´ì£¼ëŠ” ì‘ì—…ì´ í•„ìš”í•˜ë‹¤. 

ë…¼ë¬¸ì—ì„œëŠ” frozen unimodal ëª¨ë¸ë¡œ íš¨ê³¼ì ì¸ vision-language alignmentì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ 2ë‹¨ê³„ ë°©ë²•ìœ¼ë¡œ pre-trainëœ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>Querying Transformer(Q-Former)</span></mark>**ë¥¼ ì œì•ˆí–ˆë‹¤. Q-FormerëŠ” frozen image encoderì™€ frozen LLM ì‚¬ì´ì˜ **information bottleneck** ì—­í• ì„ í•˜ë©°, LLMì´ ì›í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•˜ëŠ” ë° ê°€ì¥ ìœ ìš©í•œ visual featureì„ ì œê³µí•œë‹¤.

![BLIP-2_1.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/53e79c3e-6d58-4224-8c9d-7b5e90e9a2b8){: width="600px"}
<br/><br/><br/>

BLIP-2ì˜ ì£¼ìš” ì¥ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

- BLIP-2ëŠ” frozen pre-trained ì´ë¯¸ì§€ ëª¨ë¸ê³¼ ì–¸ì–´ ëª¨ë¸ì„ ëª¨ë‘ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•œë‹¤. Representation learning stage, generative learning stageì˜ ë‘ ë‹¨ê³„ë¡œ pre-trainëœ Q-Formerë¥¼ ì‚¬ìš©í•˜ì—¬ modality gapì„ í•´ì†Œí–ˆë‹¤.
- FlanT5 LLMì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” BLIP-2ëŠ” ìì—°ì–´ ì§€ì¹¨ì— ë”°ë¼ zero-shot image-to-text generationì´ ê°€ëŠ¥í•˜ë‹¤. ì´ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì‹œê°ì  ì¶”ë¡  ë° ëŒ€í™”ê°€ ê°€ëŠ¥í•˜ë‹¤(ì•„ë˜ ê·¸ë¦¼ ì˜ˆì‹œ).
- Frozen unimodal ëª¨ë¸ê³¼ ê²½ëŸ‰ Q-Formerë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ BLIP-2ëŠ” ê¸°ì¡´ ìµœì²¨ë‹¨ ëª¨ë¸ë³´ë‹¤ ì»´í“¨íŒ… íš¨ìœ¨ì„±ì´ ë” ë†’ë‹¤.
    
    ![BLIP-2_2.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/ba214ecb-f60a-440a-ba44-22107d190130){: width="1300px"}
    
<br/><br/><br/><br/><br/><br/>

# Method

---

BLIP-2ëŠ” frozen pre-train unimodal ëª¨ë¸ì—ì„œ bootstrapì„ ìˆ˜í–‰í•˜ëŠ” vision-language pre-training ë°©ë²•ì´ë‹¤. Modality gapì„ í•´ì†Œí•˜ê¸° ìœ„í•´ **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>Querying Transformer(Q-Former)</span></mark>**ì„ ì œì•ˆí•œë‹¤. Q-FormerëŠ” ë‘ ë‹¨ê³„ë¥¼ ê±°ì³ pre-trainëœë‹¤.

1. Frozen image encoderë¥¼ ì‚¬ìš©í•˜ëŠ” vision-language **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>representation learning stage</span></mark>**
2. Frozen LLMì„ ì‚¬ìš©í•˜ëŠ” vision-to-language **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>generative learning stage</span></mark>**
<br/><br/><br/><br/>

## 1. Model Architecture

![BLIP-2_3.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/bd6cabf8-4d34-444d-a2b6-31bd88a1b80d){: width="700px"}

ì•ì„œ ì–¸ê¸‰í–ˆë“¯ Q-FormerëŠ” frozen image encoderì™€ frozen LLM ì‚¬ì´ì˜ gapì„ í•´ì†Œí•˜ê¸° ìœ„í•œ trainable moduleì´ë‹¤.  ìœ„ ê·¸ë¦¼ì´ Q-Formerì˜ ì „ë°˜ì ì¸ êµ¬ì¡°ë¥¼ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤. íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
<br/><br/>

- Image encoderì—ì„œëŠ” ì…ë ¥ ì´ë¯¸ì§€ resolutionê³¼ ìƒê´€ ì—†ì´ ê³ ì •ëœ í¬ê¸°ì˜ output featureë¥¼ ì¶”ì¶œí•œë‹¤.
- Q-FormerëŠ” í¬ê²Œ ë‘ ê°œì˜ transformer submoduleë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, submoduleì—ì„œ **self-attention layerëŠ” ê³µìœ **í•œë‹¤(ë™ì¼í•œ self-attnetion layer ì‚¬ìš©).
    1. Image transformer: visual feature ì¶”ì¶œì„ ìœ„í•´ frozen image encoderì™€ ìƒí˜¸ì‘ìš©í•¨
    2. Text transformer: text encoderì™€ text decoder ë‘ ê°€ì§€ë¡œ ê¸°ëŠ¥í•  ìˆ˜ ìˆìŒ
<br/><br/>

- Image transformerì˜ ì…ë ¥ìœ¼ë¡œëŠ” **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>learnable query embedding ì§‘í•©</span></mark>**ì„ ìƒì„±í•œë‹¤. queryë“¤ì€ image transformerì— ë“¤ì–´ê°„ ë’¤ **self-attention layer**ë¥¼ í†µí•´ query ë¼ë¦¬ ì„œë¡œ ìƒí˜¸ì‘ìš©í•˜ê³ , **cross-attnetion layer**ë¥¼ í†µí•´ frozen image featureì™€ ìƒí˜¸ì‘ìš©í•œë‹¤.
- ì—¬ê¸°ì„œ, self-attention layerì—ì„œ query ë¼ë¦¬ ì„œë¡œ ìƒí˜¸ì‘ìš© ë¿ ì•„ë‹ˆë¼ **Textì™€ë„ ì¶”ê°€ë¡œ ìƒí˜¸ì‘ìš©**í•  ìˆ˜ ìˆë‹¤.
- Textì™€ ìƒí˜¸ì‘ìš© í•  ë•Œ, í•™ìŠµì¤‘ì¸ taskì— ë”°ë¼ ë‹¤ì–‘í•œ **self-attention maskë¥¼ ì ìš©**í•˜ì—¬ query-text ìƒí˜¸ì‘ìš©ì„ ì œì–´í•  ìˆ˜ ìˆë‹¤.
- Q-FormerëŠ” BERTbaseì˜ weightìœ¼ë¡œ ì´ˆê¸°í™”ë˜ëŠ”ë°, ì—¬ê¸°ì„œ cross-attention layerëŠ” randomly ì´ˆê¸°í™” ëœë‹¤.
<br/><br/>

<details>
  <summary><b>Learnable Query Embddingì— ëŒ€í•œ ìì„¸í•œ ì¶”ê°€ ì„¤ëª…(í•´ì„)</b></summary>
  <div markdown="1">
  - Learned QueriesëŠ” **ì´ë¯¸ì§€ íŠ¹ì§•(Visual Features)ì„ í…ìŠ¤íŠ¸ íŠ¹ì§•(Language Features)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë° í•„ìš”í•œ ì¤‘ê°„ ë§¤ê°œì²´** ì—­í• ì„ í•œë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, Q-Former ë‚´ì—ì„œ ê° ì¿¼ë¦¬ ë²¡í„°ëŠ” ViTë¡œë¶€í„° ë‚˜ì˜¨ ì´ë¯¸ì§€ íŠ¹ì§• ì¤‘ì—ì„œ **í…ìŠ¤íŠ¸ ì •ë³´ì™€ ì¼ì¹˜í•  ìˆ˜ ìˆëŠ” ì •ë³´**ë¥¼ ì¶”ì¶œí•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤(Visual Featureì„ Language Featuresë¡œ ë³€í™˜í•˜ëŠ” ë° í•„ìš”í•œ ì¤‘ê°„ ë§¤ê°œì²´ ì—­í• ).
  - **Nê°œì˜ í•™ìŠµ ê°€ëŠ¥í•œ ì¿¼ë¦¬ ë²¡í„°**ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì´ ë²¡í„°ë“¤ì€ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¿¼ë¦¬ ë¶€ë¶„ì—ì„œ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.
  - ì´ ì¿¼ë¦¬ ë²¡í„°ë“¤ì€ Q-Formerì˜ pre-training ë° fine-tuning ê³¼ì •ì—ì„œ ìµœì í™”ë˜ë©°, Q-Formerê°€ ì´ë¯¸ì§€ì—ì„œ **ì–´ë–¤ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³ ì í•˜ëŠ”ì§€**ë¥¼ ê²°ì •í•˜ê²Œëœë‹¤.
  - **Learned Queries** ë²¡í„°ë“¤ì€ Q-Former ë‚´ë¶€ì—ì„œ **self-attention**ì„ í†µí•´ ì„œë¡œ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•œë‹¤. ì¦‰, ViTì—ì„œ ì¶”ì¶œëœ ì´ë¯¸ì§€ íŠ¹ì§•ê³¼ëŠ” ìƒí˜¸ì‘ìš©í•˜ì§€ ì•Šê³ , ê° ì¿¼ë¦¬ê°€ í˜„ì¬ ìƒíƒœì—ì„œ **ë‹¤ë¥¸ ì¿¼ë¦¬ë“¤ê³¼ ì–´ë–»ê²Œ ìƒí˜¸ì‘ìš©í• ì§€ë¥¼ ê²°ì •í•œë‹¤.**
  - **Cross-Attention Layer**ì—ì„œëŠ” Learned Queriesê°€ ê° íŒ¨ì¹˜ì—ì„œ ì–´ë–¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ì§€ ê²°ì •í•œë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ì¿¼ë¦¬(Query)ëŠ” Q-Formerì˜ Learned Queriesì—ì„œ, í‚¤(Key)ì™€ ê°’(Value)ì€ ViTì˜ ì´ë¯¸ì§€ featureì—ì„œ ìƒì„±ëœë‹¤. ì´ ê³¼ì •ì—ì„œ, Q-Formerì˜ ê° ì¿¼ë¦¬ ë²¡í„°ëŠ” ViTì—ì„œ ì¶”ì¶œëœ íŒ¨ì¹˜ íŠ¹ì§• ì¤‘ **ìì‹ ì—ê²Œ í•„ìš”í•œ ì •ë³´**ë¥¼ ì„ íƒì ìœ¼ë¡œ ê°•ì¡°í•˜ê³ , ë‚˜ë¨¸ì§€ íŠ¹ì§•ì€ ë¬´ì‹œí•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • ì¿¼ë¦¬ ë²¡í„°ëŠ” ì´ë¯¸ì§€ ë‚´ íŠ¹ì • ê°ì²´(ì˜ˆ: ì‚¬ëŒ, ë™ë¬¼)ì— ëŒ€í•œ ì •ë³´ë¥¼ ê°•ì¡°í•˜ê³ , ë‹¤ë¥¸ ì¿¼ë¦¬ ë²¡í„°ëŠ” ë°°ê²½ì´ë‚˜ ì£¼ë³€ í™˜ê²½ì— ëŒ€í•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤.
  - **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>ì¸ì½”ë” ìµœì í™”ëœ Learned Queries</span></mark>**: **Q-Formerì˜ Learned Queries**ëŠ” íŠ¹ì • ì´ë¯¸ì§€ ì¸ì½”ë”ì™€ì˜ ì¡°í•©ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ë‹¤ë¥¸ ì´ë¯¸ì§€ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•  ê²½ìš° í•´ë‹¹ ì¸ì½”ë”ì˜ íŠ¹ì§• ì¶”ì¶œ ë°©ì‹ì— ë§ì¶° **Q-Formerë¥¼ ë‹¤ì‹œ í•™ìŠµ**í•´ì•¼í•œë‹¤.
  - Query ì§‘í•©ì€ ì´ë¯¸ì§€ Transformerë¥¼ ê±°ì³, ê° Queryë‹¹ í•˜ë‚˜ì˜ Output embeddingìœ¼ë¡œ ìƒì„±ëœë‹¤.(ì´ output ì§‘í•©ì„ Zë¼ê³  í‘œí˜„í–ˆë‹¤)
  </div>
</details>
<br/><br/>

Input queryì˜ output(Zë¼ê³  í‘œí˜„í•¨)ì€ frozen image featureë³´ë‹¤ í›¨ì”¬ ì‘ì€ í¬ê¸°ë¥¼ ê°€ì§€ë©°, ì´ëŸ¬í•œ bottleneck êµ¬ì¡°ëŠ” queryê°€ textì™€ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ì‹œê°ì  ì •ë³´ë¥¼ ì¶”ì¶œí•˜ë„ë¡ í•˜ëŠ” íš¨ê³¼ê°€ ìˆë‹¤.
<br/><br/><br/><br/><br/><br/>

## 2. Bootstrap Vision-Language Representation Learning from a Frozen Image Encoder

ì•ì„œ ì–¸ê¸‰í•œ Q-Formerì˜ ë‘ ë‹¨ê³„ í•™ìŠµ ë°©ë²• ì¤‘ 1ë‹¨ê³„ì¸ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>representation learning stage</span></mark>**ì— ëŒ€í•´ ë¨¼ì € ì‚´í´ë³´ì. 
<br/><br/>

Representation learning stageì—ì„œëŠ” Q-Formerë¥¼ frozen image encoderì— ì—°ê²°í•˜ê³  image-text pairë¥¼ ì‚¬ìš©í•˜ì—¬ pre-trainì„ ìˆ˜í–‰í•œë‹¤. í•™ìŠµì˜ ëª©ì ì€ **queryê°€ visual representationì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì„ í•™ìŠµ**í•˜ëŠ” ê²ƒì¸ë°, ì´ ë•Œ ì´ visual representaionì€  **textì™€ ì—°ê´€ì§€ì–´ì§€ëŠ” ìœ ìµí•œ ì •ë³´**ì—¬ì•¼í•œë‹¤.

BLIP ë…¼ë¬¸([BLIP ë…¼ë¬¸ ë¦¬ë·° ì°¸ì¡°](https://lunaleee.github.io/posts/blip/))ì˜ ë°©ë²•ê³¼ ìœ ì‚¬í•˜ê²Œ ì„¸ê°€ì§€ objectiveë¥¼ jointly optimizeí•œë‹¤. Query-text ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ì¡°ì ˆí•˜ê¸° ìœ„í•´ ê°ê°ì˜ objectiveëŠ” ì„œë¡œ ë‹¤ë¥¸ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>attention masking strategy</span></mark>**ë¥¼ ì ìš©í•œë‹¤. (Cross attention layerì— ì ìš©ë¨)

![BLIP-2_4.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/95d57a08-df46-4942-b680-5712114ae880){: width="650px"}

ê°ê° Maskì— ëŒ€í•œ ì„¤ëª…

- Bi-direntional Self-Attention Mask: ëª¨ë“  Queryì™€ Text tokenì´ ì„œë¡œ attend í•  ìˆ˜ ìˆë‹¤.
- Multi-modal Causal Self-Attention Mask: QueryëŠ” ëª¨ë“  queryì— ëŒ€í•´ ì„œë¡œ attend í•  ìˆ˜ ìˆì§€ë§Œ textì—ëŠ” attend í•  ìˆ˜ ì—†ë‹¤. ë°˜ë©´ Text ëª¨ë“  queryì— attend í•  ìˆ˜ ìˆê³ , ì´ì „ ì‹œì ê¹Œì§€ì˜ textì— attend í•  ìˆ˜ ìˆë‹¤.
- Uni-modal Self-Attention Mask: Queryì™€ textëŠ” ì„œë¡œì—ê²Œ attend í•  ìˆ˜ ì—†ë‹¤.
<br/><br/><br/><br/>

#### Image-Text Contrastive Learning (ITC)

ITCëŠ” image representationê³¼ text representationì„ align í•  ë•Œ **mutual informationì´ ìµœëŒ€ê°€ ë˜ë„ë¡ í•™ìŠµ**ì„ ì§„í–‰í•œë‹¤. ì´ë¥¼ ìœ„í•´ positive pairì™€ negative pairì˜ **image-text similarityë¥¼ ë¹„êµ**í•˜ëŠ” **Contrastive learning** ë°©ë²•ìœ¼ë¡œ í•™ìŠµí•œë‹¤.
<br/><br/>

- ë¨¼ì € image transformerì˜ output query representation Zë¥¼ text transformerì˜ text representation t, ì¦‰ [CLS] tokenì˜ output embeddingê³¼ align í•œë‹¤.
- Zì—ëŠ” ì—¬ëŸ¬ ê°œì˜ output embedding(ì—¬ëŸ¬ ê°œì˜ query, ê° query ë‹¹ í•˜ë‚˜ì”© output)ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ë°, ê°ê° **query outputê³¼ t ê°„ì˜ pairwise similarity**ë¥¼ êµ¬í•˜ê³  ì´ ì¤‘ ê°€ì¥ ë†’ì€ ê²ƒì„ image-text similarityë¡œ ì„ íƒí•œë‹¤.
- Attention ê³¼ì •ì—ì„œ queryì™€ textê°€ ì„œë¡œì˜ ì •ë³´ë¥¼ ì§ì ‘ ë³´ê²Œ ë˜ë©´ ì •ë³´ë¥¼ ë¯¸ë¦¬ ìœ ì¶œí•˜ëŠ” ê²ƒì´ ë˜ë¯€ë¡œ **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>unimodal self-attention mask</span></mark>**ë¥¼ ì‚¬ìš©í•œë‹¤.
- Frozen image encoderë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ end-to-end ë°©ë²•ì— ë¹„í•´ GPUë‹¹ ë” ë§ì€ ìƒ˜í”Œì„ batchì— ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ê¸°ì¡´ì— BLIPì—ì„œ ì‚¬ìš©í•œ momentum queue ë°©ì‹ ëŒ€ì‹  in-batch negativesë¥¼ ì‚¬ìš©í–ˆë‹¤.<br/><br/>
  <details>
  <summary><b>BLIPì˜ momentum queue??</b></summary>
  <div markdown="1">
> BLIPì—ì„œëŠ” positive pairì— ëŒ€í•´ì„œ weakly-correlated ì¸ ê²½ìš°, negative textì„ì—ë„ imageì™€ ë§¤ì¹­ë˜ëŠ” ë“± **noisyí•œ web ë°ì´í„°ë¥¼ ì²˜ë¦¬**í•˜ê¸° ìœ„í•´ momentum ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤. Momentum ëª¨ë¸ì„ í†µí•´ pseudo-targetì„ ë§Œë“¤ê³ , targetì— ëŒ€í•´ **soft label**ì„ ìƒì„±í•¨ìœ¼ë¡œì„œ ê¸°ì¡´ ont-hot labelì˜ ë¬¸ì œì ì„ ë³´ì™„í•œë‹¤.<br/><br/>
> BLIP-2ì—ì„œëŠ” GPU ë‹¹ ë” ë§ì€ ìƒ˜í”Œì„  batchì— ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ **in-batch negative**ë¥¼ ì‚¬ìš©í•œë‹¤. In-batch negatives ë€, randomí•˜ê²Œ batchì˜ ìš”ì†Œë“¤ì„ êµ¬ì„±í•˜ê³  batch ìš”ì†Œë“¤ ì¤‘ íŠ¹ì • queryì— í•´ë‹¹í•˜ëŠ” text ì´ì™¸ì— ë‚˜ë¨¸ì§€ë¥¼ negative ê´€ê³„ë¡œ ë³´ê³  í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤. Batch ë‚´ ìƒ˜í”Œ ìˆ˜ê°€ ë§ìœ¼ë¯€ë¡œ noiseì— ëœ ì·¨ì•½í•  ìˆ˜ ìˆë‹¤.
  </div>
  </details>
 
<br/><br/><br/>

#### Image-grounded Text Generation (ITG)

ITGëŠ” input imageê°€ conditionìœ¼ë¡œ ì£¼ì–´ì¡Œì„ ë•Œ, Q-Formerê°€ textë¥¼ ìƒì„±(generation)í•˜ë„ë¡ í•™ìŠµí•œë‹¤. 

Q-FormerëŠ” frozen image encoderì™€ text tokenê°„ì˜ ì§ì ‘ì ì¸ ìƒí˜¸ì‘ìš©ì„ í—ˆìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, text ìƒì„±ì— í•„ìš”í•œ ì •ë³´ëŠ” queryë¥¼ í†µí•´ ì¶”ì¶œëœ ë‹¤ìŒ Self-attention layerë¥¼ í†µí•´ text tokenì— ì „ë‹¬ë˜ì–´ì•¼ í•œë‹¤. ë”°ë¼ì„œ queryëŠ” textì— ëŒ€í•œ ëª¨ë“  ì •ë³´ë¥¼ ìº¡ì³í•˜ëŠ” visual featureì„ ì¶”ì¶œí•´ì•¼ í•œë‹¤. 

Query-text ìƒí˜¸ì‘ìš©ì„ ì œì–´í•˜ê¸° ìœ„í•´ [UniLM ë…¼ë¬¸ğŸ“„](https://arxiv.org/abs/1905.03197)ì—ì„œì™€ ìœ ì‚¬í•œ **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>multimodal causal self-attention mask</span></mark>**ë¥¼ ì‚¬ìš©í•œë‹¤. ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ queryëŠ” queryë¼ë¦¬ ì„œë¡œ attend í•  ìˆ˜ ìˆì§€ë§Œ, text tokenì—ëŠ” attend í•  ìˆ˜ ì—†ë‹¤. ë°˜ë©´ì— text tokenì€ ëª¨ë“  queryì— attend í•  ìˆ˜ ìˆê³ , ì´ì „ text tokenì— attend í•  ìˆ˜ ìˆë‹¤.

ë˜í•œ decoding taskë¥¼ ì•Œë¦¬ê¸° ìœ„í•´ì„œëŠ” ì²« ë²ˆì§¸ text tokenì„ [CLS] tokenì—ì„œ [DEC] tokenìœ¼ë¡œ ëŒ€ì²´í•œë‹¤.
<br/><br/><br/><br/>

#### Image-Text Matching (ITM)

ITMì€ imageì™€ textê°„ì˜ fine-grained alignmentë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. BLIPê³¼ ë§ˆì°¬ê°€ì§€ë¡œ image-text pairê°€ positiveì¸ì§€, negativeì¸ì§€ ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ê²Œ í•˜ëŠ” **binary classification task**ì´ë‹¤. ì—¬ê¸°ì„œëŠ” **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>bi-directional self-attention mask</span></mark>**ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  queryì™€ textê°€ ì„œë¡œ attend í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ output query embedding ZëŠ” multimodal informationì„ ìº¡ì²˜í•œë‹¤. 

ê°ê°ì˜ output query embeddingì„ two-class linear classifierì— ë„£ì–´ logitì„ êµ¬í•œ ë’¤, ì „ì²´ queryë“¤ì˜ **logitë“¤ì„ average**í•˜ì—¬ **output matching score**ë¥¼ êµ¬í•œë‹¤. ë˜í•œ informative negative pairë¥¼ ì„ ì •í•˜ê¸° ìœ„í•´ hard negative mining* ì „ëµì„ ì‚¬ìš©í–ˆë‹¤. 

> **Hard negative mining***<br/>
Hard negativeí•œ ë°ì´í„°ëŠ” ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ìš´ sample(ì‹¤ì œë¡œëŠ” negativeâ†’positiveë¡œ ì˜ˆì¸¡)ì„ ì˜ë¯¸í•œë‹¤. Hard negative mining ê¸°ë²•ì€ ì´ëŸ¬í•œ ëª¨ë¸ì´ ë§ì¶”ê¸° ì–´ë ¤ìš´ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ ëª¨ë¸ì´ í•™ìŠµí•˜ê²Œ í•˜ì—¬ ëª¨ë¸ì´ False Positive ì˜¤ë¥˜ì— ê°•ì¸í•´ì§€ë„ë¡ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì˜ë¯¸í•œë‹¤.
> 

<br/><br/><br/><br/><br/>

## 3. Bootstrap Vision-to-Language Generative Learning from a Frozen LLM

![BLIP-2_5.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/bdd002a9-c6dd-4d53-8b92-865bc7a5edd4){: width="1200px"}

ë‹¤ìŒìœ¼ë¡œ Q-Formerì˜ ë‘ ë‹¨ê³„ í•™ìŠµ ë°©ë²• ì¤‘ 2ë‹¨ê³„ì¸ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>generative learning stage</span></mark>**ì— ëŒ€í•´ ì‚´í´ë³´ì.
<br/><br/>

- Generative learning stageì—ì„œëŠ”Q-Former(frozen image encoderê°€ ì—°ê²°ëœ)ë¥¼ frozen LLMì— ì—°ê²°í•˜ì—¬ LLMì˜ generative language capabilityë¥¼ ìˆ˜ì§‘í•œë‹¤.
- ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ **fully-connected(FC) layer**ë¥¼ ì‚¬ìš©í•˜ì—¬ output query embedding Zë¥¼ LLMì˜ text embeddingê³¼ ë™ì¼í•œ ì°¨ì›ìœ¼ë¡œ linearly project í•œë‹¤.
- ê·¸ ë’¤ì— projectionëœ query embeddingì„ input text embedding ì•ì— ì¶”ê°€í•œë‹¤. ì¶”ê°€ëœ projected query embeddingì€ Q-Formerì—ì„œ ì¶”ì¶œí•œ visual representationì— ë”°ë¼ LLMì— ì¡°ê±´ì„ ë¶€ì—¬í•˜ëŠ” **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>soft visual prompt</span></mark>** ì—­í• ì„ í•œë‹¤.
- Q-FormerëŠ” language-informative visual representationì„ ì¶”ì¶œí•˜ë„ë¡ pre-trainë˜ì—ˆê¸° ë•Œë¬¸ì— ê´€ë ¨ ì—†ëŠ” visual ì •ë³´ë¥¼ ì œê±°í•˜ë©´ì„œ LLMì— ê°€ì¥ ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” information bottleneckìœ¼ë¡œ íš¨ê³¼ì ìœ¼ë¡œ ê¸°ëŠ¥í•œë‹¤.
<br/><br/><br/><br/>

ì €ìëŠ” ë‘ ê°€ì§€ ìœ í˜•ì˜ LLMì— ëŒ€í•´ ì‹¤í—˜ì„ ì§„í–‰í–ˆë‹¤. 

1. Decoder-based LLMs: frozen LLMì´ Q-Formerì˜ visual representationì— ë”°ë¼ textë¥¼ ìƒì„±í•˜ë„ë¡ í•˜ëŠ” **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>language modeling loss</span></mark>**ë¥¼ ì‚¬ìš©í•˜ì—¬ pre-train ì§„í–‰.
2. Encoder-decoder-based LLMs: **<mark style='background-color: var(--hl-green)'><span style='color: var(--text-color)'>prefix language modeling loss</span></mark>**ë¥¼ ì‚¬ìš©í•˜ì—¬ pre-train í•¨. ì—¬ê¸°ì„œ textë¥¼ ë‘ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆˆë‹¤. Prefix textëŠ” visual representationê³¼ concat ë˜ì–´ **LLMì˜ encoderì˜ ì…ë ¥**ìœ¼ë¡œ ì‚¬ìš©ë˜ê³ , suffix textëŠ” LLM decoderì˜ **generation target**ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.
<br/><br/><br/><br/><br/>

## 4. Model Pre-training

#### Pre-trainig data.

- Pre-training ë°ì´í„°ì…‹ìœ¼ë¡œ BLIPê³¼ ë™ì¼í•œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í–ˆë‹¤. ì´ 129Mì˜ ì´ë¯¸ì§€(COCO, Visual Genome, CC3M, CC12M, SBU, LAION400M ì¼ë¶€)ë¥¼ ì‚¬ìš©í–ˆë‹¤.
- ë˜í•œ BLIPì˜ web ì´ë¯¸ì§€ì— ëŒ€í•œ í•©ì„± captionì„ ìƒì„±í•˜ê¸° ìœ„í•´ **<mark style='background-color: var(--hl-yellow)'><span style='color: var(--text-color)'>CapFilt ë°©ë²•</span></mark>**ì„ ì‚¬ìš©í–ˆë‹¤. CapFilt ë°©ë²•ì€ [BLIP ë…¼ë¬¸ ë¦¬ë·°](https://lunaleee.github.io/posts/blip/)ë¥¼ ì°¸ì¡°í•˜ì.
<br/><br/><br/>

#### Pre-trained image encoder and LLM.

- Frozen image encoderì˜ ê²½ìš° ViT-L/14(from CLIP), ViT-g/14(from EVA-CLIP) ë‘ ê°€ì§€ë¥¼ ì‚¬ìš©í•˜ë©° ë§ˆì§€ë§‰ layerë¥¼ ì œê±°í•˜ê³  ë’¤ì—ì„œ ë‘ ë²ˆì§¸ layerì˜ output featureë¥¼ ì‚¬ìš©í•œë‹¤.
- Frozen language modelì˜ ê²½ìš°, decoder-based LLMìœ¼ë¡œëŠ” unsupervised-trained OPT model familyë¥¼ ì‚¬ìš©í•˜ê³ , encoder-decoder-based LLMìœ¼ë¡œëŠ” instruction-trained FlanT5 model familyë¥¼ ì‚¬ìš©í•œë‹¤.
<br/><br/><br/><br/><br/><br/>

# Experiments

---

í‘œ 1ì€ ë‹¤ì–‘í•œ zero-shot vision-language taskì— ëŒ€í•œ BLIP-2 ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤. ì´ì „ SOTA ëª¨ë¸ê³¼ ë¹„êµí•˜ì—¬ BLIP-2ëŠ” parameter ìˆ˜ê°€ í›¨ì”¬ ì ìœ¼ë©´ì„œ í–¥ìƒëœ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆë‹¤.

![BLIP-2_6.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/6877fa0f-545d-4995-b5e1-057685a457a7){: width="1000px"}
<br/><br/>

### 1. Instructed Zero-shot Image-to-Text Generation

BLIP-2ë¥¼ ì‚¬ìš©í•˜ë©´ LLMì´ text prompt ë‚´ìš©ì„ ìœ ì§€í•˜ë©´ì„œ ì´ë¯¸ì§€ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, LLMì— ëŒ€í•œ ì…ë ¥ìœ¼ë¡œ visual prompt ë’¤ì— text promptë¥¼ ì¶”ê°€í•˜ì—¬ image-to-text generationì„ ì œì–´í•  ìˆ˜ ìˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ ë‹¤ì–‘í•œ zero-shot image-to-text capabilityì— ëŒ€í•œ ì˜ˆì‹œì´ë‹¤.

![BLIP-2_7.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/3830106c-132c-41e8-83a3-d38c8907f082){: width="700px"}
<br/><br/>

#### Zero-shot VQA.

ë¨¼ì € zero-shot visual question answering taskì— ëŒ€í•œ ì •ëŸ‰í‰ê°€ë¥¼ ìˆ˜í–‰í–ˆë‹¤. OPT ëª¨ë¸ì˜ ê²½ìš° "Question: {} Answer:â€ promptë¥¼ ì‚¬ìš©í–ˆê³ , FlanT5 ëª¨ë¸ì˜ ê²½ìš° â€œQuestion: {} Short answer:â€  promptë¥¼ ì‚¬ìš©í–ˆë‹¤. ìƒì„± ì¤‘ width 5ì˜ beam searchë¥¼ ì‚¬ìš©í–ˆê³  length-penaltyë¥¼ ì£¼ì–´ ì§§ì€ ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ í–ˆë‹¤.

ê²°ê³¼ëŠ” ì•„ë˜ í‘œì™€ ê°™ë‹¤. BLIP-2ê°€ VQAv2 ë°ì´í„°ì…‹ì„ ì‚¬ìš©í–ˆì„ ë•Œ, parameter ìˆ˜ê°€ 54ë°° ì ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  SOTAë¥¼ ë‹¬ì„±í•œ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê²°ê³¼ë¥¼ í†µí•´ ì¤‘ìš”í•œ ê²°ê³¼, ì¦‰ ë” **ê°•ë ¥í•œ image encoderë‚˜ LLMì´ ëª¨ë‘ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ê°€ì ¸ì˜¨ë‹¤**ëŠ” ê²ƒì„ ì¦ëª…í–ˆë‹¤ê³  í•œë‹¤.

![BLIP-2_8.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/a3ad6e38-e6c7-46df-bf50-79d43c332626){: width="700px"}
<br/><br/><br/>

ì²« ë²ˆì§¸ pre-training ë‹¨ê³„ representation learning stageì—ì„œ Q-Formerë¥¼ pre-trainí•˜ì—¬ textì™€ ê´€ë ¨ëœ visual featureë¥¼ í•™ìŠµí•˜ë¯€ë¡œ LLMì´ vision-language alignmentì„ í•™ìŠµí•´ì•¼í•˜ëŠ” ë¶€ë‹´ì´ ì¤„ì–´ë“ ë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ generative learningì— ëŒ€í•œ representation learningì˜ íš¨ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤. Representation learningì´ ì—†ìœ¼ë©´ ë‘ LLM ëª¨ë‘ zero-shot VQAì—ì„œ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.

![BLIP-2_9.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/08e74054-a809-44b6-b68c-7aff1b8cef03){: width="550px"}
<br/><br/><br/><br/>

### 2. Image Captioning

BLIP-2 ëª¨ë¸ì„ fine-tuningí•˜ì—¬ ì´ë¯¸ì§€ì˜ visual contentì— ëŒ€í•œ text descriptionì„ ìƒì„±í•˜ë„ë¡ ìš”ì²­í•˜ëŠ” **image captioning task**ë¥¼ ìˆ˜í–‰í–ˆë‹¤. LLMì„ frozen ìƒíƒœë¡œ ìœ ì§€í•˜ê³  image encoderì™€ í•¨ê»˜ Q-Formerì˜ parameterë¥¼ ì—…ë°ì´íŠ¸í–ˆë‹¤. COCOì— ëŒ€í•´ fine-tuningì„ ìˆ˜í–‰í•˜ê³  COCO testsetê³¼ NoCaps validation setìœ¼ë¡œì˜ zero-shot transferë¥¼ ëª¨ë‘ í‰ê°€í–ˆë‹¤. ê²°ê³¼ëŠ” ì•„ë˜ í‘œì™€ ê°™ë‹¤. BLIP-2ëŠ” SOTA ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì—¬ out-domain imageì— ëŒ€í•œ ê°•ë ¥í•œ generalization ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤.

![BLIP-2_10.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/ae21cffd-cc97-412a-b3cc-aef8ab22d8c8){: width="1100px"}
<br/><br/><br/><br/>

### 3. Visual Question Answering

Annotated VQA ë°ì´í„°ê°€ ì£¼ì–´ì§€ëŠ” ê²½ìš°, LLMì„ frozen ìƒíƒœë¡œ ìœ ì§€í•˜ë©´ì„œ Q-Formerì™€ image encoderë¥¼ fine-tuningí–ˆë‹¤. LLMì´ Q-Formerì˜ ì¶œë ¥ê³¼ ì§ˆë¬¸ì„ ì…ë ¥ìœ¼ë¡œ ë°›ê³  ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ ìš”ì²­ë°›ëŠ” **open-ended answer generation loss**ë¥¼ ì‚¬ìš©í•˜ì—¬ fine-tuningì„ ì§„í–‰í–ˆë‹¤. 

ì§ˆë¬¸ê³¼ ë” ê´€ë ¨ì„±ì´ ë†’ì€ ì´ë¯¸ì§€ featureë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ì§ˆë¬¸ì— ëŒ€í•´ Q-Formerì— ì¶”ê°€ì ì¸ conditionì„ ê±¸ì—ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, question tokenì€ Q-Formerì— ëŒ€í•œ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ê³  self-attention ë ˆì´ì–´ë¥¼ í†µí•´ queryë“¤ê³¼ ìƒí˜¸ ì‘ìš©í•œë‹¤. ì´ë¥¼ í†µí•´ Q-Formerì˜ cross-attention layerê°€ ë” ë§ì€ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” image regionì— ì§‘ì¤‘í•˜ë„ë¡ ìœ ë„í•œë‹¤. ê²°ê³¼ëŠ” ì•„ë˜ í‘œì™€ ê°™ë‹¤.

![BLIP-2_11.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/d79be056-3d31-4b11-ab43-83a23b116906){: width="500px"}
<br/><br/><br/><br/>

### 4. Image-Text Retrieval

image-text retrievalì„ ìœ„í•´(language generationì´ í•„ìš”í•˜ì§€ ì•ŠìŒ), LLM ì—†ì´ pre-trainëœ first-stage ëª¨ë¸ì„ ë°”ë¡œ fine-tuning í–ˆë‹¤. íŠ¹íˆ pre-trainê³¼ ë™ì¼í•œ objective(ITC, ITM ë° ITG)ë¥¼ ì‚¬ìš©í•˜ì—¬ Q-Formerì™€ image encoderë¥¼ COCO ë°ì´í„°ì…‹ìœ¼ë¡œ pre-train í–ˆë‹¤. ê·¸ ë‹¤ìŒ COCO ë° Flickr30K ë°ì´í„°ì…‹ì— ëŒ€í•´ image-to-text retrieval ë° text-to-image retrievalì— ëŒ€í•´ ëª¨ë¸ì„ í‰ê°€í–ˆë‹¤. ê²°ê³¼ëŠ” ì•„ë˜ í‘œì™€ ê°™ë‹¤.

![BLIP-2_12.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/d4c91074-0213-4425-93f4-82d087a24453){: width="1100px"}
<br/><br/><br/>

ITC ë° ITM lossëŠ” image-text similarityì„ ì§ì ‘ í•™ìŠµí•˜ë¯€ë¡œ image-text retrievalì— í•„ìˆ˜ì ì´ë‹¤. í‘œ 6ì—ì„œëŠ” ITG(image-grounded text generation) lossê°€ image-text retrievalì—ë„ ìœ ìš©í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤. ITG lossëŠ” textì™€ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ visual featureë¥¼ ì¶”ì¶œí•˜ë„ë¡ queryë¥¼ í•™ìŠµí•˜ì—¬ vision-language alignmentì„ í–¥ìƒì‹œí‚¨ë‹¤.

![BLIP-2_13.png](https://github.com/cotes2020/jekyll-theme-chirpy/assets/34572874/7a854202-0c9e-49d0-a0aa-6ac712973054){: width="500px"}

<br/><br/><br/><br/>
